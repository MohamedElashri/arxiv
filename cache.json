{"2025-09-11T00:00:00Z":{"High Energy Physics - Experimental":[{"id":"http://arxiv.org/abs/2509.09601v1","updated":"2025-09-11T16:41:41Z","published":"2025-09-11T16:41:41Z","title":"Are arXiv submissions on Wednesday better cited? Introducing Big Data\n  methods in undergraduate courses on scientific computing","summary":"  Extracting information from big data sets, both real and simulated, is a\nmodern hallmark of the physical sciences. In practice, students face barriers\nto learning ``Big Data'' methods in undergraduate physics and astronomy\ncurricula. As an attempt to alleviate some of these challenges, we present a\nsimple, farm-to-table data analysis pipeline that can collect, process, and\nplot data from the 800k entries common to the arXiv preprint repository and the\nbibliographical database inSpireHEP. The pipeline employs contemporary research\npractices and can be implemented using open-sourced Python libraries common to\nundergraduate courses on Scientific Computing. To support the use such\npipelines in classroom contexts, we make public an example implementation,\nauthored by two undergraduate physics students, that runs on off-the-shelf\nlaptops. For advanced students, we discuss applications of the pipeline,\nincluding for online DAQ monitoring and commercialization.\n","authors":["Stéphane Delorme","Leon Mach","Hubert Paszkiewicz","Richard Ruiz"],"pdf_url":"https://arxiv.org/pdf/2509.09601v1.pdf","comment":"10 pages, 8 figures, 2 tables, 1 listing, project available at\n  https://gitlab.cern.ch/riruiz/public-projects/-/tree/master/BibAPI/"},{"id":"http://arxiv.org/abs/2504.21183v2","updated":"2025-09-11T15:38:00Z","published":"2025-04-29T21:36:28Z","title":"Do QGP Droplets Drive Anisotropy in Small Systems? Insights from RHIC\n  and the LHC","summary":"  Azimuthal anisotropy scaling functions for identified mesons and baryons are\nanalyzed in large (Pb+Pb at $\\sqrt{s_{NN}} = 2.76$ and 5.02 TeV, Au+Au at\n$\\sqrt{s_{NN}} = 200$ GeV), intermediate (Cu+Cu at $\\sqrt{s_{NN}} = 200$~GeV),\nand small (p+Pb at $\\sqrt{s_{NN}} = 5.02$ and 8.16 TeV, p+Au, d+Au, and\n$^3$He+Au at $\\sqrt{s_{NN}} = 200$ GeV) collision systems. The scaling\nfunctions' fidelity supports a hydrodynamic-like origin for anisotropies in the\nflow-dominated regime. Central Pb+Pb, Au+Au, and Cu+Cu reflect QGP-driven\nexpansion with strong radial flow and significant jet quenching, while\nperipheral Pb+Pb and Cu+Cu exhibit hadronic-dominated dynamics. In contrast,\ncentral RHIC small systems show hadronic-dominated behavior, with strong\nre-scattering, negligible radial flow, and suppressed jet quenching, following\nthe hierarchy p+Au $>$ d+Au $>$ $^3$He+Au. At the LHC, ultra-central p+Pb\ncollisions display enhanced radial flow, reduced re-scattering, and small but\nnonzero jet quenching. Scaling violations at high $p_T$ reflect partial\nsuppression of partonic energy loss. These findings demonstrate that QGP-like\nbehavior in small systems depends sensitively on both system size and beam\nenergy, and establish the scaling framework as a robust diagnostic of\ncollectivity and medium properties across diverse collision conditions.\n","authors":["Roy A. Lacey"],"pdf_url":"https://arxiv.org/pdf/2504.21183v2.pdf","comment":"Seven pages, four figures, submitted for publication"},{"id":"http://arxiv.org/abs/2509.09514v1","updated":"2025-09-11T14:55:55Z","published":"2025-09-11T14:55:55Z","title":"Mapping of discrete range modulated proton radiograph to\n  water-equivalent path length using machine learning","summary":"  Objective. Proton beams enable localized dose delivery. Accurate range\nestimation is essential, but planning still relies on X-ray CT, which\nintroduces uncertainty in stopping power and range. Proton CT measures water\nequivalent thickness directly but suffers resolution loss from multiple Coulomb\nscattering. We develop a data driven method that reconstructs water equivalent\npath length (WEPL) maps from energy resolved proton radiographs, bypassing\nintermediate reconstructions. Approach. We present a machine learning pipeline\nfor WEPL from high dimensional radiographs. Data were generated with the TOPAS\nMonte Carlo toolkit, modeling a clinical nozzle and a patient CT. Proton\nenergies spanned 70-230 MeV across 72 projection angles. Principal component\nanalysis reduced input dimensionality while preserving signal. A conditional\nGAN with gradient penalty was trained for WEPL prediction using a composite\nloss (adversarial, MSE, SSIM, perceptual) to balance sharpness, accuracy, and\nstability. Main results. The model reached a mean relative WEPL deviation of\n2.5 percent, an SSIM of 0.97, and a proton radiography gamma index passing rate\nof 97.1 percent (2 percent delta WEPL, 3 mm distance-to-agreement) on a\nsimulated head phantom. Results indicate high spatial fidelity and strong\nstructural agreement. Significance. WEPL can be mapped directly from proton\nradiographs with deep learning while avoiding intermediate steps. The method\nmitigates limits of analytic techniques and may improve treatment planning.\nFuture work will tune the number of PCA components, include detector response,\nexplore low dose settings, and extend multi angle data toward full proton CT\nreconstruction; it is compatible with clinical workflows.\n","authors":["Atiq Ur Rahman","Chun-Chieh Wang","Shu-Wei Wu","Tsi-Chian Chao","I-Chun Cho"],"pdf_url":"https://arxiv.org/pdf/2509.09514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09511v1","updated":"2025-09-11T14:52:18Z","published":"2025-09-11T14:52:18Z","title":"Chromatic Calorimetry -- A Novel Approach to Validate Energy Resolution\n  and Particle Discrimination","summary":"  Chromatic calorimetry (CCAL) analyses particle detection by utilizing\nscintillators with distinct emission wavelengths to measure the longitudinal\nenergy deposition of particle showers in high-energy physics, improving\nparticle identification (PID) and energy resolution. By stacking scintillators\nin order of decreasing emission wavelength, CCAL enables layer-specific energy\nmeasurements, analyzed via amplitude fractions ($f_i = A_i / \\sum_j A_j$) and\ncenter of gravity ($\\langle z_{\\text{cog}} \\rangle = \\sum_i z_i E_i / \\sum_i\nE_i$). This thesis presents results from two CERN Super Proton Synchrotron\n(SPS) experiments conducted in 2023 and 2024, complemented by GEANT4\nsimulations of a quantum dot (QD)-based CCAL design, to validate its potential\nfor future colliders such as the Future Circular Collider (FCC).\n","authors":["Devanshi Arora"],"pdf_url":"https://arxiv.org/pdf/2509.09511v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2503.19862v3","updated":"2025-09-11T13:51:23Z","published":"2025-03-25T17:29:10Z","title":"Early Career Researcher Input to the European Strategy for Particle\n  Physics Update: White Paper","summary":"  This document, written by early career researchers (ECRs) in particle\nphysics, aims to represent the perspectives of the European ECR community and\nserves as input for the 2025--2026 update of the European Strategy for Particle\nPhysics. With input from a community-wide survey, it highlights key challenges\nfaced by ECRs -- career stability, funding access and long-term research\nopportunities -- while proposing policy recommendations and targeted\ninitiatives. It underscores the importance of practices fostering diverse,\nequitable, inclusive and healthy workplaces, as well as of stronger ECR\ncommunities, and highlights how effective communication and interdisciplinary\ncollaborations reinforce the societal relevance of particle physics and promote\ncontinued support for large-scale and long-term projects. Finally, the future\nof both collider and beyond-collider experiments is addressed, emphasising the\ncritical role of ECRs in shaping future projects. The ECR contribution is\nformed of two parts: the ten-page executive summary submitted as input to the\nEuropean Strategy for Particle Physics Update and, as backup document, this\nextended white paper providing additional context.\n","authors":["Jan-Hendrik Arling","Alexander Burgman","Christina Dimitriadi","Ulrich Einhaus","Axel Gallén","Abdelhamid Haddad","Laura Huhta","Armin Ilg","Jan Klamka","Elizabeth Long","Thomas Madlener","Arnau Morancho Tardà","Emanuela Musumeci","Krzysztof Mękała","Elena Pompa Pacchi","Marvin Pfaff","Daniel Reichelt","Leonhard Reichenbach","Birgit Stapf","Francesco P. Ucci","Erik Wallin","Harriet Watson","Sagar Vidya Addepalli","Bruno Alves","Robert Mihai Amarinei","Ricardo Barrué","Lydia Brenner","Giacomo Da Molin","Arturo de Giorgi","Bohdan Dudar","Francesco Giuli","Andrea Gurgone","César Jesús-Valls","Antoine Laudrain","Martin J. Losekamm","Rafał Masełek","Wrishik Naskar","Miquel Nebot-Guinot","Marko Pesut","Thomas Pöschl","Efrain P. Segarra","Rebecca Taylor","Pavel Vana","Hannah Wakeling","Aidan R. Wiederhold"],"pdf_url":"https://arxiv.org/pdf/2503.19862v3.pdf","comment":"Endorsed by the ECFA ECR Panel. Editor and author attribution in the\n  document"},{"id":"http://arxiv.org/abs/2509.09433v1","updated":"2025-09-11T13:18:33Z","published":"2025-09-11T13:18:33Z","title":"Long-term operation of the screen-printed graphite-based resistive\n  coatings on the HPL electrode for the Resistive Plate Chamber","summary":"  The reliability of large-area Resistive Plate Chambers (RPCs) operated under\nHigh-Luminosity Large Hadron Collider (HL-LHC) conditions is governed by the\nlong-term stability and radiation tolerance of screen-printed graphite/phenoxy\ncoatings on high-pressure-laminate (HPL) electrodes. This work presents a\ncomprehensive, end-to-end qualification of such coatings that integrates\nindustrial process control and metrology with controlled humidity/temperature\ncampaigns, extended high-voltage stress testing to decade-scale charge levels,\nand representative neutron and gamma irradiation at CERN facilities. The\nresults establish reproducible industrial coating production, stable\nperformance under sustained operation and irradiation, and practical acceptance\ncriteria with operating and monitoring guidelines. The study provides a\ntransferable quality-assurance framework for graphite-based resistive coatings\non HPL electrodes, enabling reproducible production and reliable RPC\nperformance for the HL-LHC upgrades and for future high-rate collider\nexperiments.\n","authors":["Davide Costa","Francesco Fallavollita","Hubert Kroha","Oliver Kortner","Pavel Maly","Giorgia Proto","Daniel Soyk","Elena Voevodina","Jorg Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2509.09433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08461v2","updated":"2025-09-11T13:03:04Z","published":"2025-09-10T10:07:27Z","title":"Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics","summary":"  Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics.\n","authors":["Dikshant Sagar","Kaiwen Yu","Alejandro Yankelevich","Jianming Bian","Pierre Baldi"],"pdf_url":"https://arxiv.org/pdf/2509.08461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21476v3","updated":"2025-09-11T09:32:41Z","published":"2025-05-27T17:48:30Z","title":"The anomalous magnetic moment of the muon in the Standard Model: an\n  update","summary":"  We present the current Standard Model (SM) prediction for the muon anomalous\nmagnetic moment, $a_\\mu$, updating the first White Paper (WP20) [1]. The pure\nQED and electroweak contributions have been further consolidated, while\nhadronic contributions continue to be responsible for the bulk of the\nuncertainty of the SM prediction. Significant progress has been achieved in the\nhadronic light-by-light scattering contribution using both the data-driven\ndispersive approach as well as lattice-QCD calculations, leading to a reduction\nof the uncertainty by almost a factor of two. The most important development\nsince WP20 is the change in the estimate of the leading-order\nhadronic-vacuum-polarization (LO HVP) contribution. A new measurement of the\n$e^+e^-\\to\\pi^+\\pi^-$ cross section by CMD-3 has increased the tensions among\ndata-driven dispersive evaluations of the LO HVP contribution to a level that\nmakes it impossible to combine the results in a meaningful way. At the same\ntime, the attainable precision of lattice-QCD calculations has increased\nsubstantially and allows for a consolidated lattice-QCD average of the LO HVP\ncontribution with a precision of about 0.9%. Adopting the latter in this update\nhas resulted in a major upward shift of the total SM prediction, which now\nreads $a_\\mu^\\text{SM} = 116\\,592\\,033(62)\\times 10^{-11}$ (530 ppb). When\ncompared against the current experimental average based on the E821 experiment\nand runs 1-6 of E989 at Fermilab, one finds $a_\\mu^\\text{exp} - a_\\mu^\\text{SM}\n=38(63)\\times 10^{-11}$, which implies that there is no tension between the SM\nand experiment at the current level of precision. The final precision of E989\n(127 ppb) is the target of future efforts by the Theory Initiative. The\nresolution of the tensions among data-driven dispersive evaluations of the LO\nHVP contribution will be a key element in this endeavor.\n","authors":["R. Aliberti","T. Aoyama","E. Balzani","A. Bashir","G. Benton","J. Bijnens","V. Biloshytskyi","T. Blum","D. Boito","M. Bruno","E. Budassi","S. Burri","L. Cappiello","C. M. Carloni Calame","M. Cè","V. Cirigliano","D. A. Clarke","G. Colangelo","L. Cotrozzi","M. Cottini","I. Danilkin","M. Davier","M. Della Morte","A. Denig","C. DeTar","V. Druzhinin","G. Eichmann","A. X. El-Khadra","E. Estrada","X. Feng","C. S. Fischer","R. Frezzotti","G. Gagliardi","A. Gérardin","M. Ghilardi","D. Giusti","M. Golterman","S. Gonzàlez-Solís","S. Gottlieb","R. Gruber","A. Guevara","V. Gülpers","A. Gurgone","F. Hagelstein","M. Hayakawa","N. Hermansson-Truedsson","A. Hoecker","M. Hoferichter","B. -L. Hoid","S. Holz","R. J. Hudspith","F. Ignatov","L. Jin","N. Kalntis","G. Kanwar","A. Keshavarzi","J. Komijani","J. Koponen","S. Kuberski","B. Kubis","A. Kupich","A. Kupść","S. Lahert","S. Laporta","C. Lehner","M. Lellmann","L. Lellouch","T. Leplumey","J. Leutgeb","T. Lin","Q. Liu","I. Logashenko","C. Y. London","G. López Castro","J. Lüdtke","A. Lusiani","A. Lutz","J. Mager","B. Malaescu","K. Maltman","M. K. Marinković","J. Márquez","P. Masjuan","H. B. Meyer","T. Mibe","N. Miller","A. Miramontes","A. Miranda","G. Montagna","S. E. Müller","E. T. Neil","A. V. Nesterenko","O. Nicrosini","M. Nio","D. Nomura","J. Paltrinieri","L. Parato","J. Parrino","V. Pascalutsa","M. Passera","S. Peris","P. Petit Rosàs","F. Piccinini","R. N. Pilato","L. Polat","A. Portelli","D. Portillo-Sánchez","M. Procura","L. Punzi","K. Raya","A. Rebhan","C. F. Redmer","B. L. Roberts","A. Rodríguez-Sánchez","P. Roig","J. Ruiz de Elvira","P. Sánchez-Puertas","A. Signer","J. W. Sitison","D. Stamen","D. Stöckinger","H. Stöckinger-Kim","P. Stoffer","Y. Sue","P. Tavella","T. Teubner","J. -N. Toelstede","G. Toledo","W. J. Torres Bobadilla","J. T. Tsang","F. P. Ucci","Y. Ulrich","R. S. Van de Water","G. Venanzoni","S. Volkov","G. von Hippel","G. Wang","U. Wenger","H. Wittig","A. Wright","E. Zaid","M. Zanke","Z. Zhang","M. Zillinger","C. Alexandrou","A. Altherr","M. Anderson","C. Aubin","S. Bacchio","P. Beltrame","A. Beltran","P. Boyle","I. Campos Plasencia","I. Caprini","B. Chakraborty","G. Chanturia","A. Crivellin","A. Czarnecki","L. -Y. Dai","T. Dave","L. Del Debbio","K. Demory","D. Djukanovic","T. Draper","A. Driutti","M. Endo","F. Erben","K. Ferraby","J. Finkenrath","L. Flower","A. Francis","E. Gámiz","J. Gogniat","A. V. Grebe","S. Gündogdu","M. T. Hansen","S. Hashimoto","H. Hayashii","D. W. Hertzog","L. A. Heuser","L. Hostetler","X. T. Hou","G. S. Huang","T. Iijima","K. Inami","A. Jüttner","R. Kitano","M. Knecht","S. Kollatzsch","A. S. Kronfeld","T. Lenz","G. Levati","Q. M. Li","Y. P. Liao","J. Libby","K. F. Liu","V. Lubicz","M. T. Lynch","A. T. Lytle","J. L. Ma","K. Miura","K. Möhling","J. Muskalla","F. Noël","K. Ottnad","P. Paradisi","C. T. Peterson","A. Pich","S. Pitelis","S. Plura","A. Price","D. Radic","A. Radzhabov","A. Risch","S. Romiti","S. Sahoo","F. Sannino","H. Schäfer","Y. Schelhaas","S. I. Serednyakov","O. Shekhovtsova","J. N. Simone","S. Simula","E. P. Solodov","F. M. Stokes","M. Vanderhaeghen","A. Vaquero","N. Vestergaard","W. P. Wang","Z. Wąs","K. Yamashita","Y. B. Yang","T. Yoshioka","C. Z. Yuan","A. S. Zhevlakov"],"pdf_url":"https://arxiv.org/pdf/2505.21476v3.pdf","comment":"188 pages, 83 figures; $a_\\mu^\\text{exp}$ updated to final result of\n  the Fermilab experiment, SM prediction unchanged; journal version"},{"id":"http://arxiv.org/abs/2509.09266v1","updated":"2025-09-11T08:52:22Z","published":"2025-09-11T08:52:22Z","title":"Determination of CKM matrix element and axial vector form factors from\n  weak decays of quantum-entangled strange baryons","summary":"  The electromagnetic structure of the nucleon can be determined from the\nscattering of electrons off a nucleon target. However, to study its axial\nstructure, neutrino beams are required. The results from these experiments\nshould be extrapolated to zero energy-momentum transfers to access the static\nproperties of the nucleon. For baryons with strange quarks, hyperons, the\nstatic limit can instead be approached in semi-leptonic decays, which give\ndirect access to the weak magnetism and axial-vector coupling strengths that\nare inaccessible in electromagnetic interactions. The axial-vector coupling as\nwhile weak magnetism coupling and the overall normalization, given by form\nfactor $f_1$, are being determined with increased precision from the theory of\nstrong interactions using a first principles formulation on the space--time\nlattice. Furthermore, the probability of the semi-leptonic hyperon decay is\napproximately proportional to $|V_{us}|^2\\cdot (f_1^2+3g_1^2)$, where $V_{us}$\nis the CKM matrix element responsible for the transition between an $s$ and a\n$u$ quark. Current determinations of $|V_{us}|$ come from kaon decays, but the\nresults are not consistent and could indicate a deviation from CKM matrix\nunitarity, a tell-tale sign of physics beyond the Standard Model (SM) of\nelementary particles. Here we determine the absolute branching fraction and\nweak coupling strengths for $\\Lambda\\to p e^-\\bar\\nu_e$, and $\\bar \\Lambda\\to\n\\bar p e^+\\nu_e$. These observables combined with form factors determined from\nfirst-principle lattice QCD calculations allow for the extraction of the\n$|V_{us}|$ value. We demonstrate how $|V_{us}|$ can be extracted with\nincreasing sensitivity using polarized hyperons from entangled,\nbaryon-antibaryon pairs, thus enabling a complementary road to that of meson\ndecays. In addition, the presented experimental method can be used for other\nsemileptonic decays of baryons.\n","authors":[" BESIII Collaboration","M. Ablikim","M. N. Achasov","P. Adlarson","X. C. Ai","R. Aliberti","A. Amoroso","Q. An","Y. Bai","O. Bakina","Y. Ban","H. -R. Bao","V. Batozskaya","K. Begzsuren","N. Berger","M. Berlowski","M. Bertani","D. Bettoni","F. Bianchi","E. Bianco","A. Bortone","I. Boyko","R. A. Briere","A. Brueggemann","H. Cai","M. H. Cai","X. Cai","A. Calcaterra","G. F. Cao","N. Cao","S. A. Cetin","X. Y. Chai","J. F. Chang","G. R. Che","Y. Z. Che","C. H. Chen","Chao Chen","G. Chen","H. S. Chen","H. Y. Chen","M. L. Chen","S. J. Chen","S. L. Chen","S. M. Chen","T. Chen","X. R. Chen","X. T. Chen","X. Y. Chen","Y. B. Chen","Y. Q. Chen","Y. Q. Chen","Z. Chen","Z. J. Chen","Z. K. Chen","S. K. Choi","X. Chu","G. Cibinetto","F. Cossio","J. Cottee-Meldrum","J. J. Cui","H. L. Dai","J. P. Dai","A. Dbeyssi","R. E. de Boer","D. Dedovich","C. Q. Deng","Z. Y. Deng","A. Denig","I. Denysenko","M. Destefanis","F. De Mori","B. Ding","X. X. Ding","Y. Ding","Y. Ding","Y. X. Ding","J. Dong","L. Y. Dong","M. Y. Dong","X. Dong","M. C. Du","S. X. Du","S. X. Du","Y. Y. Duan","Z. H. Duan","P. Egorov","G. F. Fan","J. J. Fan","Y. H. Fan","J. Fang","J. Fang","S. S. Fang","W. X. Fang","Y. Q. Fang","L. Fava","F. Feldbauer","G. Felici","C. Q. Feng","J. H. Feng","L. Feng","Q. X. Feng","Y. T. Feng","M. Fritsch","C. D. Fu","J. L. Fu","Y. W. Fu","H. Gao","X. B. Gao","Y. Gao","Y. N. Gao","Y. N. Gao","Y. Y. Gao","S. Garbolino","I. Garzia","L. Ge","P. T. Ge","Z. W. Ge","C. Geng","E. M. Gersabeck","A. Gilman","K. Goetzen","J. D. Gong","L. Gong","W. X. Gong","W. Gradl","S. Gramigna","M. Greco","M. H. Gu","Y. T. Gu","C. Y. Guan","A. Q. Guo","L. B. Guo","M. J. Guo","R. P. Guo","Y. P. Guo","A. Guskov","J. Gutierrez","K. L. Han","T. T. Han","F. Hanisch","K. D. Hao","X. Q. Hao","F. A. Harris","K. K. He","K. L. He","F. H. Heinsius","C. H. Heinz","Y. K. Heng","C. Herold","P. C. Hong","G. Y. Hou","X. T. Hou","Y. R. Hou","Z. L. Hou","H. M. Hu","J. F. Hu","Q. P. Hu","S. L. Hu","T. Hu","Y. Hu","Z. M. Hu","G. S. Huang","K. X. Huang","L. Q. Huang","P. Huang","X. T. Huang","Y. P. Huang","Y. S. Huang","T. Hussain","N. Hüsken","N. in der Wiesche","J. Jackson","Q. Ji","Q. P. Ji","W. Ji","X. B. Ji","X. L. Ji","Y. Y. Ji","Z. K. Jia","D. Jiang","H. B. Jiang","P. C. Jiang","S. J. Jiang","T. J. Jiang","X. S. Jiang","Y. Jiang","J. B. Jiao","J. K. Jiao","Z. Jiao","S. Jin","Y. Jin","M. Q. Jing","X. M. Jing","T. Johansson","S. Kabana","N. Kalantar-Nayestanaki","X. L. Kang","X. S. Kang","M. Kavatsyuk","B. C. Ke","V. Khachatryan","A. Khoukaz","R. Kiuchi","O. B. Kolcu","B. Kopf","M. Kuessner","X. Kui","N. Kumar","A. Kupsc","W. Kühn","Q. Lan","W. N. Lan","T. T. Lei","M. Lellmann","T. Lenz","C. Li","C. Li","C. H. Li","C. K. Li","D. M. Li","F. Li","G. Li","H. B. Li","H. J. Li","H. N. Li","Hui Li","J. R. Li","J. S. Li","K. Li","K. L. Li","K. L. Li","L. J. Li","Lei Li","M. H. Li","M. R. Li","P. L. Li","P. R. Li","Q. M. Li","Q. X. Li","R. Li","S. X. Li","T. Li","T. Y. Li","W. D. Li","W. G. Li","X. Li","X. H. Li","X. L. Li","X. Y. Li","X. Z. Li","Y. Li","Y. G. Li","Y. P. Li","Z. J. Li","Z. Y. Li","C. Liang","H. Liang","Y. F. Liang","Y. T. Liang","G. R. Liao","L. B. Liao","M. H. Liao","Y. P. Liao","J. Libby","A. Limphirat","C. C. Lin","D. X. Lin","L. Q. Lin","T. Lin","B. J. Liu","B. X. Liu","C. Liu","C. X. Liu","F. Liu","F. H. Liu","Feng Liu","G. M. Liu","H. Liu","H. B. Liu","H. H. Liu","H. M. Liu","Huihui Liu","J. B. Liu","J. J. Liu","K. Liu","K. Liu","K. Y. Liu","Ke Liu","L. C. Liu","Lu Liu","M. H. Liu","M. H. Liu","P. L. Liu","Q. Liu","S. B. Liu","T. Liu","W. K. Liu","W. M. Liu","W. T. Liu","X. Liu","X. Liu","X. K. Liu","X. L. Liu","X. Y. Liu","Y. Liu","Y. Liu","Y. Liu","Y. B. Liu","Z. A. Liu","Z. D. Liu","Z. Q. Liu","X. C. Lou","F. X. Lu","H. J. Lu","J. G. Lu","X. L. Lu","Y. Lu","Y. H. Lu","Y. P. Lu","Z. H. Lu","C. L. Luo","J. R. Luo","J. S. Luo","M. X. Luo","T. Luo","X. L. Luo","Z. Y. Lv","X. R. Lyu","Y. F. Lyu","Y. H. Lyu","F. C. Ma","H. L. Ma","Heng Ma","J. L. Ma","L. L. Ma","L. R. Ma","Q. M. Ma","R. Q. Ma","R. Y. Ma","T. Ma","X. T. Ma","X. Y. Ma","Y. M. Ma","F. E. Maas","I. MacKay","M. Maggiora","S. Malde","Q. A. Malik","H. X. Mao","Y. J. Mao","Z. P. Mao","S. Marcello","A. Marshall","F. M. Melendi","Y. H. Meng","Z. X. Meng","G. Mezzadri","H. Miao","T. J. Min","R. E. Mitchell","X. H. Mo","B. Moses","N. Yu. Muchnoi","J. Muskalla","Y. Nefedov","F. Nerling","L. S. Nie","I. B. Nikolaev","Z. Ning","S. Nisar","Q. L. Niu","W. D. Niu","C. Normand","S. L. Olsen","Q. Ouyang","S. Pacetti","X. Pan","Y. Pan","A. Pathak","Y. P. Pei","M. Pelizaeus","H. P. Peng","X. J. Peng","Y. Y. Peng","K. Peters","K. Petridis","J. L. Ping","R. G. Ping","S. Plura","V. Prasad","F. Z. Qi","H. R. Qi","M. Qi","S. Qian","W. B. Qian","C. F. Qiao","J. H. Qiao","J. J. Qin","J. L. Qin","L. Q. Qin","L. Y. Qin","P. B. Qin","X. P. Qin","X. S. Qin","Z. H. Qin","J. F. Qiu","Z. H. Qu","J. Rademacker","C. F. Redmer","A. Rivetti","M. Rolo","G. Rong","S. S. Rong","F. Rosini","Ch. Rosner","M. Q. Ruan","N. Salone","A. Sarantsev","Y. Schelhaas","K. Schoenning","M. Scodeggio","K. Y. Shan","W. Shan","X. Y. Shan","Z. J. Shang","J. F. Shangguan","L. G. Shao","M. Shao","C. P. Shen","H. F. Shen","W. H. Shen","X. Y. Shen","B. A. Shi","H. Shi","J. L. Shi","J. Y. Shi","S. Y. Shi","X. Shi","H. L. Song","J. J. Song","T. Z. Song","W. M. Song","Y. J. Song","Y. X. Song","Zirong Song","S. Sosio","S. Spataro","S Stansilaus","F. Stieler","S. S Su","Y. J. Su","G. B. Sun","G. X. Sun","H. Sun","H. K. Sun","J. F. Sun","K. Sun","L. Sun","S. S. Sun","T. Sun","Y. C. Sun","Y. H. Sun","Y. J. Sun","Y. Z. Sun","Z. Q. Sun","Z. T. Sun","C. J. Tang","G. Y. Tang","J. Tang","J. J. Tang","L. F. Tang","Y. A. Tang","L. Y. Tao","M. Tat","J. X. Teng","J. Y. Tian","W. H. Tian","Y. Tian","Z. F. Tian","I. Uman","B. Wang","B. Wang","Bo Wang","C. Wang","C. Wang","Cong Wang","D. Y. Wang","H. J. Wang","J. J. Wang","K. Wang","L. L. Wang","L. W. Wang","M. Wang","M. Wang","N. Y. Wang","Shun Wang","T. Wang","T. J. Wang","W. Wang","W. Wang","W. P. Wang","X. Wang","X. F. Wang","X. J. Wang","X. L. Wang","X. N. Wang","Y. Wang","Y. D. Wang","Y. F. Wang","Y. H. Wang","Y. J. Wang","Y. L. Wang","Y. N. Wang","Y. Q. Wang","Yaqian Wang","Yi Wang","Yuan Wang","Z. Wang","Z. L. Wang","Z. L. Wang","Z. Q. Wang","Z. Y. Wang","D. H. Wei","H. R. Wei","F. Weidner","S. P. Wen","Y. R. Wen","U. Wiedner","G. Wilkinson","M. Wolke","C. Wu","J. F. Wu","L. H. Wu","L. J. Wu","L. J. Wu","Lianjie Wu","S. G. Wu","S. M. Wu","X. Wu","X. H. Wu","Y. J. Wu","Z. Wu","L. Xia","X. M. Xian","B. H. Xiang","D. Xiao","G. Y. Xiao","H. Xiao","Y. L. Xiao","Z. J. Xiao","C. Xie","K. J. Xie","X. H. Xie","Y. Xie","Y. G. Xie","Y. H. Xie","Z. P. Xie","T. Y. Xing","C. F. Xu","C. J. Xu","G. F. Xu","H. Y. Xu","H. Y. Xu","M. Xu","Q. J. Xu","Q. N. Xu","T. D. Xu","W. Xu","W. L. Xu","X. P. Xu","Y. Xu","Y. Xu","Y. C. Xu","Z. S. Xu","F. Yan","H. Y. Yan","L. Yan","W. B. Yan","W. C. Yan","W. H. Yan","W. P. Yan","X. Q. Yan","H. J. Yang","H. L. Yang","H. X. Yang","J. H. Yang","R. J. Yang","T. Yang","Y. Yang","Y. F. Yang","Y. H. Yang","Y. Q. Yang","Y. X. Yang","Y. Z. Yang","M. Ye","M. H. Ye","Z. J. Ye","Junhao Yin","Z. Y. You","B. X. Yu","C. X. Yu","G. Yu","J. S. Yu","L. Q. Yu","M. C. Yu","T. Yu","X. D. Yu","Y. C. Yu","C. Z. Yuan","H. Yuan","J. Yuan","J. Yuan","L. Yuan","S. C. Yuan","S. H. Yuan","X. Q. Yuan","Y. Yuan","Z. Y. Yuan","C. X. Yue","Ying Yue","A. A. Zafar","S. H. Zeng","X. Zeng","Y. Zeng","Y. J. Zeng","Y. J. Zeng","X. Y. Zhai","Y. H. Zhan"," Zhang","A. Q. Zhang","B. L. Zhang","B. X. Zhang","D. H. Zhang","G. Y. Zhang","G. Y. Zhang","H. Zhang","H. Zhang","H. C. Zhang","H. H. Zhang","H. Q. Zhang","H. R. Zhang","H. Y. Zhang","J. Zhang","J. Zhang","J. J. Zhang","J. L. Zhang","J. Q. Zhang","J. S. Zhang","J. W. Zhang","J. X. Zhang","J. Y. Zhang","J. Z. Zhang","Jianyu Zhang","L. M. Zhang","Lei Zhang","N. Zhang","P. Zhang","Q. Zhang","Q. Y. Zhang","R. Y. Zhang","S. H. Zhang","Shulei Zhang","X. M. Zhang","X. Y Zhang","X. Y. Zhang","Y. Zhang","Y. Zhang","Y. T. Zhang","Y. H. Zhang","Y. M. Zhang","Y. P. Zhang","Z. D. Zhang","Z. H. Zhang","Z. L. Zhang","Z. L. Zhang","Z. X. Zhang","Z. Y. Zhang","Z. Y. Zhang","Z. Z. Zhang","Zh. Zh. Zhang","G. Zhao","J. Y. Zhao","J. Z. Zhao","L. Zhao","L. Zhao","M. G. Zhao","N. Zhao","R. P. Zhao","S. J. Zhao","Y. B. Zhao","Y. L. Zhao","Y. X. Zhao","Z. G. Zhao","A. Zhemchugov","B. Zheng","B. M. Zheng","J. P. Zheng","W. J. Zheng","X. R. Zheng","Y. H. Zheng","B. Zhong","C. Zhong","H. Zhou","J. Q. Zhou","J. Y. Zhou","S. Zhou","X. Zhou","X. K. Zhou","X. R. Zhou","X. Y. Zhou","Y. X. Zhou","Y. Z. Zhou","A. N. Zhu","J. Zhu","K. Zhu","K. J. Zhu","K. S. Zhu","L. Zhu","L. X. Zhu","S. H. Zhu","T. J. Zhu","W. D. Zhu","W. D. Zhu","W. J. Zhu","W. Z. Zhu","Y. C. Zhu","Z. A. Zhu","X. Y. Zhuang","J. H. Zou","J. Zu"],"pdf_url":"https://arxiv.org/pdf/2509.09266v1.pdf","comment":"24 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.10316v2","updated":"2025-09-11T07:38:58Z","published":"2025-06-12T03:01:28Z","title":"Search for sub-GeV invisible particles in inclusive decays of $J/ψ$\n  to $φ$","summary":"  A search for an invisible particle, $X$, with a mass between 0 and 0.96\n$\\textrm{GeV}/\\textit{c}^{2}$, is performed in the process\n$J/\\psi\\rightarrow\\phi + X$ using $(8774.0\\pm39.4)\\times10^{6}$ $J/\\psi$ events\ncollected with the BESIII detector from 2017 to 2019. The $\\phi$ meson is fully\nreconstructed and an efficient veto of photons, neutral and charged hadrons up\nto twice the $K_L^0$ mass is applied to the rest of the event and the recoil\nmass against the $\\phi$ is obtained precisely from the kinematic constraint in\nthe event. No significant signal over the expected background is observed in\nthe investigated region and the upper limit on the inclusive branching fraction\nof $J/\\psi\\rightarrow\\phi + X$ is determined to be $7.0\\times10^{-8}$ at 90\\%\nconfidence level. Upper limits at a 90\\% confidence level are also given for\nthis branching fraction as a function of the invisible particle mass, varying\nfrom $4\\times10^{-9}$ to $4\\times10^{-8}$ over the investigated mass range.\nAdditionally, a 90\\% confidence level upper limit on the branching fraction of\n$\\eta\\rightarrow \\rm{invisible}$ is determined to $2.4\\times10^{-5}$, which\nimproves the previous best results by more than four times. The analysis\ntechnique in this work offers a clean window to search for sub-GeV invisible\nparticles, which can be adapted for other $J/\\psi$ decays and direct $e^+e^-$\nannihilation experiments in future studies, and improve the sensitivity by\norders of magnitude.\n","authors":[" BESIII Collaboration","M. Ablikim","M. N. Achasov","P. Adlarson","X. C. Ai","R. Aliberti","A. Amoroso","Q. An","Y. Bai","O. Bakina","Y. Ban","H. -R. Bao","V. Batozskaya","K. Begzsuren","N. Berger","M. Berlowski","M. Bertani","D. Bettoni","F. Bianchi","E. Bianco","A. Bortone","I. Boyko","R. A. Briere","A. Brueggemann","H. Cai","M. H. Cai","X. Cai","A. Calcaterra","G. F. Cao","N. Cao","S. A. Cetin","X. Y. Chai","J. F. Chang","G. R. Che","Y. Z. Che","G. Chelkov","C. H. Chen","Chao Chen","G. Chen","H. S. Chen","H. Y. Chen","M. L. Chen","S. J. Chen","S. L. Chen","S. M. Chen","T. Chen","X. R. Chen","X. T. Chen","X. Y. Chen","Y. B. Chen","Y. Q. Chen","Y. Q. Chen","Z. J. Chen","Z. K. Chen","S. K. Choi","X. Chu","G. Cibinetto","F. Cossio","J. Cottee-Meldrum","J. J. Cui","H. L. Dai","J. P. Dai","A. Dbeyssi","R. E. de Boer","D. Dedovich","C. Q. Deng","Z. Y. Deng","A. Denig","I. Denysenko","M. Destefanis","F. De Mori","B. Ding","X. X. Ding","Y. Ding","Y. Ding","Y. X. Ding","J. Dong","L. Y. Dong","M. Y. Dong","X. Dong","M. C. Du","S. X. Du","S. X. Du","Y. Y. Duan","Z. H. Duan","P. Egorov","G. F. Fan","J. J. Fan","Y. H. Fan","J. Fang","J. Fang","S. S. Fang","W. X. Fang","Y. Q. Fang","R. Farinelli","L. Fava","F. Feldbauer","G. Felici","C. Q. Feng","J. H. Feng","L. Feng","Q. X. Feng","Y. T. Feng","M. Fritsch","C. D. Fu","J. L. Fu","Y. W. Fu","H. Gao","X. B. Gao","Y. Gao","Y. N. Gao","Y. N. Gao","Y. Y. Gao","S. Garbolino","I. Garzia","P. T. Ge","Z. W. Ge","C. Geng","E. M. Gersabeck","A. Gilman","K. Goetzen","J. D. Gong","L. Gong","W. X. Gong","W. Gradl","S. Gramigna","M. Greco","M. H. Gu","Y. T. Gu","C. Y. Guan","A. Q. Guo","L. B. Guo","M. J. Guo","R. P. Guo","Y. P. Guo","A. Guskov","J. Gutierrez","K. L. Han","T. T. Han","F. Hanisch","K. D. Hao","X. Q. Hao","F. A. Harris","K. K. He","K. L. He","F. H. Heinsius","C. H. Heinz","Y. K. Heng","C. Herold","T. Holtmann","P. C. Hong","G. Y. Hou","X. T. Hou","Y. R. Hou","Z. L. Hou","H. M. Hu","J. F. Hu","Q. P. Hu","S. L. Hu","T. Hu","Y. Hu","Z. M. Hu","G. S. Huang","K. X. Huang","L. Q. Huang","P. Huang","X. T. Huang","Y. P. Huang","Y. S. Huang","T. Hussain","N. Hüsken","N. in der Wiesche","J. Jackson","Q. Ji","Q. P. Ji","W. Ji","X. B. Ji","X. L. Ji","Y. Y. Ji","Z. K. Jia","D. Jiang","H. B. Jiang","P. C. Jiang","S. J. Jiang","T. J. Jiang","X. S. Jiang","Y. Jiang","J. B. Jiao","J. K. Jiao","Z. Jiao","S. Jin","Y. Jin","M. Q. Jing","X. M. Jing","T. Johansson","S. Kabana","N. Kalantar-Nayestanaki","X. L. Kang","X. S. Kang","M. Kavatsyuk","B. C. Ke","V. Khachatryan","A. Khoukaz","R. Kiuchi","O. B. Kolcu","B. Kopf","M. Kuessner","X. Kui","N. Kumar","A. Kupsc","W. Kühn","Q. Lan","W. N. Lan","T. T. Lei","M. Lellmann","T. Lenz","C. Li","C. Li","C. Li","C. H. Li","C. K. Li","D. M. Li","F. Li","G. Li","H. B. Li","H. J. Li","H. N. Li","Hui Li","J. R. Li","J. S. Li","K. Li","K. L. Li","K. L. Li","L. J. Li","Lei Li","M. H. Li","M. R. Li","P. L. Li","P. R. Li","Q. M. Li","Q. X. Li","R. Li","S. X. Li","T. Li","T. Y. Li","W. D. Li","W. G. Li","X. Li","X. H. Li","X. L. Li","X. Y. Li","X. Z. Li","Y. Li","Y. G. Li","Y. P. Li","Z. J. Li","Z. Y. Li","C. Liang","H. Liang","Y. F. Liang","Y. T. Liang","G. R. Liao","L. B. Liao","M. H. Liao","Y. P. Liao","J. Libby","A. Limphirat","C. C. Lin","C. X. Lin","D. X. Lin","L. Q. Lin","T. Lin","B. J. Liu","B. X. Liu","C. Liu","C. X. Liu","F. Liu","F. H. Liu","Feng Liu","G. M. Liu","H. Liu","H. B. Liu","H. H. Liu","H. M. Liu","Huihui Liu","J. B. Liu","J. J. Liu","K. Liu","K. Liu","K. Y. Liu","Ke Liu","L. Liu","L. C. Liu","Lu Liu","M. H. Liu","P. L. Liu","Q. Liu","S. B. Liu","T. Liu","W. K. Liu","W. M. Liu","W. T. Liu","X. Liu","X. Liu","X. K. Liu","X. Y. Liu","Y. Liu","Y. Liu","Y. Liu","Y. B. Liu","Z. A. Liu","Z. D. Liu","Z. Q. Liu","X. C. Lou","F. X. Lu","H. J. Lu","J. G. Lu","X. L. Lu","Y. Lu","Y. H. Lu","Y. P. Lu","Z. H. Lu","C. L. Luo","J. R. Luo","J. S. Luo","M. X. Luo","T. Luo","X. L. Luo","Z. Y. Lv","X. R. Lyu","Y. F. Lyu","Y. H. Lyu","F. C. Ma","H. Ma","H. L. Ma","J. L. Ma","L. L. Ma","L. R. Ma","Q. M. Ma","R. Q. Ma","R. Y. Ma","T. Ma","X. T. Ma","X. Y. Ma","Y. M. Ma","F. E. Maas","I. MacKay","M. Maggiora","S. Malde","Q. A. Malik","H. X. Mao","Y. J. Mao","Z. P. Mao","S. Marcello","A. Marshall","F. M. Melendi","Y. H. Meng","Z. X. Meng","J. G. Messchendorp","G. Mezzadri","H. Miao","T. J. Min","R. E. Mitchell","X. H. Mo","B. Moses","N. Yu. Muchnoi","J. Muskalla","Y. Nefedov","F. Nerling","L. S. Nie","I. B. Nikolaev","Z. Ning","S. Nisar","Q. L. Niu","W. D. Niu","C. Normand","S. L. Olsen","Q. Ouyang","S. Pacetti","X. Pan","Y. Pan","A. Pathak","Y. P. Pei","M. Pelizaeus","H. P. Peng","X. J. Peng","Y. Y. Peng","K. Peters","K. Petridis","J. L. Ping","R. G. Ping","S. Plura","V. Prasad","F. Z. Qi","H. R. Qi","M. Qi","S. Qian","W. B. Qian","C. F. Qiao","J. H. Qiao","J. J. Qin","J. L. Qin","L. Q. Qin","L. Y. Qin","P. B. Qin","X. P. Qin","X. S. Qin","Z. H. Qin","J. F. Qiu","Z. H. Qu","J. Rademacker","C. F. Redmer","A. Rivetti","M. Rolo","G. Rong","S. S. Rong","F. Rosini","Ch. Rosner","M. Q. Ruan","N. Salone","A. Sarantsev","Y. Schelhaas","K. Schoenning","M. Scodeggio","K. Y. Shan","W. Shan","X. Y. Shan","Z. J. Shang","J. F. Shangguan","L. G. Shao","M. Shao","C. P. Shen","H. F. Shen","W. H. Shen","X. Y. Shen","B. A. Shi","H. Shi","J. L. Shi","J. Y. Shi","S. Y. Shi","X. Shi","H. L. Song","J. J. Song","T. Z. Song","W. M. Song","Y. J. Song","Y. X. Song","S. Sosio","S. Spataro","F. Stieler","S. S Su","Y. J. Su","G. B. Sun","G. X. Sun","H. Sun","H. K. Sun","J. F. Sun","K. Sun","L. Sun","S. S. Sun","T. Sun","Y. C. Sun","Y. H. Sun","Y. J. Sun","Y. Z. Sun","Z. Q. Sun","Z. T. Sun","C. J. Tang","G. Y. Tang","J. Tang","J. J. Tang","L. F. Tang","Y. A. Tang","L. Y. Tao","M. Tat","J. X. Teng","J. Y. Tian","W. H. Tian","Y. Tian","Z. F. Tian","I. Uman","B. Wang","B. Wang","Bo Wang","C. Wang","C. Wang","Cong Wang","D. Y. Wang","H. J. Wang","J. J. Wang","K. Wang","L. L. Wang","L. W. Wang","M. Wang","M. Wang","N. Y. Wang","S. Wang","T. Wang","T. J. Wang","W. Wang","W. Wang","W. P. Wang","X. Wang","X. F. Wang","X. J. Wang","X. L. Wang","X. N. Wang","Y. Wang","Y. D. Wang","Y. F. Wang","Y. H. Wang","Y. J. Wang","Y. L. Wang","Y. N. Wang","Y. Q. Wang","Yaqian Wang","Yi Wang","Yuan Wang","Z. Wang","Z. L. Wang","Z. L. Wang","Z. Q. Wang","Z. Y. Wang","D. H. Wei","H. R. Wei","F. Weidner","S. P. Wen","Y. R. Wen","U. Wiedner","G. Wilkinson","M. Wolke","C. Wu","J. F. Wu","L. H. Wu","L. J. Wu","L. J. Wu","Lianjie Wu","S. G. Wu","S. M. Wu","X. Wu","X. H. Wu","Y. J. Wu","Z. Wu","L. Xia","X. M. Xian","B. H. Xiang","D. Xiao","G. Y. Xiao","H. Xiao","Y. L. Xiao","Z. J. Xiao","C. Xie","K. J. Xie","X. H. Xie","Y. Xie","Y. G. Xie","Y. H. Xie","Z. P. Xie","T. Y. Xing","C. F. Xu","C. J. Xu","G. F. Xu","H. Y. Xu","H. Y. Xu","M. Xu","Q. J. Xu","Q. N. Xu","T. D. Xu","W. Xu","W. L. Xu","X. P. Xu","Y. Xu","Y. Xu","Y. C. Xu","Z. S. Xu","F. Yan","H. Y. Yan","L. Yan","W. B. Yan","W. C. Yan","W. H. Yan","W. P. Yan","X. Q. Yan","H. J. Yang","H. L. Yang","H. X. Yang","J. H. Yang","R. J. Yang","T. Yang","Y. Yang","Y. F. Yang","Y. H. Yang","Y. Q. Yang","Y. X. Yang","Y. Z. Yang","M. Ye","M. H. Ye","Z. J. Ye","Junhao Yin","Z. Y. You","B. X. Yu","C. X. Yu","G. Yu","J. S. Yu","L. Q. Yu","M. C. Yu","T. Yu","X. D. Yu","Y. C. Yu","C. Z. Yuan","H. Yuan","J. Yuan","J. Yuan","L. Yuan","S. C. Yuan","X. Q. Yuan","Y. Yuan","Z. Y. Yuan","C. X. Yue","Ying Yue","A. A. Zafar","S. H. Zeng","X. Zeng","Y. Zeng","Y. J. Zeng","Y. J. Zeng","X. Y. Zhai","Y. H. Zhan","A. Q. Zhang","B. L. Zhang","B. X. Zhang","D. H. Zhang","G. Y. Zhang","G. Y. Zhang","H. Zhang","H. Zhang","H. C. Zhang","H. H. Zhang","H. Q. Zhang","H. R. Zhang","H. Y. Zhang","J. Zhang","J. Zhang","J. J. Zhang","J. L. Zhang","J. Q. Zhang","J. S. Zhang","J. W. Zhang","J. X. Zhang","J. Y. Zhang","J. Z. Zhang","Jianyu Zhang","L. M. Zhang","Lei Zhang","N. Zhang","P. Zhang","Q. Zhang","Q. Y. Zhang","R. Y. Zhang","S. H. Zhang","Shulei Zhang","X. M. Zhang","X. Y Zhang","X. Y. Zhang","Y. Zhang","Y. Zhang","Y. T. Zhang","Y. H. Zhang","Y. M. Zhang","Y. P. Zhang","Z. D. Zhang","Z. H. Zhang","Z. L. Zhang","Z. L. Zhang","Z. X. Zhang","Z. Y. Zhang","Z. Y. Zhang","Z. Z. Zhang","Zh. Zh. Zhang","G. Zhao","J. Y. Zhao","J. Z. Zhao","L. Zhao","L. Zhao","M. G. Zhao","N. Zhao","R. P. Zhao","S. J. Zhao","Y. B. Zhao","Y. L. Zhao","Y. X. Zhao","Z. G. Zhao","A. Zhemchugov","B. Zheng","B. M. Zheng","J. P. Zheng","W. J. Zheng","X. R. Zheng","Y. H. Zheng","B. Zhong","C. Zhong","H. Zhou","J. Q. Zhou","J. Y. Zhou","S. Zhou","X. Zhou","X. K. Zhou","X. R. Zhou","X. Y. Zhou","Y. X. Zhou","Y. Z. Zhou","A. N. Zhu","J. Zhu","K. Zhu","K. J. Zhu","K. S. Zhu","L. Zhu","L. X. Zhu","S. H. Zhu","T. J. Zhu","W. D. Zhu","W. D. Zhu","W. J. Zhu","W. Z. Zhu","Y. C. Zhu","Z. A. Zhu","X. Y. Zhuang","J. H. Zou","J. Zu"],"pdf_url":"https://arxiv.org/pdf/2506.10316v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2509.09187v1","updated":"2025-09-11T06:55:23Z","published":"2025-09-11T06:55:23Z","title":"TCT-based monitoring of LGAD radiation hardness for ATLAS-HGTD\n  production","summary":"  Production of the High Granularity Timing Detector for the ATLAS experiment\nat High Luminosity LHC requires over 21000 silicon sensors based on Low Gain\nAvalanche Diode (LGAD) technology. Their radiation hardness is monitored as a\npart of the production quality control. Dedicated test structures from each\nwafer are irradiated with neutrons and a fast and comprehensive\ncharacterization is required. We introduce a new test method based on Transient\nCurrent Technique (TCT) performed in the interface region of two LGAD devices.\nThe measurement enables extraction of numerous sensor performance parameters,\nsuch as LGAD gain layer depletion voltage, LGAD gain dependence on bias\nvoltage, sensor leakage current and effective interpad distance. Complementary\ncapacitance-voltage measurements and charge collection measurements with 90Sr\non the same samples have been performed to calibrate the TCT results in terms\nof charge collection and define acceptance criteria for wafer radiation\nhardness in the ATLAS-HGTD project.\n","authors":["Iskra Velkovska","Bojan Hiti"],"pdf_url":"https://arxiv.org/pdf/2509.09187v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2509.09156v1","updated":"2025-09-11T05:29:12Z","published":"2025-09-11T05:29:12Z","title":"Observation of $ψ(3686)\\to γη(1405)$ via $η(1405)\\to\n  f_0(980)π^0$","summary":"  The decay $\\psi(3686)\\to\\gamma\\pi^+\\pi^-\\pi^0$ is studied using a sample of\n$(2712.4\\pm14.3)\\times10^6$ $\\psi(3686)$ events collected with the BESIII\ndetector. The decay $\\eta(1405)\\to\\pi^+\\pi^-\\pi^0$ is observed for the first\ntime in $\\psi(3686)$ decays via the intermediate state $f_0(980)$ and the\nproduct branching fraction\n$\\mathcal{B}(\\psi(3686)\\to\\gamma\\eta(1405))\\times\\mathcal{B}(\\eta(1405)\\to\nf_0(980)\\pi^0)\\times \\mathcal{B}(f_0(980)\\to\\pi^+\\pi^-)$ is determined to be\n$(3.77\\pm0.43\\pm0.29)\\times10^{-7}$, where the first uncertainty is statistical\nand the second is systematic. The isospin-violating decay of\n$\\psi(3686)\\to\\gamma f_1(1285)\\to\\gamma f_0(980)\\pi^0\\to\\gamma\\pi^+\\pi^-\\pi^0$\nhas been observed with signal significance of $2.9\\sigma$. And the branching\nfraction $\\mathcal{B}(\\psi(3686)\\to\\gamma f_1(1285)\\to\\gamma\nf_0(980)\\pi^0\\to\\gamma\\pi^+\\pi^-\\pi^0)$ is determined to be $\n(7.36\\pm2.25\\pm2.26)\\times 10^{-8}$. Since no $\\eta_c$ signal is evident in\neither the $\\pi^+\\pi^-\\pi^0$ or $f_0(980)\\pi^0$ mass spectrum, upper limits are\nset to be\n$\\mathcal{B}(\\psi(3686)\\to\\gamma\\eta_c)\\times\\mathcal{B}(\\eta_c\\to\\pi^+\\pi^-\\pi^0)<3.09\\times10^{-7}$\nand $\\mathcal{B}(\\psi(3686)\\to\\gamma\\eta_c)\\times\\mathcal{B}(\\eta_c\\to\nf_0(980)\\pi^0)\\times\\mathcal{B}(f_0(980)\\to\\pi^+\\pi^-)<7.97\\times10^{-8}$ at\n90\\% confidence level, respectively.\n","authors":["M. Ablikim","M. N. Achasov","P. Adlarson","X. C. Ai","R. Aliberti","A. Amoroso","Q. An","Y. Bai","O. Bakina","Y. Ban","H. -R. Bao","V. Batozskaya","K. Begzsuren","N. Berger","M. Berlowski","M. Bertani","D. Bettoni","F. Bianchi","E. Bianco","A. Bortone","I. Boyko","R. A. Briere","A. Brueggemann","H. Cai","M. H. Cai","X. Cai","A. Calcaterra","G. F. Cao","N. Cao","S. A. Cetin","X. Y. Chai","J. F. Chang","G. R. Che","Y. Z. Che","C. H. Chen","Chao Chen","G. Chen","H. S. Chen","H. Y. Chen","M. L. Chen","S. J. Chen","S. L. Chen","S. M. Chen","T. Chen","X. R. Chen","X. T. Chen","X. Y. Chen","Y. B. Chen","Y. Q. Chen","Y. Q. Chen","Z. Chen","Z. J. Chen","Z. K. Chen","S. K. Choi","X. Chu","G. Cibinetto","F. Cossio","J. Cottee-Meldrum","J. J. Cui","H. L. Dai","J. P. Dai","A. Dbeyssi","R. E. de Boer","D. Dedovich","C. Q. Deng","Z. Y. Deng","A. Denig","I. Denysenko","M. Destefanis","F. De Mori","B. Ding","X. X. Ding","Y. Ding","Y. Ding","Y. X. Ding","J. Dong","L. Y. Dong","M. Y. Dong","X. Dong","M. C. Du","S. X. Du","S. X. Du","Y. Y. Duan","P. Egorov","G. F. Fan","J. J. Fan","Y. H. Fan","J. Fang","J. Fang","S. S. Fang","W. X. Fang","Y. Q. Fang","R. Farinelli","L. Fava","F. Feldbauer","G. Felici","C. Q. Feng","J. H. Feng","L. Feng","Q. X. Feng","Y. T. Feng","M. Fritsch","C. D. Fu","J. L. Fu","Y. W. Fu","H. Gao","X. B. Gao","Y. Gao","Y. N. Gao","Y. N. Gao","Y. Y. Gao","S. Garbolino","I. Garzia","L. Ge","P. T. Ge","Z. W. Ge","C. Geng","E. M. Gersabeck","A. Gilman","K. Goetzen","J. D. Gong","L. Gong","W. X. Gong","W. Gradl","S. Gramigna","M. Greco","M. H. Gu","Y. T. Gu","C. Y. Guan","A. Q. Guo","L. B. Guo","M. J. Guo","R. P. Guo","Y. P. Guo","A. Guskov","J. Gutierrez","K. L. Han","T. T. Han","F. Hanisch","K. D. Hao","X. Q. Hao","F. A. Harris","K. K. He","K. L. He","F. H. Heinsius","C. H. Heinz","Y. K. Heng","C. Herold","P. C. Hong","G. Y. Hou","X. T. Hou","Y. R. Hou","Z. L. Hou","H. M. Hu","J. F. Hu","Q. P. Hu","S. L. Hu","T. Hu","Y. Hu","Z. M. Hu","G. S. Huang","K. X. Huang","L. Q. Huang","P. Huang","X. T. Huang","Y. P. Huang","Y. S. Huang","T. Hussain","N. Hüsken","N. in der Wiesche","J. Jackson","Q. Ji","Q. P. Ji","W. Ji","X. B. Ji","X. L. Ji","Y. Y. Ji","Z. K. Jia","D. Jiang","H. B. Jiang","P. C. Jiang","S. J. Jiang","T. J. Jiang","X. S. Jiang","Y. Jiang","J. B. Jiao","J. K. Jiao","Z. Jiao","S. Jin","Y. Jin","M. Q. Jing","X. M. Jing","T. Johansson","S. Kabana","N. Kalantar-Nayestanaki","X. L. Kang","X. S. Kang","M. Kavatsyuk","B. C. Ke","V. Khachatryan","A. Khoukaz","R. Kiuchi","O. B. Kolcu","B. Kopf","M. Kuessner","X. Kui","N. Kumar","A. Kupsc","W. Kühn","Q. Lan","W. N. Lan","T. T. Lei","M. Lellmann","T. Lenz","C. Li","C. Li","C. H. Li","C. K. Li","D. M. Li","F. Li","G. Li","H. B. Li","H. J. Li","H. N. Li","Hui Li","J. R. Li","J. S. Li","K. Li","K. L. Li","K. L. Li","L. J. Li","Lei Li","M. H. Li","M. R. Li","P. L. Li","P. R. Li","Q. M. Li","Q. X. Li","R. Li","S. X. Li","T. Li","T. Y. Li","W. D. Li","W. G. Li","X. Li","X. H. Li","X. L. Li","X. Y. Li","X. Z. Li","Y. Li","Y. G. Li","Y. P. Li","Z. J. Li","Z. Y. Li","H. Liang","Y. F. Liang","Y. T. Liang","G. R. Liao","L. B. Liao","M. H. Liao","Y. P. Liao","J. Libby","A. Limphirat","C. C. Lin","D. X. Lin","L. Q. Lin","T. Lin","B. J. Liu","B. X. Liu","C. Liu","C. X. Liu","F. Liu","F. H. Liu","Feng Liu","G. M. Liu","H. Liu","H. B. Liu","H. H. Liu","H. M. Liu","Huihui Liu","J. B. Liu","J. J. Liu","K. Liu","K. Liu","K. Y. Liu","Ke Liu","L. C. Liu","Lu Liu","M. H. Liu","P. L. Liu","Q. Liu","S. B. Liu","T. Liu","W. K. Liu","W. M. Liu","W. T. Liu","X. Liu","X. Liu","X. K. Liu","X. L. Liu","X. Y. Liu","Y. Liu","Y. Liu","Y. Liu","Y. B. Liu","Z. A. Liu","Z. D. Liu","Z. Q. Liu","X. C. Lou","F. X. Lu","H. J. Lu","J. G. Lu","X. L. Lu","Y. Lu","Y. H. Lu","Y. P. Lu","Z. H. Lu","C. L. Luo","J. R. Luo","J. S. Luo","M. X. Luo","T. Luo","X. L. Luo","Z. Y. Lv","X. R. Lyu","Y. F. Lyu","Y. H. Lyu","F. C. Ma","H. L. Ma","Heng Ma","J. L. Ma","L. L. Ma","L. R. Ma","Q. M. Ma","R. Q. Ma","R. Y. Ma","T. Ma","X. T. Ma","X. Y. Ma","Y. M. Ma","F. E. Maas","I. MacKay","M. Maggiora","S. Malde","Q. A. Malik","H. X. Mao","Y. J. Mao","Z. P. Mao","S. Marcello","A. Marshall","F. M. Melendi","Y. H. Meng","Z. X. Meng","G. Mezzadri","H. Miao","T. J. Min","R. E. Mitchell","X. H. Mo","B. Moses","N. Yu. Muchnoi","J. Muskalla","Y. Nefedov","F. Nerling","L. S. Nie","I. B. Nikolaev","Z. Ning","S. Nisar","Q. L. Niu","W. D. Niu","C. Normand","S. L. Olsen","Q. Ouyang","S. Pacetti","X. Pan","Y. Pan","A. Pathak","Y. P. Pei","M. Pelizaeus","H. P. Peng","X. J. Peng","Y. Y. Peng","K. Peters","K. Petridis","J. L. Ping","R. G. Ping","S. Plura","V. Prasad","F. Z. Qi","H. R. Qi","M. Qi","S. Qian","W. B. Qian","C. F. Qiao","J. H. Qiao","J. J. Qin","J. L. Qin","L. Q. Qin","L. Y. Qin","P. B. Qin","X. P. Qin","X. S. Qin","Z. H. Qin","J. F. Qiu","Z. H. Qu","J. Rademacker","C. F. Redmer","A. Rivetti","M. Rolo","G. Rong","S. S. Rong","F. Rosini","Ch. Rosner","M. Q. Ruan","N. Salone","A. Sarantsev","Y. Schelhaas","K. Schoenning","M. Scodeggio","K. Y. Shan","W. Shan","X. Y. Shan","Z. J. Shang","J. F. Shangguan","L. G. Shao","M. Shao","C. P. Shen","H. F. Shen","W. H. Shen","X. Y. Shen","B. A. Shi","H. Shi","J. L. Shi","J. Y. Shi","S. Y. Shi","X. Shi","H. L. Song","J. J. Song","T. Z. Song","W. M. Song","Y. J. Song","Y. X. Song","Zirong Song","S. Sosio","S. Spataro","S Stansilaus","F. Stieler","S. S Su","Y. J. Su","G. B. Sun","G. X. Sun","H. Sun","H. K. Sun","J. F. Sun","K. Sun","L. Sun","S. S. Sun","T. Sun","Y. C. Sun","Y. H. Sun","Y. J. Sun","Y. Z. Sun","Z. Q. Sun","Z. T. Sun","C. J. Tang","G. Y. Tang","J. Tang","J. J. Tang","L. F. Tang","Y. A. Tang","L. Y. Tao","M. Tat","J. X. Teng","J. Y. Tian","W. H. Tian","Y. Tian","Z. F. Tian","I. Uman","B. Wang","B. Wang","Bo Wang","C. Wang","C. Wang","Cong Wang","D. Y. Wang","H. J. Wang","J. J. Wang","K. Wang","L. L. Wang","L. W. Wang","M. Wang","M. Wang","N. Y. Wang","S. Wang","T. Wang","T. J. Wang","W. Wang","W. Wang","W. P. Wang","X. Wang","X. F. Wang","X. J. Wang","X. L. Wang","X. N. Wang","Y. Wang","Y. D. Wang","Y. F. Wang","Y. H. Wang","Y. J. Wang","Y. L. Wang","Y. N. Wang","Y. Q. Wang","Yaqian Wang","Yi Wang","Yuan Wang","Z. Wang","Z. L. Wang","Z. L. Wang","Z. Q. Wang","Z. Y. Wang","D. H. Wei","H. R. Wei","F. Weidner","S. P. Wen","Y. R. Wen","U. Wiedner","G. Wilkinson","M. Wolke","C. Wu","J. F. Wu","L. H. Wu","L. J. Wu","L. J. Wu","Lianjie Wu","S. G. Wu","S. M. Wu","X. Wu","X. H. Wu","Y. J. Wu","Z. Wu","L. Xia","X. M. Xian","B. H. Xiang","D. Xiao","G. Y. Xiao","H. Xiao","Y. L. Xiao","Z. J. Xiao","C. Xie","K. J. Xie","X. H. Xie","Y. Xie","Y. G. Xie","Y. H. Xie","Z. P. Xie","T. Y. Xing","C. F. Xu","C. J. Xu","G. F. Xu","H. Y. Xu","H. Y. Xu","M. Xu","Q. J. Xu","Q. N. Xu","T. D. Xu","W. Xu","W. L. Xu","X. P. Xu","Y. Xu","Y. Xu","Y. C. Xu","Z. S. Xu","F. Yan","H. Y. Yan","L. Yan","W. B. Yan","W. C. Yan","W. H. Yan","W. P. Yan","X. Q. Yan","H. J. Yang","H. L. Yang","H. X. Yang","J. H. Yang","R. J. Yang","T. Yang","Y. Yang","Y. F. Yang","Y. H. Yang","Y. Q. Yang","Y. X. Yang","Y. Z. Yang","M. Ye","M. H. Ye","Z. J. Ye","Junhao Yin","Z. Y. You","B. X. Yu","C. X. Yu","G. Yu","J. S. Yu","L. Q. Yu","M. C. Yu","T. Yu","X. D. Yu","Y. C. Yu","C. Z. Yuan","H. Yuan","J. Yuan","J. Yuan","L. Yuan","S. C. Yuan","X. Q. Yuan","Y. Yuan","Z. Y. Yuan","C. X. Yue","Ying Yue","A. A. Zafar","S. H. Zeng","X. Zeng","Y. Zeng","Y. J. Zeng","Y. J. Zeng","X. Y. Zhai","Y. H. Zhan"," Zhang","A. Q. Zhang","B. L. Zhang","B. X. Zhang","D. H. Zhang","G. Y. Zhang","G. Y. Zhang","H. Zhang","H. Zhang","H. C. Zhang","H. H. Zhang","H. Q. Zhang","H. R. Zhang","H. Y. Zhang","J. Zhang","J. Zhang","J. J. Zhang","J. L. Zhang","J. Q. Zhang","J. S. Zhang","J. W. Zhang","J. X. Zhang","J. Y. Zhang","J. Z. Zhang","Jianyu Zhang","L. M. Zhang","Lei Zhang","N. Zhang","P. Zhang","Q. Zhang","Q. Y. Zhang","R. Y. Zhang","S. H. Zhang","Shulei Zhang","X. M. Zhang","X. Y Zhang","X. Y. Zhang","Y. Zhang","Y. Zhang","Y. T. Zhang","Y. H. Zhang","Y. M. Zhang","Y. P. Zhang","Z. D. Zhang","Z. H. Zhang","Z. L. Zhang","Z. L. Zhang","Z. X. Zhang","Z. Y. Zhang","Z. Y. Zhang","Z. Z. Zhang","Zh. Zh. Zhang","G. Zhao","J. Y. Zhao","J. Z. Zhao","L. Zhao","L. Zhao","M. G. Zhao","N. Zhao","R. P. Zhao","S. J. Zhao","Y. B. Zhao","Y. L. Zhao","Y. X. Zhao","Z. G. Zhao","A. Zhemchugov","B. Zheng","B. M. Zheng","J. P. Zheng","W. J. Zheng","X. R. Zheng","Y. H. Zheng","B. Zhong","C. Zhong","H. Zhou","J. Q. Zhou","J. Y. Zhou","S. Zhou","X. Zhou","X. K. Zhou","X. R. Zhou","X. Y. Zhou","Y. X. Zhou","Y. Z. Zhou","A. N. Zhu","J. Zhu","K. Zhu","K. J. Zhu","K. S. Zhu","L. Zhu","L. X. Zhu","S. H. Zhu","T. J. Zhu","W. D. Zhu","W. D. Zhu","W. J. Zhu","W. Z. Zhu","Y. C. Zhu","Z. A. Zhu","X. Y. Zhuang","J. H. Zou","J. Zu"],"pdf_url":"https://arxiv.org/pdf/2509.09156v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.03156v6","updated":"2025-09-11T04:54:31Z","published":"2024-03-05T17:49:06Z","title":"The Fast Stochastic Matching Pursuit for Neutrino and Dark Matter\n  Experiments","summary":"  Photomultiplier tubes (PMTs) are widely deployed at neutrino and dark matter\nexperiments for photon counting. When multiple photons hit a PMT consecutively,\ntheir photo-electron (PE) pulses pile up to hinder the precise measurements of\nthe count and timings. We introduce Fast Stochastic Matching Pursuit (FSMP) to\nanalyze the PMT signal waveforms into individual PEs with the strategy of\nreversible-jump Markov-chain Monte Carlo. We demonstrate that FSMP improves the\nenergy and time resolution of PMT-based experiments and gains acceleration on\nGPUs. It is suitable for dynode PMTs, and is extensible to microchannel-plate\n(MCP) PMTs. In the condition of our laboratory characterization of 8-inch\nMCP-PMTs, FSMP improves the energy resolution by up to 10% from the\nconventional method of waveform integration.\n","authors":["Yuyi Wang","Aiqiang Zhang","Yiyang Wu","Benda Xu","Xuewei Liu","Jiajie Chen","Zhe Wang","Shaomin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.03156v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09062v1","updated":"2025-09-11T00:00:01Z","published":"2025-09-11T00:00:01Z","title":"R-parity violation and 8 TeV four-jet events at the LHC: a falsification\n  opportunity for Wagner's Rule","summary":"  The CMS Collaboration at the Large Hadron Collider (LHC) has observed two\nfour-jet events with a total invariant mass of about 8 TeV; within each event,\nthe jets can be paired into two dijets with invariant masses of 2 TeV each.\nThese are extremely rare events due to the large invariant mass, which implies\na very small QCD background, as well as to the di-jet structure, which makes it\nprone to an interpretation in terms of a heavy resonance decaying into two\nlighter ones. We investigate the possible interpretation of these events in\nterms of supersymmetry with a single baryon-number and R-Parity violating term.\nSuch an interpretation would be in accordance with Wagner's rule, which asserts\nthat any collider anomaly may be explained by low-energy Supersymmetry when\nR-Parity-violating couplings are allowed. In this particular scenario, the\nlighter resonances are identified with the right-handed squarks of the first\ngeneration, while the heavy one is interpreted in terms of a down-squark of the\nsecond or third generation. We discuss the constraints that shape this\ninterpretation and outline a well-defined scenario for its realization. The\nresulting predictions can be scrutinized with forthcoming LHC data.\n","authors":["Pedro Bittar","Subhojit Roy","Carlos E. M. Wagner"],"pdf_url":"https://arxiv.org/pdf/2509.09062v1.pdf","comment":"12 pages, 2 Figures, 2 Tables"}],"High Energy Physics - Phenomenology":[{"id":"http://arxiv.org/abs/2509.09678v1","updated":"2025-09-11T17:59:46Z","published":"2025-09-11T17:59:46Z","title":"Cosmic $τ$ensions Indirectly Correlate with Reionization Optical\n  Depth","summary":"  The reionization optical depth $\\tau_{\\rm reio}$ has interesting connections\nto existing cosmological anomalies. As first studied in the context of the\nHubble tension in our previous paper, a larger $\\tau_{\\rm reio}$, which could\nbe achieved by removing the Planck low-$\\ell$ polarization data, could boost\n$H_0$ slightly, resulting in a mild reduction of the tension between the early-\nand late-universe determinations of $H_0$. It has been shown later that a\nlarger $\\tau_{\\rm reio}$ could also relieve other anomalies including: the\ntension between BAO and CMB data, the neutrino mass tension, and the latest\nDESI plus supernovae data's tension with the standard cosmological constant\nscenario. In this paper, we systematically analyze the correlations between\n$\\tau_{\\rm reio}$ and relevant cosmological parameters in the existing cosmic\nobservation anomalies. In addition to Pearson correlation coefficients\nextracted directly from the covariance matrix, we also study partial\ncorrelation coefficients which measure intrinsic relationships between pairs of\nparameters removing the influence of other parameters. We show that $\\tau_{\\rm\nreio}$ has weak intrinsic correlations with the parameters responsible for the\ntensions and anomalies discussed. The large direct Pearson correlations that\nallow larger $\\tau_{\\rm reio}$ inferences to alleviate the cosmological\ntensions each arise from complicated networks through multiple parameters. As a\nresult, the relationships between $\\tau_{\\rm reio}$ and each anomaly are not\nindependent of each other. We also employ our method of computing correlations\nto clarify the impact of large scale polarization data, and comment also on the\neffects of CMB observations from ACT and SPT.\n","authors":["Itamar J. Allali","Lingfeng Li","Praniti Singh","JiJi Fan"],"pdf_url":"https://arxiv.org/pdf/2509.09678v1.pdf","comment":"19 pages, 12 figures, 4 tables, plus appendices"},{"id":"http://arxiv.org/abs/2412.21202v2","updated":"2025-09-11T17:57:55Z","published":"2024-12-30T18:59:33Z","title":"Two-component Dark Matter and low scale Thermal Leptogenesis","summary":"  The observable cosmos exhibits sizable baryon asymmetry, small active\nneutrino masses, and the presence of dark matter (DM). To address these\nphenomena together, we propose a two component DM scenario in an extension of\nScotogenic model, imposing $\\mathbb{Z}_2 \\otimes \\mathbb{Z}_2^{\\prime}$\nsymmetry. The electroweak sphaleron process converts the $\\rm Y_{B-L}^{}$\nyield, generated through the Leptogenesis mechanism, into the baryon asymmetry\n($\\rm Y_{\\Delta B}^{}$) at $\\rm T_{\\rm sph}\\sim 130$ GeV, the sphalerons\ndecoupling temperature. In this framework, the CP asymmetry as well as the\nradiative neutrino mass generation explicitly involve the two DM particles,\nthus establishing a correlation between the baryon asymmetry, DM and observed\nactive neutrino masses. We study in details the allowed parameter space\navailable after considering all the constraints from the three phenomena as\nwell as from the collider search limits, and outline the region which could\npotentially be tested in future DM detection experiments through direct or\nindirect detection searches, lepton flavor-violating decays, etc.\n","authors":["Subhaditya Bhattacharya","Devabrat Mahanta","Niloy Mondal","Dipankar Pradhan"],"pdf_url":"https://arxiv.org/pdf/2412.21202v2.pdf","comment":"30 pages, 14 figures"},{"id":"http://arxiv.org/abs/2509.09643v1","updated":"2025-09-11T17:31:12Z","published":"2025-09-11T17:31:12Z","title":"Ultralight Boson Ionization from Comparable-Mass Binary Black Holes","summary":"  Ultralight bosons around comparable-mass binaries can form gravitationally\nbound states analogous to molecules once the binary separation decreases below\nthe boson's Bohr radius, with the inner region co-moving with the binary. We\nsimulate the formation of these gravitational molecules, determine their\nco-moving regions, and compute ionization fluxes induced by orbital motion for\nvarious binary eccentricities. We develop semi-analytic formalisms to describe\nthe ionization dynamics of both the co-moving and non-co-moving regions,\ndemonstrating consistency with numerical simulation results. From ionization\nfluxes, we estimate their backreaction on binary orbital evolution. At early\nstages, molecule ionization can dominate over gravitational wave emission,\nproducing a spectral turnover in the gravitational wave background.\nAdditionally, ionization of the co-moving component occurs solely due to binary\neccentricity, causing orbital circularization.\n","authors":["Yuhao Guo","Zhen Zhong","Yifan Chen","Vitor Cardoso","Taishi Ikeda","Lihang Zhou"],"pdf_url":"https://arxiv.org/pdf/2509.09643v1.pdf","comment":"18 pages, 5 figures, movie:\n  https://www.bilibili.com/video/BV1yXHYzrE24/"},{"id":"http://arxiv.org/abs/2509.09618v1","updated":"2025-09-11T16:59:06Z","published":"2025-09-11T16:59:06Z","title":"Electron and Photon Structure Functions at Two Loops","summary":"  We present a fully analytic computation of the complete electron and photon\nstructure functions, or QED lepton parton distribution functions (PDFs) up to\ntwo-loop order. Our computation is performed using modern techniques of\nreduction to Master Integrals and solving them with the differential equation\nmethod. We obtain explicit expressions for the electron-in-electron,\npositron-in-electron, photon-in-electron, electron-in-photon, and\nphoton-in-photon distributions at next-to-next-to-leading order (NNLO).\nCross-checks against one-loop results, existing two-loop calculations, and a\nrecent soft-collinear effective theory (SCET) analysis of the electron\nstructure functions are presented.\n","authors":["Marvin Schnubel","Robert Szafron"],"pdf_url":"https://arxiv.org/pdf/2509.09618v1.pdf","comment":"24 pages, 1 figure"},{"id":"http://arxiv.org/abs/2506.09130v2","updated":"2025-09-11T16:52:06Z","published":"2025-06-10T18:00:06Z","title":"Precision $e^+e^-$ Hemisphere Masses in the Dijet Region with Power\n  Corrections","summary":"  We derive high-precision results for the $e^+e^-$ heavy jet mass (HJM) $d\n\\sigma/d \\rho$ and dihemisphere mass (DHM) $d^2\\sigma/(d s_1 d s_2)$\ndistributions, for $s_1\\sim s_2$, in the dijet region. New results include: i)\nthe N$^3$LL resummation for HJM of large logarithms $\\ln^n(\\rho)$ at small\n$\\rho$ including the exact two-loop non-global hemisphere soft function, the\n4-loop cusp anomalous dimension and the 3-loop hard and jet functions, ii)\nN$^3$LL results for DHM with resummation of logarithms $\\ln(s_{1,2}/Q^2)$ when\nthere is no large separation between $s_1$ and $s_2$, iii) profile functions\nfor HJM to give results simultaneously valid in the peak and tail regions, iv)\na complete two-dimensional basis of non-perturbative functions which can be\nused for double differential observables, that are needed for both HJM and DHM\nin the peak region, and v) an implementation of renormalon subtractions for\nlarge-angle soft radiation to ${\\cal O}(\\alpha_s^3)$ together with a\nresummation of the additional large $\\ln(Q\\rho/\\Lambda_{QCD})$ logarithms. Here\n$Q$ is the $e^+e^-$ center-of-mass energy. Our resummation results are combined\nwith known fixed-order ${\\cal O}(\\alpha_s^3)$ results and we discuss the\nconvergence and remaining perturbative uncertainty in the cross section. We\nalso prove that, at order $1/Q$, the first moment of the HJM distribution\ninvolves an additional non-perturbative parameter compared to the power\ncorrection that shifts the tail of the spectrum (where $1\\gg \\rho\\gg\n\\Lambda_{QCD}/Q$). This differs from thrust where a single non-perturbative\nparameter at order $1/Q$ describes both the first moment and the tail, and it\ndisfavors models of power corrections employing a single non-perturbative\nparameter, such as the low-scale effective coupling model. In this paper we\nfocus only on the dijet region, not the far-tail distribution for $\\rho \\gtrsim\n0.2$.\n","authors":["Andre H. Hoang","Vicent Mateu","Matthew D. Schwartz","Iain W. Stewart"],"pdf_url":"https://arxiv.org/pdf/2506.09130v2.pdf","comment":"49 pages, 12 figures, published version"},{"id":"http://arxiv.org/abs/2509.09609v1","updated":"2025-09-11T16:49:52Z","published":"2025-09-11T16:49:52Z","title":"Constraints on Ultra-heavy DM from TeV-PeV gamma-ray diffuse\n  measurements","summary":"  Recent experiments have measured the Galactic $\\gamma$-ray diffuse emission\nup to PeV energies, opening a window to study acceleration of Galactic cosmic\nrays and their propagation up to the cosmic-ray knee. Furthermore, these\nobservations provide a powerful tool to set strong constraints into very-heavy\ndark matter particles, with masses in the TeV-PeV range. In this paper, we\nexplore the potential of the newest observations of diffuse emissions at the\nGalactic plane from HAWC and LHAASO to probe this kind of dark matter over a\nwide mass range. Here, we model secondary emissions (inverse-Compton) from the\nelectrons and positrons produced in the annihilation/decay of dark matter, on\ntop of their prompt $\\gamma$-ray emission, including the effects of absorption\nof high-energy photons via pair production. Furthermore, we show that including\nthe astrophysical backgrounds (namely diffuse emission from cosmic-ray\ncollisions or emission from unresolved sources) can significantly improve these\nlimits. We find that the new measurements provided, specially by LHAASO with\nthe combination of the WCDA and KM2A detectors, allow us to set strong\nconstraints in decaying dark matter, being competitive and even improving the\nstrongest constraints at the moment. We also highlight that these regions lead\nto constraints that are less affected by uncertainties from the dark matter\ndistribution and discuss how CTA north and SWGO will be able to improve limits\nin this mass range.\n","authors":["Manuel Rocamora","Pedro De La Torre Luque","Miguel A. Sánchez-Conde"],"pdf_url":"https://arxiv.org/pdf/2509.09609v1.pdf","comment":"10 pages main text, with 8 figures and appendix with 4 figures.\n  Comments are welcome!"},{"id":"http://arxiv.org/abs/2505.02816v2","updated":"2025-09-11T16:23:48Z","published":"2025-05-05T17:44:33Z","title":"Exploring two component doublet dark matter","summary":"  We propose a two-component dark matter (DM) scenario by extending the\nStandard Model with two additional $SU(2)_L$ doublets, one scalar, and another\nfermion. To ensure the stability of the DM components, we impose a global $Z_2\n\\times Z_2^\\prime$ symmetry. The lightest neutral states for both the scalar\nand fermion, which are non-trivially transformed under the extended symmetry,\nbehave as stable two-component DM candidates. While single components are\nunder-abundant due to their gauge interactions, in a mass region between $m_W$\nand $525$ GeV for the scalar and a mass below $1200$ GeV for the fermion, and\nthe fermion DM conflicts with direct detection limits over the whole parameter\nspace, having two components helps to saturate relic density in the regions\nwith under-abundance. Compliance with direct detection constraints leads to two\noptions, either introducing dim-5 effective operators, or embedding the\nscenarios into a complete UV theory, which reproduces a type II seesaw model,\nthus naturally including neutrino masses. We analyze the consequences of this\nscenario at the LHC.\n","authors":["Mariana Frank","Purusottam Ghosh","Chayan Majumdar","Supriya Senapati"],"pdf_url":"https://arxiv.org/pdf/2505.02816v2.pdf","comment":"42 pages, 26 figures, 4 tables, Matches the published version"},{"id":"http://arxiv.org/abs/2508.08006v3","updated":"2025-09-11T15:40:10Z","published":"2025-08-11T14:10:26Z","title":"Thermodynamic and quantum fluctuations of horizon area","summary":"  The event horizon is a source of irreversibility, analogous to statistical\nirreversibility. This is why for systems with an event horizon there is no\ndifference between quantum and thermal fluctuations. Quantum processes of\nquantum tunneling determine the thermodynamics of these systems, their\ntemperatures, entropies and fluctuations. We considered three examples of\nentropy variance that support this point of view: (i) the variance of the area\nof the black hole horizon, obtained by consideration of quantum fluctuations;\n(ii) the variance of the entropy of the Hubble volume in the de Sitter state,\nobtained by consideration of thermal fluctuations; and (iii) the variance of\nentropy in integers in the Planckon model, determined by the Poisson\ndistribution.\n","authors":["G. E. Volovik"],"pdf_url":"https://arxiv.org/pdf/2508.08006v3.pdf","comment":"4 pages, no figures"},{"id":"http://arxiv.org/abs/2504.21183v2","updated":"2025-09-11T15:38:00Z","published":"2025-04-29T21:36:28Z","title":"Do QGP Droplets Drive Anisotropy in Small Systems? Insights from RHIC\n  and the LHC","summary":"  Azimuthal anisotropy scaling functions for identified mesons and baryons are\nanalyzed in large (Pb+Pb at $\\sqrt{s_{NN}} = 2.76$ and 5.02 TeV, Au+Au at\n$\\sqrt{s_{NN}} = 200$ GeV), intermediate (Cu+Cu at $\\sqrt{s_{NN}} = 200$~GeV),\nand small (p+Pb at $\\sqrt{s_{NN}} = 5.02$ and 8.16 TeV, p+Au, d+Au, and\n$^3$He+Au at $\\sqrt{s_{NN}} = 200$ GeV) collision systems. The scaling\nfunctions' fidelity supports a hydrodynamic-like origin for anisotropies in the\nflow-dominated regime. Central Pb+Pb, Au+Au, and Cu+Cu reflect QGP-driven\nexpansion with strong radial flow and significant jet quenching, while\nperipheral Pb+Pb and Cu+Cu exhibit hadronic-dominated dynamics. In contrast,\ncentral RHIC small systems show hadronic-dominated behavior, with strong\nre-scattering, negligible radial flow, and suppressed jet quenching, following\nthe hierarchy p+Au $>$ d+Au $>$ $^3$He+Au. At the LHC, ultra-central p+Pb\ncollisions display enhanced radial flow, reduced re-scattering, and small but\nnonzero jet quenching. Scaling violations at high $p_T$ reflect partial\nsuppression of partonic energy loss. These findings demonstrate that QGP-like\nbehavior in small systems depends sensitively on both system size and beam\nenergy, and establish the scaling framework as a robust diagnostic of\ncollectivity and medium properties across diverse collision conditions.\n","authors":["Roy A. Lacey"],"pdf_url":"https://arxiv.org/pdf/2504.21183v2.pdf","comment":"Seven pages, four figures, submitted for publication"},{"id":"http://arxiv.org/abs/2505.14324v2","updated":"2025-09-11T15:15:54Z","published":"2025-05-20T13:09:41Z","title":"Holographic quark masses and radiative decays of heavy vector mesons","summary":"  Holographic models of QCD provide the spectrum of heavy vector meson masses\nand electromagnetic decay constants through bulk computations of the\ncurrent-current correlation function. Conversely, the phenomenology of heavy\nvector mesons is articulated by the constituent heavy quark model utilizing a\nnon-relativistic approximation. By applying the Segre formula from\nnon-relativistic quantum mechanics, we derive new observables from holography:\nthe constituent quark mass, the three-photon decay width, the effective fine\nstructure constant of the strong interaction, and the mixed one-photon and\ntwo-gluon decay width. We also derive the three-gluon decay width, the\nthree-photon decay width, and the mixed one-photon and two-gluon decay width\nfor the radially excited states of heavy quarkonia and compare them with\navailable experimental data. The present results reveal a new paradigm of meson\nspectroscopy in AdS/QCD.\n","authors":["Saulo Diles","Miguel Angel Martin Contreras","Alfredo Vega"],"pdf_url":"https://arxiv.org/pdf/2505.14324v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2509.09503v1","updated":"2025-09-11T14:44:08Z","published":"2025-09-11T14:44:08Z","title":"$CP$ asymmetries in the $Λ_c^+\\to pK^0_S$ and $Ξ^+_c\\to\n  Σ^+K^0_S$ decays","summary":"  $CP$ asymmetry is a crucial element in interpreting the matter-antimatter\nasymmetry in the universe and searching for new physics beyond the Standard\nModel. In this work, we study the $CP$ asymmetries in the $\\Lambda_c^+\\to\npK^0_S$ and $\\Xi^+_c\\to \\Sigma^+K^0_S$ decays. The time-independent and\ntime-integrated $\\Gamma$-, $\\alpha$-, $\\beta$-, and $\\gamma$-defined $CP$\nasymmetries in the chain decay $\\mathcal{B}_{c\\overline 3}\\to\n\\mathcal{B}K(t)(\\to \\pi^{+}\\pi^{-})$ are derived. It is found that the $CP$\nasymmetry in $K^0-\\overline K^0$ mixing cancels out in the $\\alpha$-, $\\beta$-,\nand $\\gamma$-defined $CP$ asymmetries. The $U$-spin analysis shows that the\namplitudes of the $\\Lambda_c^+\\to pK^0$, $\\Lambda_c^+\\to p\\overline K^0$,\n$\\Xi^+_c\\to \\Sigma^+ K^0$, and $\\Xi^+_c\\to \\Sigma^+ \\overline K^0$ modes are\nnot independent. The hadronic parameters determining $CP$ asymmetries in the\n$\\Lambda_c^+\\to pK^0_S$ and $\\Xi^+_c\\to \\Sigma^+K^0_S$ decays could be\nextracted from the $K^0_S-K^0_L$ asymmetry and decay parameters $\\alpha$,\n$\\beta$, and $\\gamma$ in these two decay modes. We find the $CP$-violating\neffect induced by the interference between charmed hadron decay and neutral\nkaon mixing in the $\\Xi^+_c\\to \\Sigma^+ K^0_S$ decay could reach to be\n$\\mathcal{O}(10^{-3})$, which is several times larger than those in $D$ meson\ndecays and at the same order as the $CP$ asymmetry in $K^0-\\overline K^0$\nmixing. In contrast, the same term in the $\\Lambda_c^+\\to pK^0_S$ mode are one\norder of magnitude smaller. Thus, the $\\Xi^+_c\\to \\Sigma^+ K^0_S$ decay is a\npromising mode for observing $CP$ asymmetry in the charmed hadron sector and\nverifying the $CP$-violating effect induced by the interference between charm\ndecay and neutral kaon mixing.\n","authors":["Di Wang","Si-Jia Wen"],"pdf_url":"https://arxiv.org/pdf/2509.09503v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2509.09472v1","updated":"2025-09-11T13:54:20Z","published":"2025-09-11T13:54:20Z","title":"Axion-Photon Conversion in FLRW with Primordial Magnetic Fields:\n  Explaining the Radio Excess","summary":"  We explore the possibility of axion-photon conversion as a common origin of\ntwo low-frequency anomalies: the isotropic radio excess (ARCADE2) and the deep\nglobal 21-cm absorption trough (EDGES). From the axion-photon action in an FLRW\nbackground with primordial magnetic fields (PMFs), we derive the\nscale-dependent conversion probability including plasma effects. Resonant\nconversion, arising when the axion mass matches the plasma-induced photon mass,\nproduces soft photons in the MHz-GHz range. By modeling stochastic PMFs with\namplitude $B_0$ and spectral index $n_{\\rm B}$, we show that axion-like\nparticles with mass $\\sim 10^{-14}$-$10^{-12}\\,\\mathrm{eV}$ and nanogauss-level\nnearly scale invariant PMFs can explain both ARCADE2 and EDGES. Heating from\nPMF dissipation via ambipolar diffusion and turbulent decay reduces the 21-cm\ntrough, shifting the viable parameter space. Our results stem from a consistent\ntheoretical framework developed from first principles and a combined analysis\nof the radio excess and global 21-cm signal, while remaining consistent with\nCMB bounds on PMFs and $\\Delta N_{\\rm eff}$. We conclude that global 21-cm\nobservations may offer potential sensitivity to axions, primordial magnetism,\nand dark-sector physics.\n","authors":[" Setabuddin","Md Riajul Haque","Rajesh Karmakar","Supratik Pal"],"pdf_url":"https://arxiv.org/pdf/2509.09472v1.pdf","comment":"32 pages, 8 figures, 1 table. Comments are welcome"},{"id":"http://arxiv.org/abs/2412.20192v2","updated":"2025-09-11T13:51:55Z","published":"2024-12-28T16:05:41Z","title":"Physics consistent machine learning framework for inverse modeling with\n  applications to ICF capsule implosions","summary":"  In high energy density physics (HEDP) and inertial confinement fusion (ICF),\npredictive modeling is complicated by uncertainty in parameters that\ncharacterize various aspects of the modeled system, such as those\ncharacterizing material properties, equation of state (EOS), opacities, and\ninitial conditions. Typically, however, these parameters are not directly\nobservable. What is observed instead is a time sequence of radiographic\nprojections using X-rays. In this work, we define a set of sparse hydrodynamic\nfeatures derived from the outgoing shock profile and outer material edge, which\ncan be obtained from radiographic measurements, to directly infer such\nparameters. Our machine learning (ML)-based methodology involves a pipeline of\ntwo architectures, a radiograph-to-features network (R2FNet) and a\nfeatures-to-parameters network (F2PNet), that are trained independently and\nlater combined to approximate a posterior distribution for the parameters from\nradiographs. We show that the estimated parameters can be used in a\nhydrodynamics code to obtain density fields and hydrodynamic shock and outer\nedge features that are consistent with the data. Finally, we demonstrate that\nfeatures resulting from an unknown EOS model can be successfully mapped onto\nparameters of a chosen analytical EOS model, implying that network predictions\nare learning physics, with a degree of invariance to the underlying choice of\nEOS model.\n","authors":["Daniel A. Serino","Evan Bell","Marc Klasky","Ben S. Southworth","Balasubramanya Nadiga","Trevor Wilcox","Oleg Korobkin"],"pdf_url":"https://arxiv.org/pdf/2412.20192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19862v3","updated":"2025-09-11T13:51:23Z","published":"2025-03-25T17:29:10Z","title":"Early Career Researcher Input to the European Strategy for Particle\n  Physics Update: White Paper","summary":"  This document, written by early career researchers (ECRs) in particle\nphysics, aims to represent the perspectives of the European ECR community and\nserves as input for the 2025--2026 update of the European Strategy for Particle\nPhysics. With input from a community-wide survey, it highlights key challenges\nfaced by ECRs -- career stability, funding access and long-term research\nopportunities -- while proposing policy recommendations and targeted\ninitiatives. It underscores the importance of practices fostering diverse,\nequitable, inclusive and healthy workplaces, as well as of stronger ECR\ncommunities, and highlights how effective communication and interdisciplinary\ncollaborations reinforce the societal relevance of particle physics and promote\ncontinued support for large-scale and long-term projects. Finally, the future\nof both collider and beyond-collider experiments is addressed, emphasising the\ncritical role of ECRs in shaping future projects. The ECR contribution is\nformed of two parts: the ten-page executive summary submitted as input to the\nEuropean Strategy for Particle Physics Update and, as backup document, this\nextended white paper providing additional context.\n","authors":["Jan-Hendrik Arling","Alexander Burgman","Christina Dimitriadi","Ulrich Einhaus","Axel Gallén","Abdelhamid Haddad","Laura Huhta","Armin Ilg","Jan Klamka","Elizabeth Long","Thomas Madlener","Arnau Morancho Tardà","Emanuela Musumeci","Krzysztof Mękała","Elena Pompa Pacchi","Marvin Pfaff","Daniel Reichelt","Leonhard Reichenbach","Birgit Stapf","Francesco P. Ucci","Erik Wallin","Harriet Watson","Sagar Vidya Addepalli","Bruno Alves","Robert Mihai Amarinei","Ricardo Barrué","Lydia Brenner","Giacomo Da Molin","Arturo de Giorgi","Bohdan Dudar","Francesco Giuli","Andrea Gurgone","César Jesús-Valls","Antoine Laudrain","Martin J. Losekamm","Rafał Masełek","Wrishik Naskar","Miquel Nebot-Guinot","Marko Pesut","Thomas Pöschl","Efrain P. Segarra","Rebecca Taylor","Pavel Vana","Hannah Wakeling","Aidan R. Wiederhold"],"pdf_url":"https://arxiv.org/pdf/2503.19862v3.pdf","comment":"Endorsed by the ECFA ECR Panel. Editor and author attribution in the\n  document"},{"id":"http://arxiv.org/abs/2509.09454v1","updated":"2025-09-11T13:39:26Z","published":"2025-09-11T13:39:26Z","title":"Casimir scaling in glueballs in SU($N$) and Sp($2N$) gauge theories:\n  hints from constituent approaches","summary":"  We show that the lattice glueball masses $M_G$ versus $N$ in SU($N$) and\nSp($2N$) Yang-Mills theories scale as $\\frac{M_G}{\\sqrt\\sigma}\\sim\n\\sqrt{\\frac{C_2(adj)}{C_2(f)}}$, with $\\sigma$ the fundamental string tension\nand $C_2(adj)$ and $C_2(f)$ the quadratic Casimir of the gauge algebra in the\nadjoint and fundamental representations. This scaling behaviour is followed by\nthe great majority of available lattice glueball states, and may set\nconstraints on $SU(3)$ models by imposing a specific behaviour at $N\\neq 3$.\nThe observed scaling is compatible with two assumptions: (1) The glueball\nmasses are proportional to the square root of the adjoint string tension,\n$M_G\\sim \\sqrt\\sigma_{adj}$; (2) The string tension follows the Casimir\nscaling, i.e. $\\sigma_{adj}=\\frac{C_2(adj)}{C_2(f)}\\sigma$. In a constituent\ngluon picture, our results suggest a low-lying glueball spectrum made of two\ntransverse constituent gluons bound by an adjoint string, completed by three\ntransverse constituent gluons bound by a Y-junction of adjoint strings rather\nthan a $\\Delta-$shaped junction of fundamental strings.\n","authors":["F. Buisseret","C. Chevalier","V. Mathieu","C. Semay"],"pdf_url":"https://arxiv.org/pdf/2509.09454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09437v1","updated":"2025-09-11T13:25:34Z","published":"2025-09-11T13:25:34Z","title":"Dark Vector Boson Bremsstrahlung: New Form Factors for a Broader Class\n  of Models","summary":"  We explore the sensitivity of collider experiments to a broad class of\nGeV-scale dark vector models of new physics via production in proton and\nneutron bremsstrahlung and initial state radiation. This is achieved using a\nnew physically motivated model for timelike vector form factors with generic\ncharges for both protons and neutrons, which is fit to a variety of timelike\nand spacelike data with quantified uncertainties. The production model for both\nproton and neutron bremsstrahlung is applied to re-cast and extend the reach of\nexisting FASER data to GeV-mass dark photons, $U(1)_B$, $U(1)_{B-L}$, and\nphotophobic vectors, as well as forecasts for millicharged particles at\nFORMOSA.\n","authors":["Felix Kling","Peter Reimitz","Adam Ritz"],"pdf_url":"https://arxiv.org/pdf/2509.09437v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2508.00693v2","updated":"2025-09-11T13:05:24Z","published":"2025-08-01T15:11:00Z","title":"On multi-propagator angular integrals","summary":"  We study multi-propagator angular integrals, a class of phase-space integrals\nrelevant to processes with multiple observed final states and a test-bed for\ntransferring loop-integral technology to phase space integrals without reversed\nunitarity. We present an Euler integral representation similar to\nLee-Pomeransky representation and explicitly describe a recursive IBP reduction\nand dimensional shift relations for the general case of $n$ denominators. On\nthe level of master integrals, applying a differential equation approach, we\nexplicitly calculate the previously unknown angular integrals with four\ndenominators for any number of masses to finite order in $\\varepsilon$.\nExtending the idea of dimensional recurrence, we explore the decomposition of\nangular integrals into branch integrals reducing the number of scales in the\nmaster integrals from $(n+1)n/2$ to $n+1$. To showcase the potential of this\nmethod, we calculate the massless three denominator integral to establish\nall-order results in $\\varepsilon$ including a resummation of soft logarithms.\n","authors":["Juliane Haug","Vladimir A. Smirnov","Fabian Wunder"],"pdf_url":"https://arxiv.org/pdf/2508.00693v2.pdf","comment":"30 pages, 2 figures"},{"id":"http://arxiv.org/abs/2509.09416v1","updated":"2025-09-11T12:53:03Z","published":"2025-09-11T12:53:03Z","title":"Tree-level NLO corrections to inclusive $ψ'$ production in High\n  Energy Factorization","summary":"  We consider inclusive $\\psi(2S)$ production in proton-proton collisions at\ncollider energies in the framework of non-relativistic QCD and high-energy\nfactorization beyond the low-order approximation. We utilise a matching scheme\nproposed earlier to merge the leading order and tree-level next-to-leading\norder production amplitudes and now extend it to low transverse momenta and/or\nforward rapidities. With the improved scheme, we examine the possibility to\nsimultaneously describe all unpolarized LHC data in the full kinematic range\naccessible to experimental measurements and try different Transverse Momentum\nDependent (TMD) gluon distributions in the proton. A global fit to the data is\ncarried out to extract the color octet long-distance matrix elements for\n$\\psi(2S)$ mesons. We show that taking the NLO corrections into account leads\nto better description of the data.\n","authors":["A. A. Prokhorov","S. P. Baranov","A. V. Lipatov","M. A. Malyshev","X. Chen"],"pdf_url":"https://arxiv.org/pdf/2509.09416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16747v3","updated":"2025-09-11T12:06:24Z","published":"2024-01-30T04:56:02Z","title":"Searching for Particle Dark Matter with eROSITA Early Data","summary":"  Many well motivated dark matter (DM) particle candidates can decay into\ndetectable X-ray photons. We analyze the Final Equatorial Depth Survey (eFEDS)\ndata from eROSITA early data release to search for unidentified X-ray lines\nthat could indicate DM signals. Having discovered no anomalous signal, we set\nlimits on DM decay rate in mass range between 1.8-18 keV, and constrain the\nparameter space of two DM particles: sterile neutrino and axion-like particles.\nFinally we also study the projected sensitivity of eROSITA full sky search,\nshowing that eROSITA all-sky survey is expected to set the most stringent\nlimits in the soft X-ray band.\n","authors":["Chingam Fong","Kenny C. Y. Ng","Qishan Liu"],"pdf_url":"https://arxiv.org/pdf/2401.16747v3.pdf","comment":"12+3 pages, 7+2 figures. v3 updated to match version accepted by PRD:\n  redone GC and all-sky projection; added appendix B checking different diffuse\n  sky models; added details and fixed typos; results & conlusions unchanged"},{"id":"http://arxiv.org/abs/2509.09367v1","updated":"2025-09-11T11:34:13Z","published":"2025-09-11T11:34:13Z","title":"Toward precise $ξ$ gauge fixing for the lattice QCD","summary":"  Lattice QCD provides a first-principles framework for solving Quantum\nChromodynamics (QCD). However, its application to off-shell partons has been\nlargely restricted to the Landau gauge, as achieving high-precision $\\xi$-gauge\nfixing on the lattice poses significant challenges. Motivated by a universal\npower-law dependence of off-shell parton matrix elements on gauge-fixing\nprecision in the Landau gauge, we propose an empirical precision extrapolation\nmethod to approximate high-precision $\\xi$-gauge fixing. By properly defining\nthe bare gauge coupling and then the effective $\\xi$, we validate our\n$\\xi$-gauge fixing procedure by successfully reproducing the $\\xi$-dependent\nRI/MOM renormalization constants for local quark bilinear operators at 0.2\\%\nlevel, up to $\\xi \\sim 1$.\n","authors":["Li-Jun Zhou","Dian-Jun Zhao","Wei-jie Fu","Chun-Jiang Shi","Ji-Hao Wang","Yi-Bo Yang"],"pdf_url":"https://arxiv.org/pdf/2509.09367v1.pdf","comment":"9 pages, 8 figures"}],"Machine Learning - Statistics":[{"id":"http://arxiv.org/abs/2409.03962v2","updated":"2025-09-11T15:52:22Z","published":"2024-09-06T01:07:29Z","title":"Average Causal Effect Estimation in DAGs with Hidden Variables: Beyond\n  Back-Door and Front-Door Criteria","summary":"  The identification theory for causal effects in directed acyclic graphs\n(DAGs) with hidden variables is well established, but methods for estimating\nand inferring functionals that extend beyond the g-formula remain\nunderdeveloped. Previous studies have introduced semiparametric estimators for\nsuch functionals in a broad class of DAGs with hidden variables. While these\nestimators exhibit desirable statistical properties such as double robustness\nin certain cases, they also face significant limitations. Notably, they\nencounter substantial computational challenges, particularly involving density\nestimation and numerical integration for continuous variables, and their\nestimates may fall outside the parameter space of the target estimand.\nAdditionally, the asymptotic properties of these estimators is underexplored,\nespecially when integrating flexible statistical and machine learning models\nfor nuisance functional estimations. This paper addresses these challenges by\nintroducing novel one-step corrected plug-in and targeted minimum loss-based\nestimators of causal effects for a class of hidden variable DAGs that go beyond\nclassical back-door and front-door criteria (known as the treatment primal\nfixability criterion in prior literature). These estimators leverage\ndata-adaptive machine learning algorithms to minimize modeling assumptions\nwhile ensuring key statistical properties including double robustness,\nefficiency, boundedness within the target parameter space, and asymptotic\nlinearity under $L^2(P)$-rate conditions for nuisance functional estimates that\nyield root-n consistent causal effect estimates. To ensure our estimation\nmethods are accessible in practice, we provide the flexCausal package in R.\n","authors":["Anna Guo","Razieh Nabi"],"pdf_url":"https://arxiv.org/pdf/2409.03962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18520v2","updated":"2025-09-11T14:50:14Z","published":"2025-07-24T15:45:23Z","title":"Euclidean Distance Deflation Under High-Dimensional Heteroskedastic\n  Noise","summary":"  Pairwise Euclidean distance calculation is a fundamental step in many machine\nlearning and data analysis algorithms. In real-world applications, however,\nthese distances are frequently distorted by heteroskedastic\nnoise$\\unicode{x2014}$a prevalent form of inhomogeneous corruption\ncharacterized by variable noise magnitudes across data observations. Such noise\ninflates the computed distances in a nontrivial way, leading to\nmisrepresentations of the underlying data geometry. In this work, we address\nthe tasks of estimating the noise magnitudes per observation and correcting the\npairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly,\nwe show that in general high-dimensional settings and without assuming prior\nknowledge on the clean data structure or noise distribution, both tasks can be\nperformed reliably, even when the noise levels vary considerably. Specifically,\nwe develop a principled, hyperparameter-free approach that jointly estimates\nthe noise magnitudes and corrects the distances. We provide theoretical\nguarantees for our approach, establishing probabilistic bounds on the\nestimation errors of both noise magnitudes and distances. These bounds,\nmeasured in the normalized $\\ell_1$ norm, converge to zero at polynomial rates\nas both feature dimension and dataset size increase. Experiments on synthetic\ndatasets demonstrate that our method accurately estimates distances in\nchallenging regimes, significantly improving the robustness of subsequent\ndistance-based computations. Notably, when applied to single-cell RNA\nsequencing data, our method yields noise magnitude estimates consistent with an\nestablished prototypical model, enabling accurate nearest neighbor\nidentification that is fundamental to many downstream analyses.\n","authors":["Keyi Li","Yuval Kluger","Boris Landa"],"pdf_url":"https://arxiv.org/pdf/2507.18520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07543v2","updated":"2025-09-11T14:39:19Z","published":"2025-09-09T09:23:36Z","title":"Asynchronous Gossip Algorithms for Rank-Based Statistical Methods","summary":"  As decentralized AI and edge intelligence become increasingly prevalent,\nensuring robustness and trustworthiness in such distributed settings has become\na critical issue-especially in the presence of corrupted or adversarial data.\nTraditional decentralized algorithms are vulnerable to data contamination as\nthey typically rely on simple statistics (e.g., means or sum), motivating the\nneed for more robust statistics. In line with recent work on decentralized\nestimation of trimmed means and ranks, we develop gossip algorithms for\ncomputing a broad class of rank-based statistics, including L-statistics and\nrank statistics-both known for their robustness to outliers. We apply our\nmethod to perform robust distributed two-sample hypothesis testing, introducing\nthe first gossip algorithm for Wilcoxon rank-sum tests. We provide rigorous\nconvergence guarantees, including the first convergence rate bound for\nasynchronous gossip-based rank estimation. We empirically validate our\ntheoretical results through experiments on diverse network topologies.\n","authors":["Anna Van Elst","Igor Colin","Stephan Clémençon"],"pdf_url":"https://arxiv.org/pdf/2509.07543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07735v3","updated":"2025-09-11T14:32:19Z","published":"2025-02-11T17:55:03Z","title":"Revisiting Non-Acyclic GFlowNets in Discrete Environments","summary":"  Generative Flow Networks (GFlowNets) are a family of generative models that\nlearn to sample objects from a given probability distribution, potentially\nknown up to a normalizing constant. Instead of working in the object space,\nGFlowNets proceed by sampling trajectories in an appropriately constructed\ndirected acyclic graph environment, greatly relying on the acyclicity of the\ngraph. In our paper, we revisit the theory that relaxes the acyclicity\nassumption and present a simpler theoretical framework for non-acyclic\nGFlowNets in discrete environments. Moreover, we provide various novel\ntheoretical insights related to training with fixed backward policies, the\nnature of flow functions, and connections between entropy-regularized RL and\nnon-acyclic GFlowNets, which naturally generalize the respective concepts and\ntheoretical results from the acyclic setting. In addition, we experimentally\nre-examine the concept of loss stability in non-acyclic GFlowNet training, as\nwell as validate our own theoretical findings.\n","authors":["Nikita Morozov","Ian Maksimov","Daniil Tiapkin","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2502.07735v3.pdf","comment":"ICML 2025; minor corrections in proofs of Proposition 3.6 and 3.8 in\n  v3, all results remain unchanged"},{"id":"http://arxiv.org/abs/2508.19897v3","updated":"2025-09-11T14:30:28Z","published":"2025-08-27T13:53:56Z","title":"The Information Dynamics of Generative Diffusion","summary":"  Generative diffusion models have emerged as a powerful class of models in\nmachine learning, yet a unified theoretical understanding of their operation is\nstill developing. This paper provides an integrated perspective on generative\ndiffusion by connecting their dynamic, information-theoretic, and thermodynamic\nproperties under a unified mathematical framework. We demonstrate that the rate\nof conditional entropy production during generation (i.e. the generative\nbandwidth) is directly governed by the expected divergence of the score\nfunction's vector field. This divergence, in turn, is linked to the branching\nof trajectories and generative bifurcations, which we characterize as\nsymmetry-breaking phase transitions in the energy landscape. This synthesis\noffers a powerful insight: the process of generation is fundamentally driven by\nthe controlled, noise-induced breaking of (approximate) symmetries, where peaks\nin information transfer correspond to critical transitions between possible\noutcomes. The score function acts as a dynamic non-linear filter that regulates\nthe bandwidth of the noise by suppressing fluctuations that are incompatible\nwith the data.\n","authors":["Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2508.19897v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11703v3","updated":"2025-09-11T13:42:30Z","published":"2024-06-17T16:24:23Z","title":"Unveiling Multiple Descents in Unsupervised Autoencoders","summary":"  The phenomenon of double descent has challenged the traditional bias-variance\ntrade-off in supervised learning but remains unexplored in unsupervised\nlearning, with some studies arguing for its absence. In this study, we first\ndemonstrate analytically that double descent does not occur in linear\nunsupervised autoencoders (AEs). In contrast, we show for the first time that\nboth double and triple descent can be observed with nonlinear AEs across\nvarious data models and architectural designs. We examine the effects of\npartial sample and feature noise and highlight the importance of bottleneck\nsize in influencing the double descent curve. Through extensive experiments on\nboth synthetic and real datasets, we uncover model-wise, epoch-wise, and\nsample-wise double descent across several data types and architectures. Our\nfindings indicate that over-parameterized models not only improve\nreconstruction but also enhance performance in downstream tasks such as anomaly\ndetection and domain adaptation, highlighting their practical value in complex\nreal-world scenarios.\n","authors":["Kobi Rahimi","Yehonathan Refael","Tom Tirer","Ofir Lindenbaum"],"pdf_url":"https://arxiv.org/pdf/2406.11703v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09362v1","updated":"2025-09-11T11:28:20Z","published":"2025-09-11T11:28:20Z","title":"Expressive Power of Deep Networks on Manifolds: Simultaneous\n  Approximation","summary":"  A key challenge in scientific machine learning is solving partial\ndifferential equations (PDEs) on complex domains, where the curved geometry\ncomplicates the approximation of functions and their derivatives required by\ndifferential operators. This paper establishes the first simultaneous\napproximation theory for deep neural networks on manifolds. We prove that a\nconstant-depth $\\mathrm{ReLU}^{k-1}$ network with bounded weights--a property\nthat plays a crucial role in controlling generalization error--can approximate\nany function in the Sobolev space $\\mathcal{W}_p^{k}(\\mathcal{M}^d)$ to an\nerror of $\\varepsilon$ in the $\\mathcal{W}_p^{s}(\\mathcal{M}^d)$ norm, for\n$k\\geq 3$ and $s<k$, using $\\mathcal{O}(\\varepsilon^{-d/(k-s)})$ nonzero\nparameters, a rate that overcomes the curse of dimensionality by depending only\non the intrinsic dimension $d$. These results readily extend to functions in\nH\\\"older-Zygmund spaces. We complement this result with a matching lower bound,\nproving our construction is nearly optimal by showing the required number of\nparameters matches up to a logarithmic factor. Our proof of the lower bound\nintroduces novel estimates for the Vapnik-Chervonenkis dimension and\npseudo-dimension of the network's high-order derivative classes. These\ncomplexity bounds provide a theoretical cornerstone for learning PDEs on\nmanifolds involving derivatives. Our analysis reveals that the network\narchitecture leverages a sparse structure to efficiently exploit the manifold's\nlow-dimensional geometry.\n","authors":["Hanfei Zhou","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2509.09362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09353v1","updated":"2025-09-11T11:07:36Z","published":"2025-09-11T11:07:36Z","title":"Low-degree lower bounds via almost orthonormal bases","summary":"  Low-degree polynomials have emerged as a powerful paradigm for providing\nevidence of statistical-computational gaps across a variety of high-dimensional\nstatistical models [Wein25]. For detection problems -- where the goal is to\ntest a planted distribution $\\mathbb{P}'$ against a null distribution\n$\\mathbb{P}$ with independent components -- the standard approach is to bound\nthe advantage using an $\\mathbb{L}^2(\\mathbb{P})$-orthonormal family of\npolynomials. However, this method breaks down for estimation tasks or more\ncomplex testing problems where $\\mathbb{P}$ has some planted structures, so\nthat no simple $\\mathbb{L}^2(\\mathbb{P})$-orthogonal polynomial family is\navailable. To address this challenge, several technical workarounds have been\nproposed [SW22,SW25], though their implementation can be delicate. In this\nwork, we propose a more direct proof strategy. Focusing on random graph models,\nwe construct a basis of polynomials that is almost orthonormal under\n$\\mathbb{P}$, in precisely those regimes where statistical-computational gaps\narise. This almost orthonormal basis not only yields a direct route to\nestablishing low-degree lower bounds, but also allows us to explicitly identify\nthe polynomials that optimize the low-degree criterion. This, in turn, provides\ninsights into the design of optimal polynomial-time algorithms. We illustrate\nthe effectiveness of our approach by recovering known low-degree lower bounds,\nand establishing new ones for problems such as hidden subcliques, stochastic\nblock models, and seriation models.\n","authors":["Alexandra Carpentier","Simone Maria Giancola","Christophe Giraud","Nicolas Verzelen"],"pdf_url":"https://arxiv.org/pdf/2509.09353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.11274v2","updated":"2025-09-11T09:19:48Z","published":"2025-08-15T07:20:31Z","title":"Uniform convergence for Gaussian kernel ridge regression","summary":"  This paper establishes the first polynomial convergence rates for Gaussian\nkernel ridge regression (KRR) with a fixed hyperparameter in both the uniform\nand the $L^{2}$-norm. The uniform convergence result closes a gap in the\ntheoretical understanding of KRR with the Gaussian kernel, where no such rates\nwere previously known. In addition, we prove a polynomial $L^{2}$-convergence\nrate in the case, where the Gaussian kernel's width parameter is fixed. This\nalso contributes to the broader understanding of smooth kernels, for which\npreviously only sub-polynomial $L^{2}$-rates were known in similar settings.\nTogether, these results provide new theoretical justification for the use of\nGaussian KRR with fixed hyperparameters in nonparametric regression.\n","authors":["Paul Dommel","Rajmadan Lakshmanan"],"pdf_url":"https://arxiv.org/pdf/2508.11274v2.pdf","comment":"The submission is being withdrawn because the authorship of the\n  manuscript does not comply with the publishing/authorship guidelines of our\n  department"},{"id":"http://arxiv.org/abs/2408.07016v2","updated":"2025-09-11T08:55:14Z","published":"2024-08-13T16:30:36Z","title":"Rethinking Disentanglement under Dependent Factors of Variation","summary":"  Representation learning is an approach that allows to discover and extract\nthe factors of variation from the data. Intuitively, a representation is said\nto be disentangled if it separates the different factors of variation in a way\nthat is understandable to humans. Definitions of disentanglement and metrics to\nmeasure it usually assume that the factors of variation are independent of each\nother. However, this is generally false in the real world, which limits the use\nof these definitions and metrics to very specific and unrealistic scenarios. In\nthis paper we give a definition of disentanglement based on information theory\nthat is also valid when the factors of variation are not independent.\nFurthermore, we relate this definition to the Information Bottleneck Method.\nFinally, we propose a method to measure the degree of disentanglement from the\ngiven definition that works when the factors of variation are not independent.\nWe show through different experiments that the method proposed in this paper\ncorrectly measures disentanglement with non-independent factors of variation,\nwhile other methods fail in this scenario.\n","authors":["Antonio Almudévar","Alfonso Ortega"],"pdf_url":"https://arxiv.org/pdf/2408.07016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14492v4","updated":"2025-09-11T08:33:26Z","published":"2024-05-23T12:25:22Z","title":"Iterative Methods for Full-Scale Gaussian Process Approximations for\n  Large Spatial Data","summary":"  Gaussian processes are flexible probabilistic regression models which are\nwidely used in statistics and machine learning. However, a drawback is their\nlimited scalability to large data sets. To alleviate this, full-scale\napproximations (FSAs) combine predictive process methods and covariance\ntapering, thus approximating both global and local structures. We show how\niterative methods can be used to reduce computational costs in calculating\nlikelihoods, gradients, and predictive distributions with FSAs. In particular,\nwe introduce a novel preconditioner and show theoretically and empirically that\nit accelerates the conjugate gradient method's convergence speed and mitigates\nits sensitivity with respect to the FSA parameters and the eigenvalue structure\nof the original covariance matrix, and we demonstrate empirically that it\noutperforms a state-of-the-art pivoted Cholesky preconditioner. Furthermore, we\nintroduce an accurate and fast way to calculate predictive variances using\nstochastic simulation and iterative methods. In addition, we show how our newly\nproposed FITC preconditioner can also be used in iterative methods for Vecchia\napproximations. In our experiments, it outperforms existing state-of-the-art\npreconditioners for Vecchia approximations. All methods are implemented in a\nfree C++ software library with high-level Python and R packages.\n","authors":["Tim Gyger","Reinhard Furrer","Fabio Sigrist"],"pdf_url":"https://arxiv.org/pdf/2405.14492v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09238v1","updated":"2025-09-11T08:20:30Z","published":"2025-09-11T08:20:30Z","title":"Global Optimization of Stochastic Black-Box Functions with Arbitrary\n  Noise Distributions using Wilson Score Kernel Density Estimation","summary":"  Many optimization problems in robotics involve the optimization of\ntime-expensive black-box functions, such as those involving complex simulations\nor evaluation of real-world experiments. Furthermore, these functions are often\nstochastic as repeated experiments are subject to unmeasurable disturbances.\nBayesian optimization can be used to optimize such methods in an efficient\nmanner by deploying a probabilistic function estimator to estimate with a given\nconfidence so that regions of the search space can be pruned away.\nConsequently, the success of the Bayesian optimization depends on the function\nestimator's ability to provide informative confidence bounds. Existing function\nestimators require many function evaluations to infer the underlying confidence\nor depend on modeling of the disturbances. In this paper, it is shown that the\nconfidence bounds provided by the Wilson Score Kernel Density Estimator\n(WS-KDE) are applicable as excellent bounds to any stochastic function with an\noutput confined to the closed interval [0;1] regardless of the distribution of\nthe output. This finding opens up the use of WS-KDE for stable global\noptimization on a wider range of cost functions. The properties of WS-KDE in\nthe context of Bayesian optimization are demonstrated in simulation and applied\nto the problem of automated trap design for vibrational part feeders.\n","authors":["Thorbjørn Mosekjær Iversen","Lars Carøe Sørensen","Simon Faarvang Mathiesen","Henrik Gordon Petersen"],"pdf_url":"https://arxiv.org/pdf/2509.09238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19097v2","updated":"2025-09-11T07:44:56Z","published":"2025-05-25T11:20:28Z","title":"Towards Robust Influence Functions with Flat Validation Minima","summary":"  The Influence Function (IF) is a widely used technique for assessing the\nimpact of individual training samples on model predictions. However, existing\nIF methods often fail to provide reliable influence estimates in deep neural\nnetworks, particularly when applied to noisy training data. This issue does not\nstem from inaccuracies in parameter change estimation, which has been the\nprimary focus of prior research, but rather from deficiencies in loss change\nestimation, specifically due to the sharpness of validation risk. In this work,\nwe establish a theoretical connection between influence estimation error,\nvalidation set risk, and its sharpness, underscoring the importance of flat\nvalidation minima for accurate influence estimation. Furthermore, we introduce\na novel estimation form of Influence Function specifically designed for flat\nvalidation minima. Experimental results across various tasks validate the\nsuperiority of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Cheng Jin","Yifan Chen"],"pdf_url":"https://arxiv.org/pdf/2505.19097v2.pdf","comment":"Accepted by ICML 2025. arXiv admin note: text overlap with\n  arXiv:2310.00902 by other authors"},{"id":"http://arxiv.org/abs/2509.09078v1","updated":"2025-09-11T00:52:59Z","published":"2025-09-11T00:52:59Z","title":"Scalable extensions to given-data Sobol' index estimators","summary":"  Given-data methods for variance-based sensitivity analysis have significantly\nadvanced the feasibility of Sobol' index computation for computationally\nexpensive models and models with many inputs. However, the limitations of\nexisting methods still preclude their application to models with an extremely\nlarge number of inputs. In this work, we present practical extensions to the\nexisting given-data Sobol' index method, which allow variance-based sensitivity\nanalysis to be efficiently performed on large models such as neural networks,\nwhich have $>10^4$ parameterizable inputs. For models of this size, holding all\ninput-output evaluations simultaneously in memory -- as required by existing\nmethods -- can quickly become impractical. These extensions also support\nnonstandard input distributions with many repeated values, which are not\namenable to equiprobable partitions employed by existing given-data methods.\n  Our extensions include a general definition of the given-data Sobol' index\nestimator with arbitrary partition, a streaming algorithm to process\ninput-output samples in batches, and a heuristic to filter out small indices\nthat are indistinguishable from zero indices due to statistical noise. We show\nthat the equiprobable partition employed in existing given-data methods can\nintroduce significant bias into Sobol' index estimates even at large sample\nsizes and provide numerical analyses that demonstrate why this can occur. We\nalso show that our streaming algorithm can achieve comparable accuracy and\nruntimes with lower memory requirements, relative to current methods which\nprocess all samples at once. We demonstrate our novel developments on two\napplication problems in neural network modeling.\n","authors":["Teresa Portone","Bert Debusschere","Samantha Yang","Emiliano Islas-Quinones","T. Patrick Xiao"],"pdf_url":"https://arxiv.org/pdf/2509.09078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09070v1","updated":"2025-09-11T00:19:53Z","published":"2025-09-11T00:19:53Z","title":"STRIDE: Scalable and Interpretable XAI via Subset-Free Functional\n  Decomposition","summary":"  Most explainable AI (XAI) frameworks face two practical limitations: the\nexponential cost of reasoning over feature subsets and the reduced\nexpressiveness of summarizing effects as single scalar values. We present\nSTRIDE, a scalable framework that aims to mitigate both issues by framing\nexplanation as a subset-enumeration-free, orthogonal functional decomposition\nin a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on\nscalar attributions, STRIDE computes functional components f_S(x_S) via an\nanalytical projection scheme based on a recursive kernel-centering procedure,\navoiding explicit subset enumeration. In the tabular setups we study, the\napproach is model-agnostic, provides both local and global views, and is\nsupported by theoretical results on orthogonality and L^2 convergence under\nstated assumptions. On public tabular benchmarks in our environment, we\nobserved speedups ranging from 0.6 times (slower than TreeSHAP on a small\ndataset) to 9.7 times (California), with a median approximate 3.0 times across\n10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and\nsubstantial rank agreement on most datasets. Overall, STRIDE complements scalar\nattribution methods by offering a structured functional perspective, enabling\nnovel diagnostics like 'component surgery' to quantitatively measure the impact\nof specific interactions within our experimental scope.\n","authors":["Chaeyun Ko"],"pdf_url":"https://arxiv.org/pdf/2509.09070v1.pdf","comment":"10 pages, 2 figures"}],"Machine Learning - Computer Science":[{"id":"http://arxiv.org/abs/2509.09679v1","updated":"2025-09-11T17:59:51Z","published":"2025-09-11T17:59:51Z","title":"ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms","summary":"  Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. We propose ButterflyQuant, which replaces\nHadamard rotations with learnable butterfly transforms parameterized by\ncontinuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.\n","authors":["Bingxin Xu","Zhen Dong","Oussama Elachqar","Yuzhang Shang"],"pdf_url":"https://arxiv.org/pdf/2509.09679v1.pdf","comment":"Replace discrete Hadamard transforms with continuous Butterfly\n  transforms to facilitate the learning of rotation matrices in LLM\n  quantization"},{"id":"http://arxiv.org/abs/2509.09674v1","updated":"2025-09-11T17:59:17Z","published":"2025-09-11T17:59:17Z","title":"SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning","summary":"  Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL\n","authors":["Haozhan Li","Yuxin Zuo","Jiale Yu","Yuhao Zhang","Zhaohui Yang","Kaiyan Zhang","Xuekai Zhu","Yuchen Zhang","Tianxing Chen","Ganqu Cui","Dehui Wang","Dingxiang Luo","Yuchen Fan","Youbang Sun","Jia Zeng","Jiangmiao Pang","Shanghang Zhang","Yu Wang","Yao Mu","Bowen Zhou","Ning Ding"],"pdf_url":"https://arxiv.org/pdf/2509.09674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09675v1","updated":"2025-09-11T17:59:17Z","published":"2025-09-11T17:59:17Z","title":"CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.\n","authors":["Runpeng Dai","Linfeng Song","Haolin Liu","Zhenwen Liang","Dian Yu","Haitao Mi","Zhaopeng Tu","Rui Liu","Tong Zheng","Hongtu Zhu","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2509.09675v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2509.09660v1","updated":"2025-09-11T17:55:09Z","published":"2025-09-11T17:55:09Z","title":"Steering MoE LLMs via Expert (De)Activation","summary":"  Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts.\n","authors":["Mohsen Fayyaz","Ali Modarressi","Hanieh Deilamsalehy","Franck Dernoncourt","Ryan Rossi","Trung Bui","Hinrich Schütze","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2509.09660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.06602v2","updated":"2025-09-11T17:52:20Z","published":"2025-09-08T12:15:53Z","title":"Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in\n  Molecular Tumor Boards","summary":"  Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs.\n","authors":["Matthias Blondeel","Noel Codella","Sam Preston","Hao Qiu","Leonardo Schettini","Frank Tuan","Wen-wai Yim","Smitha Saligrama","Mert Öz","Shrey Jain","Matthew P. Lungren","Thomas Osborne"],"pdf_url":"https://arxiv.org/pdf/2509.06602v2.pdf","comment":"9 pages, 1 figure; Added missing co-authors and contributors"},{"id":"http://arxiv.org/abs/2509.09655v1","updated":"2025-09-11T17:50:06Z","published":"2025-09-11T17:50:06Z","title":"Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for\n  Medicaid Care Management","summary":"  We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning\n(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds\nto reduce harm while equalizing a chosen fairness target (coverage or harm)\nacross protected subgroups. Using de-identified longitudinal trajectories from\na Medicaid population health management program, we evaluate FG-FARL against\nbehavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global\nconformal safety baseline). We report off-policy value estimates with bootstrap\n95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL\nachieves comparable value to baselines while improving fairness metrics,\ndemonstrating a practical path to safer and more equitable decision support.\n","authors":["Sanjay Basu","Sadiq Y. Patel","Parth Sheth","Bhairavi Muralidharan","Namrata Elamaran","Aakriti Kinra","Rajaie Batniji"],"pdf_url":"https://arxiv.org/pdf/2509.09655v1.pdf","comment":"12 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.08219v3","updated":"2025-09-11T17:49:08Z","published":"2025-01-14T16:02:33Z","title":"Investigating Energy Efficiency and Performance Trade-offs in LLM\n  Inference Across Tasks and DVFS Settings","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks, leading to widespread\nadoption in both research and industry. However, their inference workloads are\ncomputationally and energy intensive, raising concerns about sustainability and\nenvironmental impact. As LLMs continue to scale, it becomes essential to\nidentify and optimize the factors that influence their runtime efficiency\nwithout compromising performance. In this work, we systematically investigate\nthe energy-performance trade-offs of LLMs during inference. We benchmark models\nof varying sizes and architectures, including Falcon-7B, Mistral-7B-v0.1,\nLLaMA-3.2-1B, LLaMA-3.2-3B, and GPT-Neo-2.7B, across tasks such as question\nanswering, commonsense reasoning, and factual generation. We analyze the effect\nof input characteristics, such as sequence length, entropy, named entity\ndensity and so on. Furthermore, we examine the impact of hardware-level\noptimizations through Dynamic Voltage and Frequency Scaling (DVFS), measuring\nhow different GPU clock settings affect latency and power consumption. Our\nempirical findings show that model architecture, input complexity, and clock\nconfiguration significantly influence inference efficiency. By correlating\ninput features with energy metrics and evaluating DVFS behavior, we identify\npractical strategies that reduce energy consumption by up to 30% while\npreserving model quality. This study provides actionable insights for designing\nenergy-efficient and sustainable LLM inference systems.\n","authors":["Paul Joe Maliakel","Shashikant Ilager","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2501.08219v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09651v1","updated":"2025-09-11T17:43:42Z","published":"2025-09-11T17:43:42Z","title":"Retrieval-Augmented Generation for Reliable Interpretation of Radio\n  Regulations","summary":"  We study question answering in the domain of radio regulations, a legally\nsensitive and high-stakes area. We propose a telecom-specific\nRetrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,\nthe first multiple-choice evaluation set for this domain, constructed from\nauthoritative sources using automated filtering and human validation. To assess\nretrieval quality, we define a domain-specific retrieval metric, under which\nour retriever achieves approximately 97% accuracy. Beyond retrieval, our\napproach consistently improves generation accuracy across all tested models. In\nparticular, while naively inserting documents without structured retrieval\nyields only marginal gains for GPT-4o (less than 1%), applying our pipeline\nresults in nearly a 12% relative improvement. These findings demonstrate that\ncarefully targeted grounding provides a simple yet strong baseline and an\neffective domain-specific solution for regulatory question answering. All code\nand evaluation scripts, along with our derived question-answer dataset, are\navailable at https://github.com/Zakaria010/Radio-RAG.\n","authors":["Zakaria El Kassimi","Fares Fourati","Mohamed-Slim Alouini"],"pdf_url":"https://arxiv.org/pdf/2509.09651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19455v2","updated":"2025-09-11T17:29:56Z","published":"2025-05-26T03:21:21Z","title":"MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question\n  Answering","summary":"  Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)\nhas achieved promising progress by leveraging prompt tuning to enable continual\nmulti-modal learning. However, most existing methods adopt cross-modal prompt\nisolation, constructing visual and textual prompts separately, which\nexacerbates modality imbalance and leads to degraded performance over time. To\ntackle this issue, we propose MM-Prompt, a novel framework incorporating\ncross-modal prompt query and cross-modal prompt recovery. The former enables\nbalanced prompt selection by incorporating cross-modal signals during query\nformation, while the latter promotes joint prompt reconstruction through\niterative cross-modal interactions, guided by an alignment loss to prevent\nrepresentational drift. Extensive experiments show that MM-Prompt surpasses\nprior approaches in accuracy and knowledge retention, while maintaining\nbalanced modality engagement throughout continual learning.\n","authors":["Xu Li","Fan Lyu"],"pdf_url":"https://arxiv.org/pdf/2505.19455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15557v2","updated":"2025-09-11T17:23:43Z","published":"2025-05-21T14:16:56Z","title":"Modular Jump Gaussian Processes","summary":"  Gaussian processes (GPs) furnish accurate nonlinear predictions with\nwell-calibrated uncertainty. However, the typical GP setup has a built-in\nstationarity assumption, making it ill-suited for modeling data from processes\nwith sudden changes, or \"jumps\" in the output variable. The \"jump GP\" (JGP) was\ndeveloped for modeling data from such processes, combining local GPs and latent\n\"level\" variables under a joint inferential framework. But joint modeling can\nbe fraught with difficulty. We aim to simplify by suggesting a more modular\nsetup, eschewing joint inference but retaining the main JGP themes: (a)\nlearning optimal neighborhood sizes that locally respect manifolds of\ndiscontinuity; and (b) a new cluster-based (latent) feature to capture regions\nof distinct output levels on both sides of the manifold. We show that each of\n(a) and (b) separately leads to dramatic improvements when modeling processes\nwith jumps. In tandem (but without requiring joint inference) that benefit is\ncompounded, as illustrated on real and synthetic benchmark examples from the\nrecent literature.\n","authors":["Anna R. Flowers","Christopher T. Franck","Mickaël Binois","Chiwoo Park","Robert B. Gramacy"],"pdf_url":"https://arxiv.org/pdf/2505.15557v2.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2509.06942v3","updated":"2025-09-11T17:14:11Z","published":"2025-09-08T17:54:08Z","title":"Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human\n  Preference","summary":"  Recent studies have demonstrated the effectiveness of directly aligning\ndiffusion models with human preferences using differentiable reward. However,\nthey exhibit two primary challenges: (1) they rely on multistep denoising with\ngradient computation for reward scoring, which is computationally expensive,\nthus restricting optimization to only a few diffusion steps; (2) they often\nneed continuous offline adaptation of reward models in order to achieve desired\naesthetic quality, such as photorealism or precise lighting effects. To address\nthe limitation of multistep denoising, we propose Direct-Align, a method that\npredefines a noise prior to effectively recover original images from any time\nsteps via interpolation, leveraging the equation that diffusion states are\ninterpolations between noise and target images, which effectively avoids\nover-optimization in late timesteps. Furthermore, we introduce Semantic\nRelative Preference Optimization (SRPO), in which rewards are formulated as\ntext-conditioned signals. This approach enables online adjustment of rewards in\nresponse to positive and negative prompt augmentation, thereby reducing the\nreliance on offline reward fine-tuning. By fine-tuning the FLUX model with\noptimized denoising and online reward adjustment, we improve its\nhuman-evaluated realism and aesthetic quality by over 3x.\n","authors":["Xiangwei Shen","Zhimin Li","Zhantao Yang","Shiyi Zhang","Yingfang Zhang","Donghao Li","Chunyu Wang","Qinglin Lu","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2509.06942v3.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.07715v2","updated":"2025-09-11T17:08:49Z","published":"2025-02-11T17:15:55Z","title":"Near-Optimal Sample Complexity in Reward-Free Kernel-Based Reinforcement\n  Learning","summary":"  Reinforcement Learning (RL) problems are being considered under increasingly\nmore complex structures. While tabular and linear models have been thoroughly\nexplored, the analytical study of RL under nonlinear function approximation,\nespecially kernel-based models, has recently gained traction for their strong\nrepresentational capacity and theoretical tractability. In this context, we\nexamine the question of statistical efficiency in kernel-based RL within the\nreward-free RL framework, specifically asking: how many samples are required to\ndesign a near-optimal policy? Existing work addresses this question under\nrestrictive assumptions about the class of kernel functions. We first explore\nthis question by assuming a generative model, then relax this assumption at the\ncost of increasing the sample complexity by a factor of H, the length of the\nepisode. We tackle this fundamental problem using a broad class of kernels and\na simpler algorithm compared to prior work. Our approach derives new confidence\nintervals for kernel ridge regression, specific to our RL setting, which may be\nof broader applicability. We further validate our theoretical findings through\nsimulations.\n","authors":["Aya Kayal","Sattar Vakili","Laura Toni","Alberto Bernacchia"],"pdf_url":"https://arxiv.org/pdf/2502.07715v2.pdf","comment":"Accepted at AISTATS 2025"},{"id":"http://arxiv.org/abs/2509.09619v1","updated":"2025-09-11T17:01:31Z","published":"2025-09-11T17:01:31Z","title":"Functional Groups are All you Need for Chemically Interpretable\n  Molecular Property Prediction","summary":"  Molecular property prediction using deep learning (DL) models has accelerated\ndrug and materials discovery, but the resulting DL models often lack\ninterpretability, hindering their adoption by chemists. This work proposes\ndeveloping molecule representations using the concept of Functional Groups (FG)\nin chemistry. We introduce the Functional Group Representation (FGR) framework,\na novel approach to encoding molecules based on their fundamental chemical\nsubstructures. Our method integrates two types of functional groups: those\ncurated from established chemical knowledge (FG), and those mined from a large\nmolecular corpus using sequential pattern mining (MFG). The resulting FGR\nframework encodes molecules into a lower-dimensional latent space by leveraging\npre-training on a large dataset of unlabeled molecules. Furthermore, the\nproposed framework allows the inclusion of 2D structure-based descriptors of\nmolecules. We demonstrate that the FGR framework achieves state-of-the-art\nperformance on a diverse range of 33 benchmark datasets spanning physical\nchemistry, biophysics, quantum mechanics, biological activity, and\npharmacokinetics while enabling chemical interpretability. Crucially, the\nmodel's representations are intrinsically aligned with established chemical\nprinciples, allowing chemists to directly link predicted properties to specific\nfunctional groups and facilitating novel insights into structure-property\nrelationships. Our work presents a significant step toward developing\nhigh-performing, chemically interpretable DL models for molecular discovery.\n","authors":["Roshan Balaji","Joe Bobby","Nirav Pravinbhai Bhatt"],"pdf_url":"https://arxiv.org/pdf/2509.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09616v1","updated":"2025-09-11T16:58:34Z","published":"2025-09-11T16:58:34Z","title":"Explaining Concept Drift through the Evolution of Group Counterfactuals","summary":"  Machine learning models in dynamic environments often suffer from concept\ndrift, where changes in the data distribution degrade performance. While\ndetecting this drift is a well-studied topic, explaining how and why the\nmodel's decision-making logic changes still remains a significant challenge. In\nthis paper, we introduce a novel methodology to explain concept drift by\nanalyzing the temporal evolution of group-based counterfactual explanations\n(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their\nassociated counterfactual action vectors before and after a drift. These\nevolving GCEs act as an interpretable proxy, revealing structural changes in\nthe model's decision boundary and its underlying rationale. We operationalize\nthis analysis within a three-layer framework that synergistically combines\ninsights from the data layer (distributional shifts), the model layer\n(prediction disagreement), and our proposed explanation layer. We show that\nsuch holistic view allows for a more comprehensive diagnosis of drift, making\nit possible to distinguish between different root causes, such as a spatial\ndata shift versus a re-labeling of concepts.\n","authors":["Ignacy Stępka","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2509.09616v1.pdf","comment":"TempXAI Workshop @ ECML PKDD 2025"},{"id":"http://arxiv.org/abs/2509.09611v1","updated":"2025-09-11T16:52:54Z","published":"2025-09-11T16:52:54Z","title":"ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and\n  Achieving Discretization Invariance","summary":"  We propose a novel data-lean operator learning algorithm, the Reduced Basis\nNeural Operator (ReBaNO), to solve a group of PDEs with multiple distinct\ninputs. Inspired by the Reduced Basis Method and the recently introduced\nGenerative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a\nmathematically rigorous greedy algorithm to build its network structure offline\nadaptively from the ground up. Knowledge distillation via task-specific\nactivation function allows ReBaNO to have a compact architecture requiring\nminimal computational cost online while embedding physics. In comparison to\nstate-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,\nand CNO, numerical results demonstrate that ReBaNO significantly outperforms\nthem in terms of eliminating/shrinking the generalization gap for both in- and\nout-of-distribution tests and being the only operator learning algorithm\nachieving strict discretization invariance.\n","authors":["Haolan Zheng","Yanlai Chen","Jiequn Han","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2509.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09599v1","updated":"2025-09-11T16:37:45Z","published":"2025-09-11T16:37:45Z","title":"Conditioning on PDE Parameters to Generalise Deep Learning Emulation of\n  Stochastic and Chaotic Dynamics","summary":"  We present a deep learning emulator for stochastic and chaotic\nspatio-temporal systems, explicitly conditioned on the parameter values of the\nunderlying partial differential equations (PDEs). Our approach involves\npre-training the model on a single parameter domain, followed by fine-tuning on\na smaller, yet diverse dataset, enabling generalisation across a broad range of\nparameter values. By incorporating local attention mechanisms, the network is\ncapable of handling varying domain sizes and resolutions. This enables\ncomputationally efficient pre-training on smaller domains while requiring only\na small additional dataset to learn how to generalise to larger domain sizes.\nWe demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky\nequation and stochastically-forced beta-plane turbulence, showcasing its\nability to capture phenomena at interpolated parameter values. The emulator\nprovides significant computational speed-ups over conventional numerical\nintegration, facilitating efficient exploration of parameter space, while a\nprobabilistic variant of the emulator provides uncertainty quantification,\nallowing for the statistical study of rare events.\n","authors":["Ira J. S. Shokar","Rich R. Kerswell","Peter H. Haynes"],"pdf_url":"https://arxiv.org/pdf/2509.09599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09597v1","updated":"2025-09-11T16:36:16Z","published":"2025-09-11T16:36:16Z","title":"Graph Alignment via Dual-Pass Spectral Encoding and Latent Space\n  Communication","summary":"  Graph alignment-the problem of identifying corresponding nodes across\nmultiple graphs-is fundamental to numerous applications. Most existing\nunsupervised methods embed node features into latent representations to enable\ncross-graph comparison without ground-truth correspondences. However, these\nmethods suffer from two critical limitations: the degradation of node\ndistinctiveness due to oversmoothing in GNN-based embeddings, and the\nmisalignment of latent spaces across graphs caused by structural noise, feature\nheterogeneity, and training instability, ultimately leading to unreliable node\ncorrespondences. We propose a novel graph alignment framework that\nsimultaneously enhances node distinctiveness and enforces geometric consistency\nacross latent spaces. Our approach introduces a dual-pass encoder that combines\nlow-pass and high-pass spectral filters to generate embeddings that are both\nstructure-aware and highly discriminative. To address latent space\nmisalignment, we incorporate a geometry-aware functional map module that learns\nbijective and isometric transformations between graph embeddings, ensuring\nconsistent geometric relationships across different representations. Extensive\nexperiments on graph benchmarks demonstrate that our method consistently\noutperforms existing unsupervised alignment baselines, exhibiting superior\nrobustness to structural inconsistencies and challenging alignment scenarios.\nAdditionally, comprehensive evaluation on vision-language benchmarks using\ndiverse pretrained models shows that our framework effectively generalizes\nbeyond graph domains, enabling unsupervised alignment of vision and language\nrepresentations.\n","authors":["Maysam Behmanesh","Erkan Turan","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2509.09597v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2509.09594v1","updated":"2025-09-11T16:34:17Z","published":"2025-09-11T16:34:17Z","title":"ObjectReact: Learning Object-Relative Control for Visual Navigation","summary":"  Visual navigation using only a single camera and a topological map has\nrecently become an appealing alternative to methods that require additional\nsensors and 3D maps. This is typically achieved through an \"image-relative\"\napproach to estimating control from a given pair of current observation and\nsubgoal image. However, image-level representations of the world have\nlimitations because images are strictly tied to the agent's pose and\nembodiment. In contrast, objects, being a property of the map, offer an\nembodiment- and trajectory-invariant world representation. In this work, we\npresent a new paradigm of learning \"object-relative\" control that exhibits\nseveral desirable characteristics: a) new routes can be traversed without\nstrictly requiring to imitate prior experience, b) the control prediction\nproblem can be decoupled from solving the image matching problem, and c) high\ninvariance can be achieved in cross-embodiment deployment for variations across\nboth training-testing and mapping-execution settings. We propose a topometric\nmap representation in the form of a \"relative\" 3D scene graph, which is used to\nobtain more informative object-level global path planning costs. We train a\nlocal controller, dubbed \"ObjectReact\", conditioned directly on a high-level\n\"WayObject Costmap\" representation that eliminates the need for an explicit RGB\ninput. We demonstrate the advantages of learning object-relative control over\nits image-relative counterpart across sensor height variations and multiple\nnavigation tasks that challenge the underlying spatial understanding\ncapability, e.g., navigating a map trajectory in the reverse direction. We\nfurther show that our sim-only policy is able to generalize well to real-world\nindoor environments. Code and supplementary material are accessible via project\npage: https://object-react.github.io/\n","authors":["Sourav Garg","Dustin Craggs","Vineeth Bhat","Lachlan Mares","Stefan Podgorski","Madhava Krishna","Feras Dayoub","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2509.09594v1.pdf","comment":"CoRL 2025; 23 pages including appendix"},{"id":"http://arxiv.org/abs/2509.08031v2","updated":"2025-09-11T16:27:59Z","published":"2025-09-09T15:30:40Z","title":"AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs","summary":"  Large Audio Language Models (LALMs) are rapidly advancing, but evaluating\nthem remains challenging due to inefficient toolkits that limit fair comparison\nand systematic assessment. Current frameworks suffer from three critical\nissues: slow processing that bottlenecks large-scale studies, inconsistent\nprompting that hurts reproducibility, and narrow task coverage that misses\nimportant audio reasoning capabilities. We introduce AU-Harness, an efficient\nand comprehensive evaluation framework for LALMs. Our system achieves a speedup\nof up to 127% over existing toolkits through optimized batch processing and\nparallel execution, enabling large-scale evaluations previously impractical. We\nprovide standardized prompting protocols and flexible configurations for fair\nmodel comparison across diverse scenarios. Additionally, we introduce two new\nevaluation categories: LLM-Adaptive Diarization for temporal audio\nunderstanding and Spoken Language Reasoning for complex audio-based cognitive\ntasks. Through evaluation across 380+ tasks, we reveal significant gaps in\ncurrent LALMs, particularly in temporal understanding and complex spoken\nlanguage reasoning tasks. Our findings also highlight a lack of standardization\nin instruction modality existent across audio benchmarks, which can lead up\nperformance differences up to 9.5 absolute points on the challenging complex\ninstruction following downstream tasks. AU-Harness provides both practical\nevaluation tools and insights into model limitations, advancing systematic LALM\ndevelopment.\n","authors":["Sidharth Surapaneni","Hoang Nguyen","Jash Mehta","Aman Tiwari","Oluwanifemi Bamgbose","Akshay Kalkunte","Sai Rajeswar","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2509.08031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18544v2","updated":"2025-09-11T16:27:30Z","published":"2025-04-10T02:48:20Z","title":"Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data:\n  A Systematic Review","summary":"  Generating synthetic tabular data can be challenging, however evaluation of\ntheir quality is just as challenging, if not more. This systematic review sheds\nlight on the critical importance of rigorous evaluation of synthetic health\ndata to ensure reliability, relevance, and their appropriate use. Based on\nscreening of 1766 papers and a detailed review of 101 papers we identified key\nchallenges, including lack of consensus on evaluation methods, improper use of\nevaluation metrics, limited input from domain experts, inadequate reporting of\ndataset characteristics, and limited reproducibility of results. In response,\nwe provide several guidelines on the generation and evaluation of synthetic\ndata, to allow the community to unlock and fully harness the transformative\npotential of synthetic data and accelerate innovation.\n","authors":["Nazia Nafis","Inaki Esnaola","Alvaro Martinez-Perez","Maria-Cruz Villa-Uriol","Venet Osmani"],"pdf_url":"https://arxiv.org/pdf/2504.18544v2.pdf","comment":null}],"Programming Languages":[{"id":"http://arxiv.org/abs/2508.15264v2","updated":"2025-09-11T02:55:06Z","published":"2025-08-21T05:55:07Z","title":"Exploring the Theory and Practice of Concurrency in the\n  Entity-Component-System Pattern","summary":"  The Entity-Component-System (ECS) software design pattern, long used in game\ndevelopment, encourages a clean separation of identity (entities), data\nproperties (components), and computational behaviors (systems). Programs\nwritten using the ECS pattern are naturally concurrent, and the pattern offers\nmodularity, flexibility, and performance benefits that have led to a\nproliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known\nand not well understood outside of a few domains. Existing explanations of the\nECS pattern tend to be mired in the concrete details of particular ECS\nframeworks, or they explain the pattern in terms of imperfect metaphors or in\nterms of what it is not. We seek a rigorous understanding of the ECS pattern\nvia the design of a formal model, Core ECS, that abstracts away the details of\nspecific implementations to reveal the essence of software using the ECS\npattern. We identify a class of Core ECS programs that behave deterministically\nregardless of scheduling, enabling use of the ECS pattern as a\ndeterministic-by-construction concurrent programming model. With Core ECS as a\npoint of comparison, we then survey several real-world ECS frameworks and find\nthat they all leave opportunities for deterministic concurrency unexploited.\nOur findings point out a space for new ECS implementation techniques that\nbetter leverage such opportunities.\n","authors":["Patrick Redmond","Jonathan Castello","José Manuel Calderón Trilla","Lindsey Kuper"],"pdf_url":"https://arxiv.org/pdf/2508.15264v2.pdf","comment":"This is an extended version (with appendices) of the OOPSLA 2025\n  paper"}]},"2025-09-10T00:00:00Z":{"High Energy Physics - Experimental":[{"id":"http://arxiv.org/abs/2509.07685v2","updated":"2025-09-10T21:50:46Z","published":"2025-09-09T12:52:10Z","title":"Measurement of the space-like $π^0$ transition form factor","summary":"  Based on $2.93\\,\\text{fb}^{-1}$ of $e^+e^-$ collision data taken with the\nBESIII detector at a center-of-mass energy of $3.773\\,\\text{GeV}$, the\ntwo-photon fusion process $e^+e^-\\to e^+e^-\\pi^0$ is investigated using a\nsingle-tag approach. The differential Born cross section\n$\\text{d}\\sigma/\\text{d}Q^2$ and the space-like transition form factor\n$|F(Q^2)|$ of the $\\pi^0$ are measured as functions of the squared momentum\ntransfer $Q^2$ of the tagged, scattered lepton. The measurement covers the\nrange $0.2 < Q^2 < 3.5\\,\\text{GeV}^2$. The results are consistent with previous\nmeasurements, and provide a significant improvement for $Q^2<2\\,\\text{GeV}^2$.\n","authors":[" BESIII Collaboration","M. Ablikim","M. N. Achasov","P. Adlarson","X. C. Ai","R. Aliberti","A. Amoroso","Q. An","Y. Bai","O. Bakina","Y. Ban","H. -R. Bao","V. Batozskaya","K. Begzsuren","N. Berger","M. Berlowski","M. Bertani","D. Bettoni","F. Bianchi","E. Bianco","A. Bortone","I. Boyko","R. A. Briere","A. Brueggemann","H. Cai","M. H. Cai","X. Cai","A. Calcaterra","G. F. Cao","N. Cao","S. A. Cetin","X. Y. Chai","J. F. Chang","G. R. Che","Y. Z. Che","C. H. Chen","Chao Chen","G. Chen","H. S. Chen","H. Y. Chen","M. L. Chen","S. J. Chen","S. L. Chen","S. M. Chen","T. Chen","X. R. Chen","X. T. Chen","X. Y. Chen","Y. B. Chen","Y. Q. Chen","Y. Q. Chen","Z. Chen","Z. J. Chen","Z. K. Chen","S. K. Choi","X. Chu","G. Cibinetto","F. Cossio","J. Cottee-Meldrum","J. J. Cui","H. L. Dai","J. P. Dai","A. Dbeyssi","R. E. de Boer","D. Dedovich","C. Q. Deng","Z. Y. Deng","A. Denig","I. Denysenko","M. Destefanis","F. De Mori","B. Ding","X. X. Ding","Y. Ding","Y. Ding","Y. X. Ding","J. Dong","L. Y. Dong","M. Y. Dong","X. Dong","M. C. Du","S. X. Du","S. X. Du","Y. Y. Duan","P. Egorov","G. F. Fan","J. J. Fan","Y. H. Fan","J. Fang","J. Fang","S. S. Fang","W. X. Fang","Y. Q. Fang","R. Farinelli","L. Fava","F. Feldbauer","G. Felici","C. Q. Feng","J. H. Feng","L. Feng","Q. X. Feng","Y. T. Feng","M. Fritsch","C. D. Fu","J. L. Fu","Y. W. Fu","H. Gao","X. B. Gao","Y. Gao","Y. N. Gao","Y. N. Gao","Y. Y. Gao","S. Garbolino","I. Garzia","P. T. Ge","Z. W. Ge","C. Geng","E. M. Gersabeck","A. Gilman","K. Goetzen","J. D. Gong","L. Gong","W. X. Gong","W. Gradl","S. Gramigna","M. Greco","M. H. Gu","Y. T. Gu","C. Y. Guan","A. Q. Guo","L. B. Guo","M. J. Guo","R. P. Guo","Y. P. Guo","A. Guskov","J. Gutierrez","K. L. Han","T. T. Han","F. Hanisch","K. D. Hao","X. Q. Hao","F. A. Harris","K. K. He","K. L. He","F. H. Heinsius","C. H. Heinz","Y. K. Heng","C. Herold","P. C. Hong","G. Y. Hou","X. T. Hou","Y. R. Hou","Z. L. Hou","H. M. Hu","J. F. Hu","Q. P. Hu","S. L. Hu","T. Hu","Y. Hu","Z. M. Hu","G. S. Huang","K. X. Huang","L. Q. Huang","P. Huang","X. T. Huang","Y. P. Huang","Y. S. Huang","T. Hussain","N. Hüsken","N. in der Wiesche","J. Jackson","Q. Ji","Q. P. Ji","W. Ji","X. B. Ji","X. L. Ji","Y. Y. Ji","Z. K. Jia","D. Jiang","H. B. Jiang","P. C. Jiang","S. J. Jiang","T. J. Jiang","X. S. Jiang","Y. Jiang","J. B. Jiao","J. K. Jiao","Z. Jiao","S. Jin","Y. Jin","M. Q. Jing","X. M. Jing","T. Johansson","S. Kabana","N. Kalantar-Nayestanaki","X. L. Kang","X. S. Kang","M. Kavatsyuk","B. C. Ke","V. Khachatryan","A. Khoukaz","R. Kiuchi","O. B. Kolcu","B. Kopf","M. Kuessner","X. Kui","N. Kumar","A. Kupsc","W. Kühn","Q. Lan","W. N. Lan","T. T. Lei","M. Lellmann","T. Lenz","C. Li","C. Li","C. Li","C. H. Li","C. K. Li","D. M. Li","F. Li","G. Li","H. B. Li","H. J. Li","H. N. Li","Hui Li","J. R. Li","J. S. Li","K. Li","K. L. Li","K. L. Li","L. J. Li","Lei Li","M. H. Li","M. R. Li","P. L. Li","P. R. Li","Q. M. Li","Q. X. Li","R. Li","S. X. Li","T. Li","T. Y. Li","W. D. Li","W. G. Li","X. Li","X. H. Li","X. L. Li","X. Y. Li","X. Z. Li","Y. Li","Y. G. Li","Y. P. Li","Z. J. Li","Z. Y. Li","H. Liang","Y. F. Liang","Y. T. Liang","G. R. Liao","L. B. Liao","M. H. Liao","Y. P. Liao","J. Libby","A. Limphirat","C. C. Lin","D. X. Lin","L. Q. Lin","T. Lin","B. J. Liu","B. X. Liu","C. Liu","C. X. Liu","F. Liu","F. H. Liu","Feng Liu","G. M. Liu","H. Liu","H. B. Liu","H. H. Liu","H. M. Liu","Huihui Liu","J. B. Liu","J. J. Liu","K. Liu","K. Liu","K. Y. Liu","Ke Liu","L. C. Liu","Lu Liu","M. H. Liu","P. L. Liu","Q. Liu","S. B. Liu","T. Liu","W. K. Liu","W. M. Liu","W. T. Liu","X. Liu","X. Liu","X. K. Liu","X. Y. Liu","Y. Liu","Y. Liu","Y. Liu","Y. B. Liu","Z. A. Liu","Z. D. Liu","Z. Q. Liu","X. C. Lou","F. X. Lu","H. J. Lu","J. G. Lu","X. L. Lu","Y. Lu","Y. H. Lu","Y. P. Lu","Z. H. Lu","C. L. Luo","J. R. Luo","J. S. Luo","M. X. Luo","T. Luo","X. L. Luo","Z. Y. Lv","X. R. Lyu","Y. F. Lyu","Y. H. Lyu","F. C. Ma","H. L. Ma","J. L. Ma","L. L. Ma","L. R. Ma","Q. M. Ma","R. Q. Ma","R. Y. Ma","T. Ma","X. T. Ma","X. Y. Ma","Y. M. Ma","F. E. Maas","I. MacKay","M. Maggiora","S. Malde","Q. A. Malik","H. X. Mao","Y. J. Mao","Z. P. Mao","S. Marcello","A. Marshall","F. M. Melendi","Y. H. Meng","Z. X. Meng","G. Mezzadri","H. Miao","T. J. Min","R. E. Mitchell","X. H. Mo","B. Moses","N. Yu. Muchnoi","J. Muskalla","Y. Nefedov","F. Nerling","L. S. Nie","I. B. Nikolaev","Z. Ning","S. Nisar","Q. L. Niu","W. D. Niu","C. Normand","S. L. Olsen","Q. Ouyang","S. Pacetti","X. Pan","Y. Pan","A. Pathak","Y. P. Pei","M. Pelizaeus","H. P. Peng","X. J. Peng","Y. Y. Peng","K. Peters","K. Petridis","J. L. Ping","R. G. Ping","S. Plura","V. Prasad","F. Z. Qi","H. R. Qi","M. Qi","S. Qian","W. B. Qian","C. F. Qiao","J. H. Qiao","J. J. Qin","J. L. Qin","L. Q. Qin","L. Y. Qin","P. B. Qin","X. P. Qin","X. S. Qin","Z. H. Qin","J. F. Qiu","Z. H. Qu","J. Rademacker","C. F. Redmer","A. Rivetti","M. Rolo","G. Rong","S. S. Rong","F. Rosini","Ch. Rosner","M. Q. Ruan","N. Salone","A. Sarantsev","Y. Schelhaas","K. Schoenning","M. Scodeggio","K. Y. Shan","W. Shan","X. Y. Shan","Z. J. Shang","J. F. Shangguan","L. G. Shao","M. Shao","C. P. Shen","H. F. Shen","W. H. Shen","X. Y. Shen","B. A. Shi","H. Shi","J. L. Shi","J. Y. Shi","S. Y. Shi","X. Shi","H. L. Song","J. J. Song","T. Z. Song","W. M. Song","Y. J. Song","Y. X. Song","S. Sosio","S. Spataro","F. Stieler","S. S Su","Y. J. Su","G. B. Sun","G. X. Sun","H. Sun","H. K. Sun","J. F. Sun","K. Sun","L. Sun","S. S. Sun","T. Sun","Y. C. Sun","Y. H. Sun","Y. J. Sun","Y. Z. Sun","Z. Q. Sun","Z. T. Sun","C. J. Tang","G. Y. Tang","J. Tang","J. J. Tang","L. F. Tang","Y. A. Tang","L. Y. Tao","M. Tat","J. X. Teng","J. Y. Tian","W. H. Tian","Y. Tian","Z. F. Tian","I. Uman","B. Wang","B. Wang","Bo Wang","C. Wang","C. Wang","Cong Wang","D. Y. Wang","H. J. Wang","J. J. Wang","K. Wang","L. L. Wang","L. W. Wang","M. Wang","M. Wang","N. Y. Wang","S. Wang","T. Wang","T. J. Wang","W. Wang","W. Wang","W. P. Wang","X. Wang","X. F. Wang","X. J. Wang","X. L. Wang","X. N. Wang","Y. Wang","Y. D. Wang","Y. F. Wang","Y. H. Wang","Y. J. Wang","Y. L. Wang","Y. N. Wang","Y. Q. Wang","Yaqian Wang","Yi Wang","Yuan Wang","Z. Wang","Z. L. Wang","Z. L. Wang","Z. Q. Wang","Z. Y. Wang","D. H. Wei","H. R. Wei","F. Weidner","S. P. Wen","Y. R. Wen","U. Wiedner","G. Wilkinson","M. Wolke","C. Wu","J. F. Wu","L. H. Wu","L. J. Wu","L. J. Wu","Lianjie Wu","S. G. Wu","S. M. Wu","X. Wu","X. H. Wu","Y. J. Wu","Z. Wu","L. Xia","X. M. Xian","B. H. Xiang","D. Xiao","G. Y. Xiao","H. Xiao","Y. L. Xiao","Z. J. Xiao","C. Xie","K. J. Xie","X. H. Xie","Y. Xie","Y. G. Xie","Y. H. Xie","Z. P. Xie","T. Y. Xing","C. F. Xu","C. J. Xu","G. F. Xu","H. Y. Xu","H. Y. Xu","M. Xu","Q. J. Xu","Q. N. Xu","T. D. Xu","W. Xu","W. L. Xu","X. P. Xu","Y. Xu","Y. Xu","Y. C. Xu","Z. S. Xu","F. Yan","H. Y. Yan","L. Yan","W. B. Yan","W. C. Yan","W. H. Yan","W. P. Yan","X. Q. Yan","H. J. Yang","H. L. Yang","H. X. Yang","J. H. Yang","R. J. Yang","T. Yang","Y. Yang","Y. F. Yang","Y. H. Yang","Y. Q. Yang","Y. X. Yang","Y. Z. Yang","M. Ye","M. H. Ye","Z. J. Ye","Junhao Yin","Z. Y. You","B. X. Yu","C. X. Yu","G. Yu","J. S. Yu","L. Q. Yu","M. C. Yu","T. Yu","X. D. Yu","Y. C. Yu","C. Z. Yuan","H. Yuan","J. Yuan","J. Yuan","L. Yuan","S. C. Yuan","X. Q. Yuan","Y. Yuan","Z. Y. Yuan","C. X. Yue","Ying Yue","A. A. Zafar","S. H. Zeng","X. Zeng","Y. Zeng","Y. J. Zeng","Y. J. Zeng","X. Y. Zhai","Y. H. Zhan","A. Q. Zhang","B. L. Zhang","B. X. Zhang","D. H. Zhang","G. Y. Zhang","G. Y. Zhang","H. Zhang","H. Zhang","H. C. Zhang","H. H. Zhang","H. Q. Zhang","H. R. Zhang","H. Y. Zhang","J. Zhang","J. Zhang","J. J. Zhang","J. L. Zhang","J. Q. Zhang","J. S. Zhang","J. W. Zhang","J. X. Zhang","J. Y. Zhang","J. Z. Zhang","Jianyu Zhang","L. M. Zhang","Lei Zhang","N. Zhang","P. Zhang","Q. Zhang","Q. Y. Zhang","R. Y. Zhang","S. H. Zhang","Shulei Zhang","X. M. Zhang","X. Y Zhang","X. Y. Zhang","Y. Zhang","Y. Zhang","Y. T. Zhang","Y. H. Zhang","Y. M. Zhang","Y. P. Zhang","Z. D. Zhang","Z. H. Zhang","Z. L. Zhang","Z. L. Zhang","Z. X. Zhang","Z. Y. Zhang","Z. Y. Zhang","Z. Z. Zhang","Zh. Zh. Zhang","G. Zhao","J. Y. Zhao","J. Z. Zhao","L. Zhao","L. Zhao","M. G. Zhao","N. Zhao","R. P. Zhao","S. J. Zhao","Y. B. Zhao","Y. L. Zhao","Y. X. Zhao","Z. G. Zhao","A. Zhemchugov","B. Zheng","B. M. Zheng","J. P. Zheng","W. J. Zheng","X. R. Zheng","Y. H. Zheng","B. Zhong","C. Zhong","H. Zhou","J. Q. Zhou","J. Y. Zhou","S. Zhou","X. Zhou","X. K. Zhou","X. R. Zhou","X. Y. Zhou","Y. X. Zhou","Y. Z. Zhou","A. N. Zhu","J. Zhu","K. Zhu","K. J. Zhu","K. S. Zhu","L. Zhu","L. X. Zhu","S. H. Zhu","T. J. Zhu","W. D. Zhu","W. D. Zhu","W. J. Zhu","W. Z. Zhu","Y. C. Zhu","Z. A. Zhu","X. Y. Zhuang","J. H. Zou","J. Zu"],"pdf_url":"https://arxiv.org/pdf/2509.07685v2.pdf","comment":"14 pages, 3 figures, submitted to Phys.Lett.B"},{"id":"http://arxiv.org/abs/2509.08931v1","updated":"2025-09-10T18:51:47Z","published":"2025-09-10T18:51:47Z","title":"Fabrication of thin planar radiopure foils with 82Se for the SuperNEMO\n  Demonstrator","summary":"  The SuperNEMO Demonstrator, designed to search for double beta decay using\nenriched 82Se, has been assembled in the Modane Underground Laboratory under\nthe French Alps. Thin foils with radio - purified and enriched 82Se are\ninstalled centrally in the detector. A novel foil fabrication method has been\ndeveloped, improving the radiopurity achieved in the previous generation\nexperiment. It consists of wrapping standalone selenium pads in raw Mylar,\ncombined with selenium purified by a new reverse-chromatography method. This\npaper describes the features of these foils, their fabrication process, the\ncharacterization results, and the integration of the foils into the SuperNEMO\nDemonstrator.\n","authors":["X. Aguerre","A. Barabash","A. Basharina-Freshville","M. Bongrand","Ch. Bourgeois","D. Breton","R. Breier","J. Busto","C. Cerna","J. Cesar","M. Ceschia","E. Chauveau","S. De Capua","D. Duchesneau","J. J. Evans","D. V. Filosofov","M. Granjon","M. Hoballah","R. Hodák","J. Horkley","A. Jeremie","S. Jullian","J. Kaizer","A. A. Klimenko","O. Kochetov","F. Koňařík","S. Konovalov","T. Křižák","A. Lahaie","K. Lang","Y. Lemière","T. Le Noblet","P. Li","P. Loaiza","J. Maalmi","M. Macko","F. Mamedov","C. Marquet","F. Mauger","A. Mendl","B. Morgan","I. Nemchenok","V. Palušová","C. Patrick","F. Perrot","M. Petro","F. Piquemal","P. Povinec","S. Pratt","M. Proga","W. S. Quinn","A. V. Rakhimov","Y. R amachers","A. Remoto","N. I. Rukhadze","R. Saakyan","R. Salazar","J. Sedgbeer","Y. Shitov","L. Simard","F. Šimkovic","A. A. Smolnikov","S. Söldner-Rembold","I. Štekl","J. Suhonen","H. Tedjditi","J. Thomas","V. Timkin","V. Tretyak","V. I. Tretyak","G. Turnbull","Y. Vereshchaka","G. Warot","D. Waters","V. Yumatov"],"pdf_url":"https://arxiv.org/pdf/2509.08931v1.pdf","comment":"20 pages, 11 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.21162v3","updated":"2025-09-10T18:40:15Z","published":"2024-10-28T16:03:41Z","title":"Phase space compression of a positive muon beam in two spatial\n  dimensions","summary":"  We present the first demonstration of simultaneous phase space compression in\ntwo spatial dimensions of a positive muon beam, the first stage of the novel\nhigh-brightness muon beam under development by the muCool collaboration at the\nPaul Scherrer Institute. The keV-energy, sub-mm size beam would enable a factor\n10$^5$ improvement in brightness for precision muSR, and atomic and particle\nphysics measurements with positive muons. This compression is achieved within a\ncryogenic helium gas target with a strong density gradient, placed in a\nhomogeneous magnetic field, under the influence of a complex electric field. In\nthe next phase, the muon beam will be extracted into vacuum.\n","authors":["A. Antognini","N. J. Ayres","I. Belosevic","V. Bondar","A. Eggenberger","M. Hildebrandt","R. Iwai","K. Kirch","A. Knecht","G. Lospalluto","J. Nuber","A. Papa","M. Sakurai","I. Solovyev","D. Taqqu","T. Yan"],"pdf_url":"https://arxiv.org/pdf/2410.21162v3.pdf","comment":"Submission to SciPost"},{"id":"http://arxiv.org/abs/2503.07941v2","updated":"2025-09-10T18:37:55Z","published":"2025-03-11T00:47:10Z","title":"Gain characterization of LGAD sensors with beta particles and 28-MeV\n  protons","summary":"  Low Gain Avalanche Diodes, also known as LGADs, are widely considered for\nfast-timing applications in high energy physics, nuclear physics, space\nscience, medical imaging, and precision measurements of rare processes. Such\ndevices are silicon-based and feature an intrinsic gain due to a $p{^+}$-doped\nlayer that allows the production of a controlled avalanche of carriers, with\nmultiplication on the order of 10-100. This technology can provide time\nresolution on the order of 20-30 ps, and variants of this technology can\nprovide precision tracking too. The characterization of LGAD performance has so\nfar primarily been focused on the interaction of minimum ionizing particles for\nhigh energy and nuclear physics applications. This article expands the study of\nLGAD performance to highly-ionizing particles, such as 28-MeV protons, which\nare relevant for several future scientific applications, e.g. in biology and\nmedical physics, among others. These studies were performed with a beam of\n28-MeV protons from a tandem Van de Graaff accelerator at Brookhaven National\nLaboratory and beta particles from a $^{90}{\\rm Sr}$ source; these were used to\ncharacterize the response and the gain of an LGAD as a function of bias voltage\nand collected charge. The experimental results are also compared to TCAD\nsimulations.\n","authors":["Mohamed Hijas Mohamed Farook","Gabriele Giacomini","Gabriele DAmen","Giovanni Pinaroli","Enrico Rossi","Sally Seidel","Alessandro Tricoli"],"pdf_url":"https://arxiv.org/pdf/2503.07941v2.pdf","comment":"Submitted to JINST"},{"id":"http://arxiv.org/abs/2509.08901v1","updated":"2025-09-10T18:03:22Z","published":"2025-09-10T18:03:22Z","title":"ORLCA: A concept for an open-source Life Cycle Assessment repository\n  built for research","summary":"  Comprehensive Life Cycle Assessment (LCA) as a tool to account for the full\nrange of environmental impacts of resource use in commodities or services is a\nfirst step in reducing these impacts. There is an increasing necessity to\naccount for these aspects in the planning, running and end-of-life of\nscientific experiments and research infrastructure. In the following, the\nconcept for an Open Research Life Cycle Assessment (ORLCA) repository is\npresented to support this endeavour. It is designed to comply fully with the\nprinciples of findability, accessibility, interoperability, and reusability\n(FAIR).\n","authors":["Hannah Wakeling","Kristin Lohwasser","Peter Millington"],"pdf_url":"https://arxiv.org/pdf/2509.08901v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2509.08878v1","updated":"2025-09-10T18:00:00Z","published":"2025-09-10T18:00:00Z","title":"Finding Unexpected Non-Helical Tracks","summary":"  Many theories of physics beyond the Standard Model predict particles with\nnon-helical trajectories in a uniform magnetic field, but standard tracking\nalgorithms assume helical paths and so are incapable of discovering non-helical\ntracks. While alternative algorithms have been developed for specific\ntrajectories, unforeseen physics could lead to unanticipated behavior, and such\nunexpected tracks are largely invisible to current algorithms, despite being\npotentially striking to the naked eye. A model-agnostic tracking algorithm is\npresented, capable of reconstructing a broad class of smooth non-helical tracks\nwithout requiring explicit specification of particle trajectories, instead\ndefining the target trajectories implicitly in the training sample. The network\nexhibits strong performance, even outside of the trajectories defined by the\ntraining sample. This proof-of-principle study takes the first step towards\nsearches for unexpected tracks which may await discovery in current data.\n","authors":["Levi Condren","Daniel Whiteson"],"pdf_url":"https://arxiv.org/pdf/2509.08878v1.pdf","comment":"11 pages, 8 figures"}],"Machine Learning - Statistics":[{"id":"http://arxiv.org/abs/2508.19441v2","updated":"2025-09-10T18:54:57Z","published":"2025-08-26T21:22:11Z","title":"Data-Augmented Few-Shot Neural Stencil Emulation for System\n  Identification of Computer Models","summary":"  Partial differential equations (PDEs) underpin the modeling of many natural\nand engineered systems. It can be convenient to express such models as neural\nPDEs rather than using traditional numerical PDE solvers by replacing part or\nall of the PDE's governing equations with a neural network representation.\nNeural PDEs are often easier to differentiate, linearize, reduce, or use for\nuncertainty quantification than the original numerical solver. They are usually\ntrained on solution trajectories obtained by long time integration of the PDE\nsolver. Here we propose a more sample-efficient data-augmentation strategy for\ngenerating neural PDE training data from a computer model by space-filling\nsampling of local \"stencil\" states. This approach removes a large degree of\nspatiotemporal redundancy present in trajectory data and oversamples states\nthat may be rarely visited but help the neural PDE generalize across the state\nspace. We demonstrate that accurate neural PDE stencil operators can be learned\nfrom synthetic training data generated by the computational equivalent of 10\ntimesteps' worth of numerical simulation. Accuracy is further improved if we\nassume access to a single full-trajectory simulation from the computer model,\nwhich is typically available in practice. Across several PDE systems, we show\nthat our data-augmented synthetic stencil data yield better trained neural\nstencil operators, with clear performance gains compared with naively sampled\nstencil data from simulation trajectories.\n","authors":["Sanket Jantre","Deepak Akhare","Xiaoning Qian","Nathan M. Urban"],"pdf_url":"https://arxiv.org/pdf/2508.19441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08926v1","updated":"2025-09-10T18:42:19Z","published":"2025-09-10T18:42:19Z","title":"Similarity-based Outlier Detection for Noisy Object Re-Identification\n  Using Beta Mixtures","summary":"  Object re-identification (Re-ID) methods are highly sensitive to label noise,\nwhich typically leads to significant performance degradation. We address this\nchallenge by reframing Re-ID as a supervised image similarity task and adopting\na Siamese network architecture trained to capture discriminative pairwise\nrelationships. Central to our approach is a novel statistical outlier detection\n(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier\nDetection), which models the distribution of cosine similarities between\nembedding pairs using a two-component Beta distribution mixture model. We\nestablish a novel identifiability result for mixtures of two Beta\ndistributions, ensuring that our learning task is well-posed.The proposed OD\nstep complements the Re-ID architecture combining binary cross-entropy,\ncontrastive, and cosine embedding losses that jointly optimize feature-level\nsimilarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising\nand Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and\nvehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance\ncompared to the state-of-the-art methods across various noise levels (10-30\\%),\ndemonstrating both robustness and broad applicability in noisy Re-ID scenarios.\nThe implementation of Beta-SOD is available at:\nhttps://github.com/waqar3411/Beta-SOD\n","authors":["Waqar Ahmad","Evan Murphy","Vladimir A. Krylov"],"pdf_url":"https://arxiv.org/pdf/2509.08926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07998v2","updated":"2025-09-10T18:21:17Z","published":"2025-02-11T22:34:49Z","title":"Adaptive kernel predictors from feature-learning infinite limits of\n  neural networks","summary":"  Previous influential work showed that infinite width limits of neural\nnetworks in the lazy training regime are described by kernel machines. Here, we\nshow that neural networks trained in the rich, feature learning infinite-width\nregime in two different settings are also described by kernel machines, but\nwith data-dependent kernels. For both cases, we provide explicit expressions\nfor the kernel predictors and prescriptions to numerically calculate them. To\nderive the first predictor, we study the large-width limit of feature-learning\nBayesian networks, showing how feature learning leads to task-relevant\nadaptation of layer kernels and preactivation densities. The saddle point\nequations governing this limit result in a min-max optimization problem that\ndefines the kernel predictor. To derive the second predictor, we study gradient\nflow training of randomly initialized networks trained with weight decay in the\ninfinite-width limit using dynamical mean field theory (DMFT). The fixed point\nequations of the arising DMFT defines the task-adapted internal representations\nand the kernel predictor. We compare our kernel predictors to kernels derived\nfrom lazy regime and demonstrate that our adaptive kernels achieve lower test\nloss on benchmark datasets.\n","authors":["Clarissa Lauditi","Blake Bordelon","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2502.07998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08911v1","updated":"2025-09-10T18:15:41Z","published":"2025-09-10T18:15:41Z","title":"Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum\n  Applications","summary":"  The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning\nalgorithm with numerous applications. Applied to the matrix version of the\nLearning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,\nit is well known that MMWU achieves the minimax-optimal regret bound of\n$O(\\sqrt{T\\log d})$, where $T$ is the time horizon. In this paper, we present\nan improved algorithm achieving the instance-optimal regret bound of\n$O(\\sqrt{T\\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,\n$I_d$ is the identity matrix, and $S(\\cdot||\\cdot)$ denotes the quantum\nrelative entropy. Furthermore, our algorithm has the same computational\ncomplexity as MMWU, indicating that the improvement in the regret bound is\n``free''.\n  Technically, we first develop a general potential-based framework for matrix\nLEA, with MMWU being its special case induced by the standard exponential\npotential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace\ninequality built on a Laplace transform technique, which allows the application\nof general potential functions beyond exponential to matrix LEA. Our algorithm\nis finally induced by an optimal potential function from the vector LEA\nproblem, based on the imaginary error function.\n  Complementing the above, we provide a memory lower bound for matrix LEA, and\nexplore the applications of our algorithm in quantum learning theory. We show\nthat it outperforms the state of the art for learning quantum states corrupted\nby depolarization noise, random quantum states, and Gibbs states. In addition,\napplying our algorithm to linearized convex losses enables predicting nonlinear\nquantum properties, such as purity, quantum virtual cooling, and R\\'{e}nyi-$2$\ncorrelation.\n","authors":["Weiyuan Gong","Tongyang Li","Xinzhao Wang","Zhiyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.08911v1.pdf","comment":"47 pages"},{"id":"http://arxiv.org/abs/2509.08785v1","updated":"2025-09-10T17:14:12Z","published":"2025-09-10T17:14:12Z","title":"Narrative-Guided Reinforcement Learning: A Platform for Studying\n  Language Model Influence on Decision Making","summary":"  We present a preliminary experimental platform that explores how narrative\nelements might shape AI decision-making by combining reinforcement learning\n(RL) with language model reasoning. While AI systems can now both make\ndecisions and engage in narrative reasoning, these capabilities have mostly\nbeen studied separately. Our platform attempts to bridge this gap using a\ndual-system architecture to examine how narrative frameworks could influence\nreward-based learning. The system comprises a reinforcement learning policy\nthat suggests actions based on past experience, and a language model that\nprocesses these suggestions through different narrative frameworks to guide\ndecisions. This setup enables initial experimentation with narrative elements\nwhile maintaining consistent environment and reward structures. We implement\nthis architecture in a configurable gridworld environment, where agents receive\nboth policy suggestions and information about their surroundings. The\nplatform's modular design facilitates controlled testing of environmental\ncomplexity, narrative parameters, and the interaction between reinforcement\nlearning and narrative-based decisions. Our logging system captures basic\ndecision metrics, from RL policy values to language model reasoning to action\nselection patterns. While preliminary, this implementation provides a\nfoundation for studying how different narrative frameworks might affect\nreward-based decisions and exploring potential interactions between\noptimization-based learning and symbolic reasoning in AI systems.\n","authors":["Anup Tuladhar","Araz Minhas","Adam Kirton","Eli Kinney-Lang"],"pdf_url":"https://arxiv.org/pdf/2509.08785v1.pdf","comment":"Extended Abstract for RLDM 2025"}],"Programming Languages":[{"id":"http://arxiv.org/abs/2509.09059v1","updated":"2025-09-10T23:45:19Z","published":"2025-09-10T23:45:19Z","title":"Dependent-Type-Preserving Memory Allocation","summary":"  Dependently typed programming languages such as Coq, Agda, Idris, and F*,\nallow programmers to write detailed specifications of their programs and prove\ntheir programs meet these specifications. However, these specifications can be\nviolated during compilation since they are erased after type checking. External\nprograms linked with the compiled program can violate the specifications of the\noriginal program and change the behavior of the compiled program -- even when\ncompiled with a verified compiler. For example, since Coq does not allow\nexplicitly allocating memory, a programmer might link their Coq program with a\nC program that can allocate memory. Even if the Coq program is compiled with a\nverified compiler, the external C program can still violate the memory-safe\nspecification of the Coq program by providing an uninitialized pointer to\nmemory. This error could be ruled out by type checking in a language expressive\nenough to indicate whether memory is initialized versus uninitialized. Linking\nwith a program with an uninitialized pointer could be considered ill-typed, and\nour linking process could prevent linking with ill-typed programs. To\nfacilitate type checking during linking, we can use type-preserving\ncompilation, which preserves the types through the compilation process. In this\nongoing work, we develop a typed intermediate language that supports dependent\nmemory allocation, as well as a dependent-type-preserving compiler pass for\nmemory allocation.\n","authors":["Paulette Koronkevich","William J. Bowman"],"pdf_url":"https://arxiv.org/pdf/2509.09059v1.pdf","comment":"Submitted and received second place at the Student Research\n  Competition at Principles of Programming Languages 2022"},{"id":"http://arxiv.org/abs/2509.09019v1","updated":"2025-09-10T21:27:35Z","published":"2025-09-10T21:27:35Z","title":"Towards Verified Compilation of Floating-point Optimization in\n  Scientific Computing Programs","summary":"  Scientific computing programs often undergo aggressive compiler optimization\nto achieve high performance and efficient resource utilization. While\nperformance is critical, we also need to ensure that these optimizations are\ncorrect. In this paper, we focus on a specific class of optimizations,\nfloating-point optimizations, notably due to fast math, at the LLVM IR level.\nWe present a preliminary work, which leverages the Verified LLVM framework in\nthe Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)\noptimization for a basic block implementing the arithmetic expression $a * b +\nc$ . We then propose ways to extend this preliminary results by adding more\nprogram features and fast math floating-point optimizations.\n","authors":["Mohit Tekriwal","John Sarracino"],"pdf_url":"https://arxiv.org/pdf/2509.09019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08804v1","updated":"2025-09-10T17:37:56Z","published":"2025-09-10T17:37:56Z","title":"Approximate Algorithms for Verifying Differential Privacy with Gaussian\n  Distributions","summary":"  The verification of differential privacy algorithms that employ Gaussian\ndistributions is little understood. This paper tackles the challenge of\nverifying such programs by introducing a novel approach to approximating\nprobability distributions of loop-free programs that sample from both discrete\nand continuous distributions with computable probability density functions,\nincluding Gaussian and Laplace. We establish that verifying\n$(\\epsilon,\\delta)$-differential privacy for these programs is \\emph{almost\ndecidable}, meaning the problem is decidable for all values of $\\delta$ except\nthose in a finite set. Our verification algorithm is based on computing\nprobabilities to any desired precision by combining integral approximations,\nand tail probability bounds. The proposed methods are implemented in the tool,\nDipApprox, using the FLINT library for high-precision integral computations,\nand incorporate optimizations to enhance scalability. We validate {\\ourtool} on\nfundamental privacy-preserving algorithms, such as Gaussian variants of the\nSparse Vector Technique and Noisy Max, demonstrating its effectiveness in both\nconfirming privacy guarantees and detecting violations.\n","authors":["Bishnu Bhusal","Rohit Chadha","A. Prasad Sistla","Mahesh Viswanathan"],"pdf_url":"https://arxiv.org/pdf/2509.08804v1.pdf","comment":"An extended abstract appears in CCS 2025"},{"id":"http://arxiv.org/abs/2509.08727v1","updated":"2025-09-10T16:17:31Z","published":"2025-09-10T16:17:31Z","title":"Securing Cryptographic Software via Typed Assembly Language (Extended\n  Version)","summary":"  Authors of cryptographic software are well aware that their code should not\nleak secrets through its timing behavior, and, until 2018, they believed that\nfollowing industry-standard constant-time coding guidelines was sufficient.\nHowever, the revelation of the Spectre family of speculative execution attacks\ninjected new complexities.\n  To block speculative attacks, prior work has proposed annotating the\nprogram's source code to mark secret data, with hardware using this information\nto decide when to speculate (i.e., when only public values are involved) or not\n(when secrets are in play). While these solutions are able to track secret\ninformation stored on the heap, they suffer from limitations that prevent them\nfrom correctly tracking secrets on the stack, at a cost in performance.\n  This paper introduces SecSep, a transformation framework that rewrites\nassembly programs so that they partition secret and public data on the stack.\nBy moving from the source-code level to assembly rewriting, SecSep is able to\naddress limitations of prior work. The key challenge in performing this\nassembly rewriting stems from the loss of semantic information through the\nlengthy compilation process. The key innovation of our methodology is a new\nvariant of typed assembly language (TAL), Octal, which allows us to address\nthis challenge. Assembly rewriting is driven by compile-time inference within\nOctal. We apply our technique to cryptographic programs and demonstrate that it\nenables secure speculation efficiently, incurring a low average overhead of\n$1.2\\%$.\n","authors":["Shixin Song","Tingzhen Dong","Kosi Nwabueze","Julian Zanders","Andres Erbsen","Adam Chlipala","Mengjia Yan"],"pdf_url":"https://arxiv.org/pdf/2509.08727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.17233v2","updated":"2025-09-10T15:58:41Z","published":"2025-07-23T05:57:15Z","title":"Hiord#: An Approach to the Specification and Verification of\n  Higher-Order (C)LP Programs","summary":"  Higher-order constructs enable more expressive and concise code by allowing\nprocedures to be parameterized by other procedures. Assertions allow expressing\npartial program specifications, which can be verified either at compile time\n(statically) or run time (dynamically). In higher-order programs, assertions\ncan also describe higher-order arguments. While in the context of (constraint)\nlogic programming ((C)LP), run-time verification of higher-order assertions has\nreceived some attention, compile-time verification remains relatively\nunexplored. We propose a novel approach for statically verifying higher-order\n(C)LP programs with higher-order assertions. Although we use the Ciao assertion\nlanguage for illustration, our approach is quite general and we believe is\napplicable to similar contexts. Higher-order arguments are described using\npredicate properties -- a special kind of property which exploits the (Ciao)\nassertion language. We refine the syntax and semantics of these properties and\nintroduce an abstract criterion to determine conformance to a predicate\nproperty at compile time, based on a semantic order relation comparing the\npredicate property with the predicate assertions. We then show how to handle\nthese properties using an abstract interpretation-based static analyzer for\nprograms with first-order assertions by reducing predicate properties to\nfirst-order properties. Finally, we report on a prototype implementation and\nevaluate it through various examples within the Ciao system.\n","authors":["Marco Ciccalè","Daniel Jurjo-Rivas","Jose F. Morales","Pedro López-García","Manuel V. Hermenegildo"],"pdf_url":"https://arxiv.org/pdf/2507.17233v2.pdf","comment":"Accepted for publication in Theory and Practice of Logic Programming\n  (TPLP)"}]},"2025-09-09T00:00:00Z":{"Programming Languages":[{"id":"http://arxiv.org/abs/2509.08182v1","updated":"2025-09-09T23:03:53Z","published":"2025-09-09T23:03:53Z","title":"XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics,\n  Convergence Guarantees, and Human-AI Protocols","summary":"  Structured prompting with XML tags has emerged as an effective way to steer\nlarge language models (LLMs) toward parseable, schema-adherent outputs in\nreal-world systems. We develop a logic-first treatment of XML prompting that\nunifies (i) grammar-constrained decoding, (ii) fixed-point semantics over\nlattices of hierarchical prompts, and (iii) convergent human-AI interaction\nloops. We formalize a complete lattice of XML trees under a refinement order\nand prove that monotone prompt-to-prompt operators admit least fixed points\n(Knaster-Tarski) that characterize steady-state protocols; under a task-aware\ncontraction metric on trees, we further prove Banach-style convergence of\niterative guidance. We instantiate these results with context-free grammars\n(CFGs) for XML schemas and show how constrained decoding guarantees\nwell-formedness while preserving task performance. A set of multi-layer\nhuman-AI interaction recipes demonstrates practical deployment patterns,\nincluding multi-pass \"plan $\\to$ verify $\\to$ revise\" routines and agentic tool\nuse. We provide mathematically complete proofs and tie our framework to recent\nadvances in grammar-aligned decoding, chain-of-verification, and programmatic\nprompting.\n","authors":["Faruk Alpay","Taylan Alpay"],"pdf_url":"https://arxiv.org/pdf/2509.08182v1.pdf","comment":"7 pages, multiple XML prompts"},{"id":"http://arxiv.org/abs/2509.07763v1","updated":"2025-09-09T13:58:46Z","published":"2025-09-09T13:58:46Z","title":"What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring\n  Motivations in Open-Source Projects","summary":"  Context. Code refactoring improves software quality without changing external\nbehavior. Despite its advantages, its benefits are hindered by the considerable\ncost of time, resources, and continuous effort it demands. Aim. Understanding\nwhy developers refactor, and which metrics capture these motivations, may\nsupport wider and more effective use of refactoring in practice. Method. We\nperformed a large-scale empirical study to analyze developers refactoring\nactivity, leveraging Large Language Models (LLMs) to identify underlying\nmotivations from version control data, comparing our findings with previous\nmotivations reported in the literature. Results. LLMs matched human judgment in\n80% of cases, but aligned with literature-based motivations in only 47%. They\nenriched 22% of motivations with more detailed rationale, often highlighting\nreadability, clarity, and structural improvements. Most motivations were\npragmatic, focused on simplification and maintainability. While metrics related\nto developer experience and code readability ranked highest, their correlation\nwith motivation categories was weak. Conclusions. We conclude that LLMs\neffectively capture surface-level motivations but struggle with architectural\nreasoning. Their value lies in providing localized explanations, which, when\ncombined with software metrics, can form hybrid approaches. Such integration\noffers a promising path toward prioritizing refactoring more systematically and\nbalancing short-term improvements with long-term architectural goals.\n","authors":["Mikel Robredo","Matteo Esposito","Fabio Palomba","Rafael Peñaloza","Valentina Lenarduzzi"],"pdf_url":"https://arxiv.org/pdf/2509.07763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07609v1","updated":"2025-09-09T11:34:40Z","published":"2025-09-09T11:34:40Z","title":"What's in the Box: Ergonomic and Expressive Capture Tracking over\n  Generic Data Structures (Extended Version)","summary":"  Capturing types in Scala unify static effect and resource tracking with\nobject capabilities, enabling lightweight effect polymorphism with minimal\nnotational overhead. However, their expressiveness has been insufficient for\ntracking capabilities embedded in generic data structures, preventing them from\nscaling to the standard collections library -- an essential prerequisite for\nbroader adoption. This limitation stems from the inability to name capabilities\nwithin the system's notion of box types.\n  This paper develops System Capless, a new foundation for capturing types that\nprovides the theoretical basis for reach capabilities (rcaps), a novel\nmechanism for naming \"what's in the box.\" The calculus refines the universal\ncapability notion into a new scheme with existential and universal capture set\nquantification. Intuitively, rcaps witness existentially quantified capture\nsets inside the boxes of generic types in a way that does not require exposing\nexistential capture types in the surface language. We have fully mechanized the\nformal metatheory of System Capless in Lean, including proofs of type soundness\nand scope safety. System Capless supports the same lightweight notation of\ncapturing types plus rcaps, as certified by a type-preserving translation, and\nalso enables fully optional explicit capture-set quantification to increase\nexpressiveness.\n  Finally, we present a full reimplementation of capture checking in Scala 3\nbased on System Capless and migrate the entire Scala collections library and an\nasynchronous programming library to evaluate its practicality and ergonomics.\nOur results demonstrate that reach capabilities enable the adoption of capture\nchecking in production code with minimal changes and minimal-to-zero notational\noverhead in a vast majority of cases.\n","authors":["Yichen Xu","Oliver Bračevac","Cao Nguyen Pham","Martin Odersky"],"pdf_url":"https://arxiv.org/pdf/2509.07609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07551v1","updated":"2025-09-09T09:41:20Z","published":"2025-09-09T09:41:20Z","title":"Fast and Extensible Hybrid Embeddings with Micros","summary":"  Macro embedding is a popular approach to defining extensible shallow\nembeddings of object languages in Scheme like host languages. While macro\nembedding has even been shown to enable implementing extensible typed languages\nin systems like Racket, it comes at a cost: compile-time performance. In this\npaper, we revisit micros - syntax to intermediate representation (IR)\ntransformers, rather than source syntax to source syntax transformers (macros).\nMicro embedding enables stopping at an IR, producing a deep embedding and\nenabling high performance compile-time functions over an efficient IR, before\nshallowly embedding the IR back into source syntax. Combining micros with\nseveral design patterns to enable the IR and functions over it to be\nextensible, we achieve extensible hybrid embedding of statically typed\nlanguages with significantly improved compile-time compared to macro-embedding\napproaches. We describe our design patterns and propose new abstractions\npackaging these patterns.\n","authors":["Sean Bocirnea","William J. Bowman"],"pdf_url":"https://arxiv.org/pdf/2509.07551v1.pdf","comment":"13 pages"}]},"2025-09-08T00:00:00Z":{"Programming Languages":[{"id":"http://arxiv.org/abs/2508.15137v3","updated":"2025-09-08T23:15:20Z","published":"2025-08-21T00:19:43Z","title":"Software Model Checking via Summary-Guided Search (Extended Version)","summary":"  In this work, we describe a new software model-checking algorithm called GPS.\nGPS treats the task of model checking a program as a directed search of the\nprogram states, guided by a compositional, summary-based static analysis. The\nsummaries produced by static analysis are used both to prune away infeasible\npaths and to drive test generation to reach new, unexplored program states. GPS\ncan find both proofs of safety and counter-examples to safety (i.e., inputs\nthat trigger bugs), and features a novel two-layered search strategy that\nrenders it particularly efficient at finding bugs in programs featuring long,\ninput-dependent error paths. To make GPS refutationally complete (in the sense\nthat it will find an error if one exists, if it is allotted enough time), we\nintroduce an instrumentation technique and show that it helps GPS achieve\nrefutation-completeness without sacrificing overall performance. We benchmarked\nGPS on a diverse suite of benchmarks including programs from the Software\nVerification Competition (SV-COMP), from prior literature, as well as synthetic\nprograms based on examples in this paper. We found that our implementation of\nGPS outperforms state-of-the-art software model checkers (including the top\nperformers in SV-COMP ReachSafety-Loops category), both in terms of the number\nof benchmarks solved and in terms of running time.\n","authors":["Ruijie Fang","Zachary Kincaid","Thomas Reps"],"pdf_url":"https://arxiv.org/pdf/2508.15137v3.pdf","comment":"Extended version of paper in OOPSLA 2025 (with typo and stylistic\n  fixes compared to v2 manuscript). 37 pages"},{"id":"http://arxiv.org/abs/2507.03659v3","updated":"2025-09-08T19:11:06Z","published":"2025-07-04T15:36:12Z","title":"Specification-Guided Repair of Arithmetic Errors in Dafny Programs using\n  LLMs","summary":"  Debugging and repairing faults when programs fail to formally verify can be\ncomplex and time-consuming. Automated Program Repair (APR) can ease this burden\nby automatically identifying and fixing faults. However, traditional APR\ntechniques often rely on test suites for validation, but these may not capture\nall possible scenarios. In contrast, formal specifications provide strong\ncorrectness criteria, enabling more effective automated repair.\n  In this paper, we present an APR tool for Dafny, a verification-aware\nprogramming language that uses formal specifications - including\npre-conditions, post-conditions, and invariants - as oracles for fault\nlocalization and repair. Assuming the correctness of the specifications and\nfocusing on arithmetic bugs, we localize faults through a series of steps,\nwhich include using Hoare logic to determine the state of each statement within\nthe program, and applying Large Language Models (LLMs) to synthesize candidate\nfixes. The models considered are GPT-4o mini, Llama 3, Mistral 7B, and Llemma\n7B.\n  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny\nprograms. Our tool achieves 89.6% fault localization coverage and GPT-4o mini\nyields the highest repair success rate of 74.18%. These results highlight the\npotential of combining formal reasoning with LLM-based program synthesis for\nautomated program repair.\n","authors":["Valentina Wu","Alexandra Mendes","Alexandre Abreu"],"pdf_url":"https://arxiv.org/pdf/2507.03659v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.06872v1","updated":"2025-09-08T16:42:02Z","published":"2025-09-08T16:42:02Z","title":"Mechanized Metatheory of Forward Reasoning for End-to-End\n  Linearizability Proofs","summary":"  In the past decade, many techniques have been developed to prove\nlinearizability, the gold standard of correctness for concurrent data\nstructures. Intuitively, linearizability requires that every operation on a\nconcurrent data structure appears to take place instantaneously, even when\ninterleaved with other operations. Most recently, Jayanti et al. presented the\nfirst sound and complete \"forward reasoning\" technique for proving\nlinearizability that relates the behavior of a concurrent data structure to a\nreference atomic data structure as time moves forward. This technique can be\nused to produce machine-checked proofs of linearizability in TLA+. However,\nwhile Jayanti et al.'s approach is shown to be sound and complete, a\nmechanization of this important metatheoretic result is still outstanding. As a\nresult, it is not possible to produce verified end-to-end proofs of\nlinearizability. To reduce the size of this trusted computing base, we\nformalize this forward reasoning technique and mechanize proofs of its\nsoundness and completeness in Rocq. As a case study, we use the approach to\nproduce a verified end-to-end proof of linearizability for a simple concurrent\nregister.\n","authors":["Zachary Kent","Ugur Y. Yavuz","Siddhartha Jayanti","Stephanie Balzer","Guy Blelloch"],"pdf_url":"https://arxiv.org/pdf/2509.06872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.06845v1","updated":"2025-09-08T16:15:18Z","published":"2025-09-08T16:15:18Z","title":"MIO: Multiverse Debugging in the Face of Input/Output -- Extended\n  Version with Additional Appendices","summary":"  Debugging non-deterministic programs on microcontrollers is notoriously\nchallenging, especially when bugs manifest in unpredictable, input-dependent\nexecution paths. A recent approach, called multiverse debugging, makes it\neasier to debug non-deterministic programs by allowing programmers to explore\nall potential execution paths. Current multiverse debuggers enable both forward\nand backward traversal of program paths, and some facilitate jumping to any\npreviously visited states, potentially branching into alternative execution\npaths within the state space.\n  Unfortunately, debugging programs that involve input/output operations using\nexisting multiverse debuggers can reveal inaccessible program states, i.e.\nstates which are not encountered during regular execution. This can\nsignificantly hinder the debugging process, as the programmer may spend\nsubstantial time exploring and examining inaccessible program states, or worse,\nmay mistakenly assume a bug is present in the code, when in fact, the issue is\ncaused by the debugger.\n  This paper presents a novel approach to multiverse debugging, which can\naccommodate a broad spectrum of input/output operations. We provide the\nsemantics of our approach and prove the correctness of our debugger, ensuring\nthat despite having support for a wide range of input/output operations the\ndebugger will only explore those program states which can be reached during\nregular execution.\n  We have developed a prototype, called MIO, leveraging the WARDuino\nWebAssembly virtual machine to demonstrate the feasibility and efficiency of\nour techniques. As a demonstration of the approach we highlight a color dial\nbuilt with a Lego Mindstorms motor, and color sensor, providing a tangible\nexample of how our approach enables multiverse debugging for programs running\non an STM32 microcontroller.\n","authors":["Tom Lauwaerts","Maarten Steevens","Christophe Scholliers"],"pdf_url":"https://arxiv.org/pdf/2509.06845v1.pdf","comment":"This extended version provides auxiliary material to the article of\n  the same title that will appear in the ACM Digital Library as part of the\n  PACMPL issue for OOPSLA 2025"},{"id":"http://arxiv.org/abs/2509.06794v1","updated":"2025-09-08T15:22:51Z","published":"2025-09-08T15:22:51Z","title":"Dato: A Task-Based Programming Model for Dataflow Accelerators","summary":"  Recent deep learning workloads increasingly push computational demand beyond\nwhat current memory systems can sustain, with many kernels stalling on data\nmovement rather than computation. While modern dataflow accelerators\nincorporate on-chip streaming to mitigate off-chip bandwidth limitations,\nexisting programming models struggle to harness these capabilities effectively.\nLow-level interfaces provide fine-grained control but impose significant\ndevelopment overhead, whereas high-level tile-based languages abstract away\ncommunication details, restricting optimization and forcing compilers to\nreconstruct the intended dataflow. We present Dato, a Python-embedded,\ntask-based programming model for dataflow accelerators that elevates data\ncommunication and sharding to first-class type constructs. Developers write\nprograms as a graph of tasks connected via explicit stream types, with sharded\ninputs specified using layout types. These tasks are first mapped virtually\nonto the accelerator's spatial fabric, and the compiler then generates a\nphysical mapping that respects hardware constraints. Experimental results on\nboth AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves\nhigh performance while significantly reducing the burden of writing optimized\ncode. On the NPU, Dato attains up to 84% hardware utilization for GEMM and\ndelivers a 2.81x speedup on attention kernels compared to a state-of-the-art\ncommercial framework. On the FPGA, Dato surpasses leading frameworks in\nperformance when generating custom systolic arrays, achieving 98% of the\ntheoretical peak performance.\n","authors":["Shihan Fang","Hongzheng Chen","Niansong Zhang","Jiajie Li","Han Meng","Adrian Liu","Zhiru Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.06794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.06752v1","updated":"2025-09-08T14:40:34Z","published":"2025-09-08T14:40:34Z","title":"Termination Analysis of Linear-Constraint Programs","summary":"  This Survey provides an overview of techniques in termination analysis for\nprograms with numerical variables and transitions defined by linear\nconstraints. This subarea of program analysis is challenging due to the\nexistence of undecidable problems, and this Survey systematically explores\napproaches that mitigate this inherent difficulty. These include foundational\ndecidability results, the use of ranking functions, and disjunctive\nwell-founded transition invariants. The Survey also discusses non-termination\nwitnesses, used to prove that a program will not halt. We examine the\nalgorithmic and complexity aspects of these methods, showing how different\napproaches offer a trade-off between expressive power and computational\ncomplexity. The Survey does not discuss how termination analysis is performed\non real-world programming languages, nor does it consider more expressive\nabstract models that include non-linear arithmetic, probabilistic choice, or\nterm rewriting systems.\n","authors":["Amir M. Ben-Amram","Samir Genaim","Joël Ouaknine","James Worrell"],"pdf_url":"https://arxiv.org/pdf/2509.06752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.06724v1","updated":"2025-09-08T14:22:00Z","published":"2025-09-08T14:22:00Z","title":"Pacing Types: Safe Monitoring of Asynchronous Streams","summary":"  Stream-based monitoring is a real-time safety assurance mechanism for complex\ncyber-physical systems such as unmanned aerial vehicles. In this context, a\nmonitor aggregates streams of input data from sensors and other sources to give\nreal-time statistics and assessments of the system's health. Since monitors are\nsafety-critical components, it is crucial to ensure that they are free of\npotential runtime errors. One of the central challenges in designing reliable\nstream-based monitors is to deal with the asynchronous nature of data streams:\nin concrete applications, the different sensors being monitored produce values\nat different speeds, and it is the monitor's responsibility to correctly react\nto the asynchronous arrival of different streams of values. To ease this\nprocess, modern frameworks for stream-based monitoring such as RTLola feature\nan expressive specification language that allows to finely specify data\nsynchronization policies. While this feature dramatically simplifies the design\nof monitors, it can also lead to subtle runtime errors. To mitigate this issue,\nthis paper presents pacing types, a novel type system implemented in RTLola to\nensure that monitors for asynchronous streams are well-behaved at runtime. We\nformalize the essence of pacing types for a core fragment of RTLola, and\npresent a soundness proof of the pacing type system using a new logical\nrelation.\n","authors":["Florian Kohn","Arthur Correnson","Jan Baumeister","Bernd Finkbeiner"],"pdf_url":"https://arxiv.org/pdf/2509.06724v1.pdf","comment":null}]},"2025-09-07T00:00:00Z":{"Programming Languages":[{"id":"http://arxiv.org/abs/2506.22370v4","updated":"2025-09-07T11:26:53Z","published":"2025-06-27T16:34:13Z","title":"Can Large Language Models Help Students Prove Software Correctness? An\n  Experimental Study with Dafny","summary":"  Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. Our findings show that\nstudents perform significantly better when using ChatGPT; however, performance\ngains are tied to prompt quality. We conclude with practical recommendations\nfor integrating LLMs into formal methods courses more effectively, including\ndesigning LLM-aware challenges that promote learning rather than substitution.\n","authors":["Carolina Carreira","Álvaro Silva","Alexandre Abreu","Alexandra Mendes"],"pdf_url":"https://arxiv.org/pdf/2506.22370v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.23696v2","updated":"2025-09-07T11:10:45Z","published":"2025-06-30T10:17:39Z","title":"What Challenges Do Developers Face When Using Verification-Aware\n  Programming Languages?","summary":"  Software reliability is critical in ensuring that the digital systems we\ndepend on function correctly. In software development, increasing software\nreliability often involves testing. However, for complex and critical systems,\ndevelopers can use Design by Contract (DbC) methods to define precise\nspecifications that software components must satisfy. Verification-Aware (VA)\nprogramming languages support DbC and formal verification at compile-time or\nrun-time, offering stronger correctness guarantees than traditional testing.\nHowever, despite the strong guarantees provided by VA languages, their adoption\nremains limited. In this study, we investigate the barriers to adopting VA\nlanguages by analyzing developer discussions on public forums using topic\nmodeling techniques. We complement this analysis with a developer survey to\nbetter understand the practical challenges associated with VA languages. Our\nfindings reveal key obstacles to adoption, including steep learning curves and\nusability issues. Based on these insights, we identify actionable\nrecommendations to improve the usability and accessibility of VA languages. Our\nfindings suggest that simplifying tool interfaces, providing better educational\nmaterials, and improving integration with everyday development environments\ncould improve the usability and adoption of these languages. Our work provides\nactionable insights for improving the usability of VA languages and making\nverification tools more accessible.\n","authors":["Francisco Oliveira","Alexandra Mendes","Carolina Carreira"],"pdf_url":"https://arxiv.org/pdf/2506.23696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00948v2","updated":"2025-09-07T04:40:03Z","published":"2025-08-31T17:57:01Z","title":"Decision Procedure for A Theory of String Sequences","summary":"  The theory of sequences, supported by many SMT solvers, can model program\ndata types including bounded arrays and lists. Sequences are parameterized by\nthe element data type and provide operations such as accessing elements,\nconcatenation, forming sub-sequences and updating elements. Strings and\nsequences are intimately related; many operations, e.g., matching a string\naccording to a regular expression, splitting strings, or joining strings in a\nsequence, are frequently used in string-manipulating programs. Nevertheless,\nthese operations are typically not directly supported by existing SMT solvers,\nwhich instead only consider the generic theory of sequences. In this paper, we\npropose a theory of string sequences and study its satisfiability. We show\nthat, while it is undecidable in general, the decidability can be recovered by\nrestricting to the straight-line fragment. This is shown by encoding each\nstring sequence as a string, and each string sequence operation as a\ncorresponding string operation. We provide pre-image computation for the\nresulting string operations with respect to automata, effectively casting it\ninto the generic OSTRICH string constraint solving framework. We implement the\nnew decision procedure as a tool $\\ostrichseq$, and carry out experiments on\nbenchmark constraints generated from real-world JavaScript programs,\nhand-crafted templates and unit tests. The experiments confirm the efficacy of\nour approach.\n","authors":["Denghang Hu","Taolue Chen","Philipp Rümmer","Fu Song","Zhilin Wu"],"pdf_url":"https://arxiv.org/pdf/2509.00948v2.pdf","comment":"21 pages, 2 tables, APLAS 2025"}]}}